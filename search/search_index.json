{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"PlanGEN Documentation","text":"<p>Welcome to the PlanGEN documentation. This guide will help you understand and use the PlanGEN framework for solving complex problems using large language models (LLMs) in a multi-agent approach.</p>"},{"location":"#overview","title":"Overview","text":"<p>PlanGEN is a Python framework that implements the workflow described in the paper \"PlanGEN: Generative Planning with Large Language Models.\" It enables constraint extraction, solution generation, verification, and solution selection through various planning algorithms.</p> <p>The framework features:</p> <ul> <li>Multi-agent system: Specialized agents for constraint extraction, solution generation, verification, and selection</li> <li>Planning algorithms: BestOfN, TreeOfThought, REBASE, and MixtureOfAlgorithms</li> <li>Model support: OpenAI and AWS Bedrock integrations</li> <li>Template-based prompting: Jinja2 templates for customizable LLM interactions</li> <li>Visualization capabilities: Graph-based visualization of planning processes</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To quickly get started with PlanGEN, check out the Installation Guide and the Quick Start Guide.</p>"},{"location":"#documentation-sections","title":"Documentation Sections","text":""},{"location":"#user-guide","title":"User Guide","text":"<p>Step-by-step guides for using PlanGEN, including installation, configuration, and common use cases.</p>"},{"location":"#api-reference","title":"API Reference","text":"<p>Complete reference for all public classes, methods, and functions in the PlanGEN framework.</p>"},{"location":"#algorithm-reference","title":"Algorithm Reference","text":"<p>Detailed explanation of the planning algorithms available in PlanGEN, including their parameters and use cases.</p>"},{"location":"#examples","title":"Examples","text":"<p>Code examples for various scenarios and use cases.</p>"},{"location":"#api-design-and-orchestration-integration","title":"API Design and Orchestration Integration","text":"<p>Comprehensive review of API design, compatibility with agent orchestration frameworks, and recommendations for integration. Includes guides for LangChain, LangGraph, and other frameworks.</p>"},{"location":"#support","title":"Support","text":"<p>If you encounter issues or have questions, please open an issue on GitHub.</p>"},{"location":"API_DESIGN_AND_ORCHESTRATION/","title":"API Design and Orchestration Integration Guide","text":""},{"location":"API_DESIGN_AND_ORCHESTRATION/#overview","title":"Overview","text":"<p>This document provides a comprehensive review of the PlanGEN API design and recommendations for integrating PlanGEN into existing agent orchestration frameworks.</p> <p>API Quality Rating: 8/10 - Well-defined with room for improvement Orchestration Compatibility: 6.5/10 - Good for sync workflows, limited for async</p>"},{"location":"API_DESIGN_AND_ORCHESTRATION/#1-api-definition-quality-excellent","title":"1. API Definition Quality: \u2705 Excellent","text":""},{"location":"API_DESIGN_AND_ORCHESTRATION/#strengths","title":"Strengths","text":"<ul> <li>Clear Public API Surface: Well-separated public API through <code>api.py</code></li> <li><code>PlanGen</code> - Main entry point</li> <li><code>Algorithm</code> - Algorithm factory</li> <li><code>Visualization</code> - Visualization tools</li> <li><code>Verifiers</code> - Verifier factory</li> <li>Protocol-Based Design: Uses Python Protocols for extensibility</li> <li><code>ModelProtocol</code> - Custom model implementations</li> <li><code>VerifierProtocol</code> - Custom verifier implementations</li> <li>Fluent Factory Methods: Multiple ways to initialize</li> </ul> <pre><code>plangen = PlanGen.create()                              # Auto-detect\nplangen = PlanGen.with_openai(model_name=\"gpt-4o\")     # Explicit\nplangen = PlanGen.with_bedrock(model_id=\"...\")         # Alternative\nplangen = PlanGen.with_model(custom_model)             # Custom\n</code></pre> <ul> <li>Consistent Method Signatures: All methods well-documented with type hints</li> </ul>"},{"location":"API_DESIGN_AND_ORCHESTRATION/#known-limitations","title":"Known Limitations","text":"<ul> <li>Generic Return Types: <code>solve()</code> returns <code>Dict[str, Any]</code> instead of typed structure</li> <li>Encapsulation Issues: Access to internal agents via <code>self._plangen.solution_agent</code> suggests incomplete abstraction</li> <li>Limited Configuration: No way to set default algorithms, verifiers, or behaviors at instance level</li> </ul>"},{"location":"API_DESIGN_AND_ORCHESTRATION/#2-orchestration-framework-compatibility","title":"2. Orchestration Framework Compatibility","text":""},{"location":"API_DESIGN_AND_ORCHESTRATION/#compatibility-matrix","title":"Compatibility Matrix","text":"Framework Rating Primary Issue LangGraph \u26a0\ufe0f 6/10 Uses LangGraph internally but doesn't expose it LangChain \u26a0\ufe0f 5/10 No native integration; can wrap as Tool CrewAI \ud83d\udd34 3/10 Expects async agents; PlanGen is sync-only AutoGen \ud83d\udd34 2/10 Designed for multi-turn; PlanGen is single-turn FastAPI/AsyncIO \ud83d\udd34 2/10 Blocks event loop without thread pool Pure Async \ud83d\udd34 1/10 No async methods available"},{"location":"API_DESIGN_AND_ORCHESTRATION/#the-core-problem-no-async-support","title":"The Core Problem: No Async Support \u26a0\ufe0f","text":"<p>This is the biggest limitation for modern orchestration:</p> <pre><code># This blocks the entire async event loop\nasync def my_orchestration():\n    plangen = PlanGen.create()\n    result = plangen.solve(problem)  # BLOCKS - no await available\n    return result\n</code></pre> <p>Workaround (not ideal):</p> <pre><code>from concurrent.futures import ThreadPoolExecutor\n\nasync def my_orchestration():\n    loop = asyncio.get_event_loop()\n    executor = ThreadPoolExecutor()\n    result = await loop.run_in_executor(\n        executor,\n        lambda: PlanGen.create().solve(problem)\n    )\n    return result\n</code></pre>"},{"location":"API_DESIGN_AND_ORCHESTRATION/#3-integration-patterns","title":"3. Integration Patterns","text":""},{"location":"API_DESIGN_AND_ORCHESTRATION/#easy-simple-synchronous-orchestrator","title":"\u2705 Easy: Simple Synchronous Orchestrator","text":"<pre><code>def solve_step(input):\n    plangen = PlanGen.create()\n    result = plangen.solve(input[\"problem\"])\n    return result[\"selected_solution\"]\n</code></pre>"},{"location":"API_DESIGN_AND_ORCHESTRATION/#moderate-langchain-tool","title":"\u2705 Moderate: LangChain Tool","text":"<pre><code>from langchain_core.tools import tool\n\n@tool\ndef plangen_solver(problem: str) -&gt; str:\n    \"\"\"Use PlanGEN to solve problems.\"\"\"\n    plangen = PlanGen.create()\n    result = plangen.solve(problem)\n    return result[\"selected_solution\"]\n\n# Works but blocks event loop if used with async agents\n</code></pre>"},{"location":"API_DESIGN_AND_ORCHESTRATION/#difficult-custom-workflow-control","title":"\u26a0\ufe0f Difficult: Custom Workflow Control","text":"<pre><code># Current limitation: Can't easily intercept between steps\n\n# Current best practice:\nconstraints = plangen.extract_constraints(problem)\n# ... custom processing (not part of framework) ...\nplan = plangen.generate_plan(problem, constraints)\n# ... custom evaluation ...\nfeedback, score = plangen.verify_plan(problem, plan, constraints)\n\n# Desired: Hook into workflow\n# NOT CURRENTLY POSSIBLE - see recommendations below\n</code></pre>"},{"location":"API_DESIGN_AND_ORCHESTRATION/#very-difficult-real-time-progress-tracking","title":"\u274c Very Difficult: Real-Time Progress Tracking","text":"<pre><code># Current: No way to track progress during execution\nresult = plangen.solve(problem)  # Black box\n\n# Desired (see recommendations):\nplangen.on_step_complete(lambda step, result: print(f\"{step}: {result}\"))\nresult = await plangen.solve_async(problem)\n</code></pre>"},{"location":"API_DESIGN_AND_ORCHESTRATION/#4-recommended-improvements","title":"4. Recommended Improvements","text":""},{"location":"API_DESIGN_AND_ORCHESTRATION/#phase-1-critical-1-2-weeks","title":"Phase 1: Critical (1-2 weeks)","text":"<p>1.1 Add Async Support</p> <pre><code>from typing import Optional\n\nclass PlanGen:\n    async def solve_async(\n        self,\n        problem: str,\n        algorithm: str = \"default\",\n        verifier: Optional[VerifierProtocol] = None,\n        **algorithm_params,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Async version of solve() for use in async orchestrators.\"\"\"\n        # Implementation would use asyncio for parallelization\n</code></pre> <p>1.2 Add Callback System</p> <pre><code>from typing import Callable, Dict, Any\n\nclass PlanGen:\n    def on_constraint_extracted(\n        self,\n        callback: Callable[[List[str]], None]\n    ) -&gt; None:\n        \"\"\"Register callback when constraints are extracted.\"\"\"\n\n    def on_solution_generated(\n        self,\n        callback: Callable[[str, int], None]  # plan, index\n    ) -&gt; None:\n        \"\"\"Register callback when solution is generated.\"\"\"\n\n    def on_verification_complete(\n        self,\n        callback: Callable[[str, float, str], None]  # plan, score, feedback\n    ) -&gt; None:\n        \"\"\"Register callback when verification is complete.\"\"\"\n\n    def on_solution_selected(\n        self,\n        callback: Callable[[str, float], None]  # solution, score\n    ) -&gt; None:\n        \"\"\"Register callback when final solution is selected.\"\"\"\n</code></pre> <p>1.3 Improve Type Definitions</p> <pre><code>from typing import TypedDict, List, Dict, Any\n\nclass SolveResult(TypedDict):\n    \"\"\"Type-safe result structure.\"\"\"\n    problem: str\n    constraints: List[str]\n    solutions: List[str]\n    verification_results: List[Dict[str, Any]]\n    selected_solution: str\n    score: float\n    metadata: Dict[str, Any]\n</code></pre>"},{"location":"API_DESIGN_AND_ORCHESTRATION/#phase-2-important-1-2-months","title":"Phase 2: Important (1-2 months)","text":"<p>2.1 Configuration Objects</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass PlanGenConfig:\n    model_name: str = \"gpt-4o\"\n    temperature: float = 0.7\n    max_tokens: Optional[int] = None\n    default_algorithm: str = \"best_of_n\"\n    num_solutions: int = 3\n    enable_logging: bool = False\n    cache_enabled: bool = False\n\nplangen = PlanGen.from_config(config)\n</code></pre> <p>2.2 Expose Workflow Customization</p> <pre><code>class PlanGen:\n    def get_workflow_graph(self) -&gt; StateGraph:\n        \"\"\"Get the underlying LangGraph workflow.\n\n        Allows advanced users to inspect or modify the workflow\n        before execution.\n        \"\"\"\n        return self._plangen.workflow\n</code></pre> <p>2.3 Streaming Support</p> <pre><code>from typing import Iterator\n\nclass PlanGen:\n    def solve_stream(\n        self,\n        problem: str,\n        algorithm: str = \"default\",\n        **kwargs,\n    ) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"Stream results as they become available.\n\n        Yields intermediate results for real-time UI updates\n        and progress tracking.\n        \"\"\"\n</code></pre>"},{"location":"API_DESIGN_AND_ORCHESTRATION/#phase-3-nice-to-have-2-3-months","title":"Phase 3: Nice to Have (2-3 months)","text":"<p>3.1 Verifier Composition</p> <pre><code>class Verifiers:\n    @staticmethod\n    def combine(\n        verifiers: List[VerifierProtocol],\n        strategy: str = \"average\"  # or \"weighted\", \"first_fail\"\n    ) -&gt; VerifierProtocol:\n        \"\"\"Combine multiple verifiers.\"\"\"\n</code></pre> <p>3.2 Middleware/Interceptor System</p> <pre><code>def logging_middleware(step_name: str, input: Any, next: Callable) -&gt; Any:\n    \"\"\"Example middleware for logging.\"\"\"\n    logger.info(f\"Executing {step_name}\")\n    result = next(input)\n    logger.info(f\"Completed {step_name}\")\n    return result\n\nplangen.add_middleware(logging_middleware)\n</code></pre> <p>3.3 Result Caching</p> <pre><code>class PlanGen:\n    def enable_cache(\n        self,\n        ttl: int = 3600,\n        max_entries: int = 100\n    ) -&gt; None:\n        \"\"\"Enable result caching for identical problems.\"\"\"\n</code></pre>"},{"location":"API_DESIGN_AND_ORCHESTRATION/#5-integration-examples","title":"5. Integration Examples","text":""},{"location":"API_DESIGN_AND_ORCHESTRATION/#example-1-using-with-langchain-agents-improved","title":"Example 1: Using with LangChain Agents (Improved)","text":"<pre><code>from langchain_core.tools import tool\nfrom plangen import PlanGen\n\n@tool\nasync def solve_with_plangen(problem: str) -&gt; str:\n    \"\"\"Use PlanGEN to solve complex problems.\n\n    This tool breaks down problems, generates solutions,\n    and selects the best one.\n    \"\"\"\n    plangen = PlanGen.create()\n\n    # Track progress\n    plangen.on_constraint_extracted(\n        lambda c: print(f\"Found {len(c)} constraints\")\n    )\n    plangen.on_solution_generated(\n        lambda p, i: print(f\"Generated solution {i+1}\")\n    )\n\n    # Execute asynchronously\n    result = await plangen.solve_async(problem)\n    return result[\"selected_solution\"]\n</code></pre>"},{"location":"API_DESIGN_AND_ORCHESTRATION/#example-2-custom-workflow-in-orchestration","title":"Example 2: Custom Workflow in Orchestration","text":"<pre><code>from plangen import PlanGen\nfrom langgraph.graph import StateGraph\n\n# Get PlanGEN workflow\nplangen = PlanGen.create()\nplanning_workflow = plangen.get_workflow_graph()\n\n# Integrate into larger workflow\norchestration_graph = StateGraph(MyState)\n\n# Add constraint extraction with custom pre-processing\norchestration_graph.add_node(\n    \"extract_and_validate\",\n    extract_and_validate_constraints\n)\n\n# Add the planning workflow\norchestration_graph.add_node(\n    \"planning\",\n    planning_workflow\n)\n\n# Add custom post-processing\norchestration_graph.add_node(\n    \"refine_solution\",\n    refine_solution\n)\n\n# Connect nodes\norchestration_graph.add_edge(\"extract_and_validate\", \"planning\")\norchestration_graph.add_edge(\"planning\", \"refine_solution\")\n</code></pre>"},{"location":"API_DESIGN_AND_ORCHESTRATION/#6-migration-path-for-users","title":"6. Migration Path for Users","text":""},{"location":"API_DESIGN_AND_ORCHESTRATION/#for-existing-code-no-breaking-changes","title":"For Existing Code (No Breaking Changes)","text":"<p>The recommended improvements are backward compatible:</p> <ul> <li>New async methods alongside sync versions</li> <li>New callbacks are optional</li> <li>TypedDict is hint-only, doesn't affect runtime</li> </ul>"},{"location":"API_DESIGN_AND_ORCHESTRATION/#migration-timeline","title":"Migration Timeline","text":"<ol> <li>Week 1-2: Deprecation notices in docstrings</li> <li>Week 3-4: Release v0.2.0 with new features</li> <li>Month 2: Encourage migration through examples</li> <li>Month 3: Consider deprecating old patterns in v0.3.0</li> </ol>"},{"location":"API_DESIGN_AND_ORCHESTRATION/#7-decision-matrix-when-to-use-plangen","title":"7. Decision Matrix: When to Use PlanGEN","text":""},{"location":"API_DESIGN_AND_ORCHESTRATION/#use-plangen-when","title":"Use PlanGEN When","text":"<p>\u2705 You need multi-step problem decomposition \u2705 You want constraint extraction + solution generation + verification \u2705 You're building synchronous systems \u2705 You need multiple solution candidates \u2705 You want custom verifiers for specific domains</p>"},{"location":"API_DESIGN_AND_ORCHESTRATION/#dont-use-plangen-when","title":"Don't Use PlanGEN When","text":"<p>\u274c You need pure async/streaming LLM interaction \u274c You need real-time token streaming \u274c You require multi-turn conversations \u274c You need WebSocket connections or server-sent events \u274c You want fine-grained control over every LLM call</p>"},{"location":"API_DESIGN_AND_ORCHESTRATION/#8-feedback-and-contributions","title":"8. Feedback and Contributions","text":"<p>If you're integrating PlanGEN into a new framework, please share:</p> <ul> <li>Challenges you encounter</li> <li>Workarounds you develop</li> <li>Feature requests for better integration</li> </ul> <p>For feature requests and roadmap discussions, see the GitHub Issues.</p>"},{"location":"streaming_guide/","title":"Streaming Support in PlanGEN","text":"<p>PlanGEN provides streaming support to enable real-time progress tracking and intermediate result access during problem-solving workflows.</p>"},{"location":"streaming_guide/#overview","title":"Overview","text":"<p>Streaming allows you to receive updates as PlanGEN progresses through its workflow stages:</p> <ol> <li>Constraint Extraction - Extract constraints from the problem</li> <li>Solution Generation - Generate multiple candidate solutions</li> <li>Solution Verification - Verify each solution against constraints</li> <li>Solution Selection - Select the best solution based on verification results</li> </ol>"},{"location":"streaming_guide/#basic-usage","title":"Basic Usage","text":""},{"location":"streaming_guide/#using-solve_stream","title":"Using <code>solve_stream()</code>","text":"<p>The <code>solve_stream()</code> method yields dictionaries with step information as the workflow progresses:</p> <pre><code>from plangen import PlanGen\n\n# Create PlanGen instance\nplangen = PlanGen.create(model=\"gpt-4o\")\n\n# Stream the solving process\nproblem = \"Your problem statement here\"\nfor update in plangen.solve_stream(problem):\n    step = update[\"step\"]\n    status = update[\"status\"]\n\n    if status == \"in_progress\":\n        print(f\"Starting {step}...\")\n    elif status == \"complete\":\n        print(f\"Completed {step}\")\n        # Access step data\n        data = update.get(\"data\", {})\n    elif status == \"error\":\n        print(f\"Error in {step}: {update['error']}\")\n        break\n</code></pre>"},{"location":"streaming_guide/#update-structure","title":"Update Structure","text":"<p>Each update dictionary contains:</p> <ul> <li><code>step</code> (str): The current workflow step</li> <li><code>\"extract_constraints\"</code> - Constraint extraction</li> <li><code>\"generate_solutions\"</code> - Solution generation</li> <li><code>\"verify_solutions\"</code> - Solution verification</li> <li><code>\"select_solution\"</code> - Solution selection</li> <li> <p><code>\"error\"</code> - General error (only on critical failures)</p> </li> <li> <p><code>status</code> (str): Current status of the step</p> </li> <li><code>\"in_progress\"</code> - Step has started</li> <li><code>\"complete\"</code> - Step has finished successfully</li> <li> <p><code>\"error\"</code> - Step encountered an error</p> </li> <li> <p><code>data</code> (dict or None): Step-specific data</p> </li> <li>Available when <code>status == \"complete\"</code></li> <li> <p>Contains results from the step (constraints, solutions, etc.)</p> </li> <li> <p><code>error</code> (str, optional): Error message</p> </li> <li>Only present when <code>status == \"error\"</code></li> </ul>"},{"location":"streaming_guide/#common-patterns","title":"Common Patterns","text":""},{"location":"streaming_guide/#pattern-1-progress-tracking","title":"Pattern 1: Progress Tracking","text":"<p>Track overall progress as PlanGEN processes your problem:</p> <pre><code>steps = [\n    \"extract_constraints\",\n    \"generate_solutions\", \n    \"verify_solutions\",\n    \"select_solution\"\n]\n\nprogress = {step: \"pending\" for step in steps}\n\nfor update in plangen.solve_stream(problem):\n    step = update[\"step\"]\n    if step in progress:\n        progress[step] = update[\"status\"]\n\n    # Calculate percentage\n    completed = sum(1 for s in progress.values() if s == \"complete\")\n    percentage = (completed / len(steps)) * 100\n    print(f\"Progress: {percentage:.0f}%\")\n</code></pre>"},{"location":"streaming_guide/#pattern-2-collecting-intermediate-results","title":"Pattern 2: Collecting Intermediate Results","text":"<p>Collect all intermediate results for analysis or debugging:</p> <pre><code>results = {\n    \"constraints\": None,\n    \"solutions\": [],\n    \"verification_results\": [],\n    \"selected_solution\": None,\n}\n\nfor update in plangen.solve_stream(problem):\n    if update[\"status\"] == \"complete\":\n        data = update.get(\"data\", {})\n\n        # Extract relevant data\n        if \"constraints\" in data:\n            results[\"constraints\"] = data[\"constraints\"]\n        if \"solutions\" in data:\n            results[\"solutions\"] = data[\"solutions\"]\n        if \"verification_results\" in data:\n            results[\"verification_results\"] = data[\"verification_results\"]\n        if \"selected_solution\" in data:\n            results[\"selected_solution\"] = data[\"selected_solution\"]\n\n# Now analyze all intermediate results\nprint(f\"Found {len(results['solutions'])} candidate solutions\")\n</code></pre>"},{"location":"streaming_guide/#pattern-3-real-time-ui-updates","title":"Pattern 3: Real-time UI Updates","text":"<p>Update a user interface in real-time as processing occurs:</p> <pre><code>def update_ui(step: str, status: str, data: dict):\n    \"\"\"Update UI based on streaming updates.\"\"\"\n    if status == \"in_progress\":\n        show_spinner(f\"Processing {step}...\")\n    elif status == \"complete\":\n        hide_spinner()\n        if step == \"extract_constraints\":\n            display_constraints(data[\"constraints\"])\n        elif step == \"generate_solutions\":\n            display_solutions(data[\"solutions\"])\n        elif step == \"select_solution\":\n            display_final_result(data[\"selected_solution\"])\n\nfor update in plangen.solve_stream(problem):\n    update_ui(update[\"step\"], update[\"status\"], update.get(\"data\", {}))\n</code></pre>"},{"location":"streaming_guide/#pattern-4-early-termination","title":"Pattern 4: Early Termination","text":"<p>Stop processing early if a condition is met:</p> <pre><code>for update in plangen.solve_stream(problem):\n    if update[\"status\"] == \"error\":\n        # Handle error and stop\n        handle_error(update[\"error\"])\n        break\n\n    if update[\"step\"] == \"generate_solutions\" and update[\"status\"] == \"complete\":\n        # Check if we got enough solutions\n        solutions = update[\"data\"][\"solutions\"]\n        if len(solutions) &lt; 2:\n            print(\"Not enough solutions generated, stopping\")\n            break\n</code></pre>"},{"location":"streaming_guide/#pattern-5-logging-and-monitoring","title":"Pattern 5: Logging and Monitoring","text":"<p>Log all steps for debugging or monitoring:</p> <pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\nfor update in plangen.solve_stream(problem):\n    step = update[\"step\"]\n    status = update[\"status\"]\n\n    if status == \"in_progress\":\n        logger.info(f\"Started {step}\")\n    elif status == \"complete\":\n        logger.info(f\"Completed {step}\")\n        logger.debug(f\"Data: {update.get('data', {})}\")\n    elif status == \"error\":\n        logger.error(f\"Error in {step}: {update['error']}\")\n</code></pre>"},{"location":"streaming_guide/#model-level-streaming","title":"Model-Level Streaming","text":"<p>For more advanced use cases, models also support streaming at the generation level:</p> <pre><code>from plangen.models import OpenAIModelInterface\n\nmodel = OpenAIModelInterface(model_name=\"gpt-4o\")\n\n# Stream text generation\nfor chunk in model.generate_stream(\"Your prompt here\"):\n    print(chunk, end=\"\", flush=True)\n</code></pre> <p>This is useful when:</p> <ul> <li>Building custom workflows</li> <li>Displaying token-by-token generation to users</li> <li>Implementing custom agents with streaming</li> </ul>"},{"location":"streaming_guide/#comparison-with-non-streaming","title":"Comparison with Non-Streaming","text":""},{"location":"streaming_guide/#non-streaming-traditional","title":"Non-Streaming (Traditional)","text":"<pre><code>result = plangen.solve(problem)\n# Wait for entire process to complete\nprint(result[\"selected_solution\"])\n</code></pre> <p>Pros:</p> <ul> <li>Simpler API</li> <li>Get complete result in one call</li> <li>Better for batch processing</li> </ul> <p>Cons:</p> <ul> <li>No progress feedback</li> <li>Can't access intermediate results</li> <li>Must wait for entire workflow</li> </ul>"},{"location":"streaming_guide/#streaming","title":"Streaming","text":"<pre><code>for update in plangen.solve_stream(problem):\n    # Process updates in real-time\n    pass\n</code></pre> <p>Pros:</p> <ul> <li>Real-time progress updates</li> <li>Access to intermediate results</li> <li>Better user experience</li> <li>Can implement early termination</li> </ul> <p>Cons:</p> <ul> <li>More complex to use</li> <li>Requires iteration handling</li> <li>Must track state yourself</li> </ul>"},{"location":"streaming_guide/#best-practices","title":"Best Practices","text":"<ol> <li>Always handle errors: Check for <code>status == \"error\"</code> and handle appropriately</li> <li>Use progress tracking: Provide feedback to users on long-running operations</li> <li>Collect what you need: Only store intermediate results if you actually need them</li> <li>Don't block the event loop: In async contexts, use appropriate patterns</li> <li>Log streaming updates: For production systems, log all steps for debugging</li> </ol>"},{"location":"streaming_guide/#error-handling","title":"Error Handling","text":"<p>Errors can occur at any step. Always check the status:</p> <pre><code>try:\n    for update in plangen.solve_stream(problem):\n        if update[\"status\"] == \"error\":\n            step = update[\"step\"]\n            error = update[\"error\"]\n            print(f\"Error in {step}: {error}\")\n\n            # Decide how to handle based on step\n            if step == \"extract_constraints\":\n                # Constraint extraction failed\n                # Maybe retry with a simpler problem statement\n                pass\n            elif step == \"generate_solutions\":\n                # Solution generation failed\n                # Maybe reduce num_solutions\n                pass\n\n            break  # Stop processing\n\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"streaming_guide/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Streaming overhead: Streaming adds minimal overhead compared to non-streaming</li> <li>Memory usage: Streaming allows processing without keeping all data in memory</li> <li>Latency: First update comes faster than waiting for complete result</li> <li>Concurrency: Streaming doesn't affect underlying model concurrency</li> </ul>"},{"location":"streaming_guide/#examples","title":"Examples","text":"<p>See the <code>examples/streaming_example.py</code> file for complete working examples:</p> <pre><code>python examples/streaming_example.py\n</code></pre>"},{"location":"streaming_guide/#integration-with-web-frameworks","title":"Integration with Web Frameworks","text":""},{"location":"streaming_guide/#fastapi-example","title":"FastAPI Example","text":"<pre><code>from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nimport json\n\napp = FastAPI()\n\n@app.post(\"/solve-stream\")\nasync def solve_stream(problem: str):\n    plangen = PlanGen.create(model=\"gpt-4o\")\n\n    def generate():\n        for update in plangen.solve_stream(problem):\n            # Stream as JSON lines\n            yield json.dumps(update) + \"\\n\"\n\n    return StreamingResponse(generate(), media_type=\"application/x-ndjson\")\n</code></pre>"},{"location":"streaming_guide/#flask-example","title":"Flask Example","text":"<pre><code>from flask import Flask, Response, request\nimport json\n\napp = Flask(__name__)\n\n@app.route(\"/solve-stream\", methods=[\"POST\"])\ndef solve_stream():\n    problem = request.json[\"problem\"]\n    plangen = PlanGen.create(model=\"gpt-4o\")\n\n    def generate():\n        for update in plangen.solve_stream(problem):\n            yield f\"data: {json.dumps(update)}\\n\\n\"\n\n    return Response(generate(), mimetype=\"text/event-stream\")\n</code></pre>"},{"location":"streaming_guide/#future-enhancements","title":"Future Enhancements","text":"<p>Planned improvements to streaming support:</p> <ul> <li>Async streaming: <code>async for update in plangen.solve_stream_async(problem)</code></li> <li>Token-level streaming: Stream LLM tokens for each step</li> <li>Custom callbacks: Register callbacks for specific steps</li> <li>Cancellation: Cancel streaming mid-process</li> <li>Backpressure: Handle slow consumers gracefully</li> </ul>"},{"location":"streaming_guide/#see-also","title":"See Also","text":"<ul> <li>API Reference - Complete API documentation</li> <li>User Guide - General usage guide</li> <li>Examples - More code examples</li> </ul>"},{"location":"algorithm_reference/","title":"Algorithm Reference","text":"<p>This section provides detailed information about the planning algorithms available in PlanGEN.</p>"},{"location":"algorithm_reference/#available-algorithms","title":"Available Algorithms","text":"<ul> <li>BestOfN - Generates multiple plans and selects the best one based on verification scores</li> <li>TreeOfThought - Explores multiple reasoning paths in a tree structure, allowing for backtracking</li> <li>REBASE - Uses recursive refinement to improve plans through iterative feedback</li> <li>MixtureOfAlgorithms - Dynamically selects the best algorithm for the problem</li> </ul>"},{"location":"algorithm_reference/#algorithm-comparison","title":"Algorithm Comparison","text":"Algorithm When to Use Pros Cons BestOfN Quick solutions, diverse options Simple, parallelizable Limited exploration TreeOfThought Complex reasoning, step-by-step planning Structured exploration, backtracking Higher computational cost REBASE When refinement is needed Iterative improvement More API calls, higher cost MixtureOfAlgorithms When uncertain which algorithm is best Adaptability Overhead from algorithm switching"},{"location":"algorithm_reference/algorithm_selection_guide/","title":"Algorithm Selection Guide","text":"<p>This guide helps you choose the right planning algorithm for your problem. PlanGEN offers four main algorithms, each with different strengths and use cases.</p>"},{"location":"algorithm_reference/algorithm_selection_guide/#quick-reference","title":"Quick Reference","text":"Algorithm Best For Speed Quality Complexity BestOfN Simple problems, quick results Fast Good Low TreeOfThought Complex reasoning, exploration Medium Excellent Medium REBASE Iterative refinement Slow Excellent High Mixture Automatic selection Varies Excellent Low (auto)"},{"location":"algorithm_reference/algorithm_selection_guide/#decision-tree","title":"Decision Tree","text":"<pre><code>flowchart TD\n    A[Start Here] --&gt; B{Need automatic&lt;br/&gt;algorithm selection?}\n    B --&gt;|YES| C[Use MixtureOfAlgorithms]\n    B --&gt;|NO| D{Is the problem&lt;br/&gt;simple/straightforward?}\n    D --&gt;|YES| E[Use BestOfN]\n    D --&gt;|NO| F{Need to explore multiple&lt;br/&gt;reasoning paths?}\n    F --&gt;|YES| G[Use TreeOfThought]\n    F --&gt;|NO| H{Need iterative&lt;br/&gt;refinement?}\n    H --&gt;|YES| I[Use REBASE]\n    H --&gt;|NO| E\n</code></pre>"},{"location":"algorithm_reference/algorithm_selection_guide/#algorithm-comparison","title":"Algorithm Comparison","text":""},{"location":"algorithm_reference/algorithm_selection_guide/#bestofn","title":"BestOfN","text":"<p>How it works:</p> <ol> <li>Generate N independent solutions</li> <li>Evaluate each solution</li> <li>Select the best one</li> </ol> <p>Strengths:</p> <ul> <li>Simple and fast</li> <li>Highly parallelizable</li> <li>Good for diverse solution space</li> <li>Low complexity</li> </ul> <p>Weaknesses:</p> <ul> <li>No interaction between solutions</li> <li>No refinement</li> <li>May require many samples for complex problems</li> </ul> <p>Best for:</p> <ul> <li>Simple to moderate problems</li> <li>When speed is important</li> <li>When you want diverse solutions</li> <li>Prototyping and testing</li> </ul> <p>Example:</p> <pre><code>result = plangen.solve(\n    problem,\n    algorithm=\"best_of_n\",\n    n_plans=5,\n    parallel=True\n)\n</code></pre>"},{"location":"algorithm_reference/algorithm_selection_guide/#treeofthought","title":"TreeOfThought","text":"<p>How it works:</p> <ol> <li>Start with initial reasoning</li> <li>Branch into multiple paths</li> <li>Evaluate and prune branches</li> <li>Explore promising paths deeper</li> <li>Select best final solution</li> </ol> <p>Strengths:</p> <ul> <li>Explores multiple reasoning paths</li> <li>Can backtrack from dead ends</li> <li>Good for complex problems</li> <li>Structured exploration</li> </ul> <p>Weaknesses:</p> <ul> <li>Slower than BestOfN</li> <li>More API calls</li> <li>Can be overkill for simple problems</li> </ul> <p>Best for:</p> <ul> <li>Complex reasoning tasks</li> <li>Problems with multiple approaches</li> <li>When exploration is valuable</li> <li>Multi-step problems</li> </ul> <p>Example:</p> <pre><code>result = plangen.solve(\n    problem,\n    algorithm=\"tree_of_thought\",\n    branching_factor=3,\n    max_depth=5,\n    beam_width=2\n)\n</code></pre>"},{"location":"algorithm_reference/algorithm_selection_guide/#rebase","title":"REBASE","text":"<p>How it works:</p> <ol> <li>Generate initial solution</li> <li>Verify and get feedback</li> <li>Refine based on feedback</li> <li>Repeat until convergence or max iterations</li> </ol> <p>Strengths:</p> <ul> <li>Iterative improvement</li> <li>Learns from mistakes</li> <li>Can achieve high quality</li> <li>Good for optimization</li> </ul> <p>Weaknesses:</p> <ul> <li>Slowest algorithm</li> <li>Most API calls</li> <li>May get stuck in local optima</li> </ul> <p>Best for:</p> <ul> <li>Problems needing refinement</li> <li>When quality is critical</li> <li>Optimization problems</li> <li>When initial solutions are incomplete</li> </ul> <p>Example:</p> <pre><code>result = plangen.solve(\n    problem,\n    algorithm=\"rebase\",\n    max_iterations=10,\n    improvement_threshold=0.1\n)\n</code></pre>"},{"location":"algorithm_reference/algorithm_selection_guide/#mixtureofalgorithms","title":"MixtureOfAlgorithms","text":"<p>How it works:</p> <ol> <li>Analyze problem characteristics</li> <li>Select appropriate algorithm</li> <li>Can switch algorithms mid-execution</li> <li>Returns best result</li> </ol> <p>Strengths:</p> <ul> <li>Automatic selection</li> <li>Adapts to problem</li> <li>Often best performance</li> <li>No tuning needed</li> </ul> <p>Weaknesses:</p> <ul> <li>Less control</li> <li>Harder to debug</li> <li>Overhead of selection</li> </ul> <p>Best for:</p> <ul> <li>Unknown problem types</li> <li>Production systems</li> <li>When you want \"best\" automatically</li> <li>Diverse problem sets</li> </ul> <p>Example:</p> <pre><code>result = plangen.solve(\n    problem,\n    algorithm=\"mixture\"\n)\n</code></pre>"},{"location":"algorithm_reference/algorithm_selection_guide/#use-case-examples","title":"Use Case Examples","text":""},{"location":"algorithm_reference/algorithm_selection_guide/#calendar-scheduling","title":"Calendar Scheduling","text":"<p>Recommended: BestOfN</p> <p>Scheduling is usually straightforward with clear constraints. Multiple diverse solutions help explore different time slots.</p> <pre><code>result = plangen.solve(\n    \"Schedule a 30-min meeting for 3 people...\",\n    algorithm=\"best_of_n\",\n    n_plans=5\n)\n</code></pre>"},{"location":"algorithm_reference/algorithm_selection_guide/#algorithm-design","title":"Algorithm Design","text":"<p>Recommended: TreeOfThought</p> <p>Algorithm design benefits from exploring different approaches and backtracking from inefficient solutions.</p> <pre><code>result = plangen.solve(\n    \"Design an algorithm to find the kth largest element...\",\n    algorithm=\"tree_of_thought\",\n    branching_factor=3,\n    max_depth=4\n)\n</code></pre>"},{"location":"algorithm_reference/algorithm_selection_guide/#mathematical-proofs","title":"Mathematical Proofs","text":"<p>Recommended: REBASE</p> <p>Proofs often need refinement to fix errors and improve clarity through iteration.</p> <pre><code>result = plangen.solve(\n    \"Prove that the sum of first n natural numbers is n(n+1)/2\",\n    algorithm=\"rebase\",\n    max_iterations=5\n)\n</code></pre>"},{"location":"algorithm_reference/algorithm_selection_guide/#creative-writing","title":"Creative Writing","text":"<p>Recommended: TreeOfThought or BestOfN</p> <p>Creative tasks benefit from exploration (TreeOfThought) or diverse options (BestOfN).</p> <pre><code># Exploration-focused\nresult = plangen.solve(\n    \"Write a short story about...\",\n    algorithm=\"tree_of_thought\"\n)\n\n# Diversity-focused\nresult = plangen.solve(\n    \"Write a short story about...\",\n    algorithm=\"best_of_n\",\n    n_plans=10,\n    sampling_strategy=\"diverse\"\n)\n</code></pre>"},{"location":"algorithm_reference/algorithm_selection_guide/#code-generation","title":"Code Generation","text":"<p>Recommended: REBASE</p> <p>Code often needs debugging and refinement based on test failures.</p> <pre><code>result = plangen.solve(\n    \"Write a function to sort a list...\",\n    algorithm=\"rebase\",\n    max_iterations=5\n)\n</code></pre>"},{"location":"algorithm_reference/algorithm_selection_guide/#planning-tasks","title":"Planning Tasks","text":"<p>Recommended: TreeOfThought</p> <p>Planning benefits from considering multiple paths and contingencies.</p> <pre><code>result = plangen.solve(\n    \"Plan a 7-day trip to Japan...\",\n    algorithm=\"tree_of_thought\",\n    branching_factor=4\n)\n</code></pre>"},{"location":"algorithm_reference/algorithm_selection_guide/#performance-considerations","title":"Performance Considerations","text":""},{"location":"algorithm_reference/algorithm_selection_guide/#api-calls","title":"API Calls","text":"<p>Approximate API calls per problem:</p> <ul> <li>BestOfN: <code>2 * n_plans</code> (generation + verification)</li> <li>TreeOfThought: <code>branching_factor * max_depth * 2</code></li> <li>REBASE: <code>2 * max_iterations</code></li> <li>Mixture: Varies (typically like BestOfN)</li> </ul>"},{"location":"algorithm_reference/algorithm_selection_guide/#time-complexity","title":"Time Complexity","text":"<p>From fastest to slowest:</p> <ol> <li>BestOfN (especially with parallel=True)</li> <li>Mixture</li> <li>TreeOfThought</li> <li>REBASE</li> </ol>"},{"location":"algorithm_reference/algorithm_selection_guide/#cost-considerations","title":"Cost Considerations","text":"<p>Consider both API costs and latency:</p> <pre><code># Most cost-effective for simple problems\nresult = plangen.solve(problem, algorithm=\"best_of_n\", n_plans=3)\n\n# Medium cost, good quality\nresult = plangen.solve(problem, algorithm=\"mixture\")\n\n# Higher cost but best quality\nresult = plangen.solve(problem, algorithm=\"rebase\", max_iterations=10)\n</code></pre>"},{"location":"algorithm_reference/algorithm_selection_guide/#tuning-parameters","title":"Tuning Parameters","text":""},{"location":"algorithm_reference/algorithm_selection_guide/#bestofn-parameters","title":"BestOfN Parameters","text":"<pre><code># Quick and cheap\nn_plans=3, parallel=True\n\n# Balanced\nn_plans=5, sampling_strategy=\"diverse\", parallel=True\n\n# High quality\nn_plans=10, sampling_strategy=\"adaptive\", parallel=True\n</code></pre>"},{"location":"algorithm_reference/algorithm_selection_guide/#treeofthought-parameters","title":"TreeOfThought Parameters","text":"<pre><code># Quick exploration\nbranching_factor=2, max_depth=3, beam_width=1\n\n# Balanced\nbranching_factor=3, max_depth=5, beam_width=2\n\n# Deep exploration\nbranching_factor=4, max_depth=7, beam_width=3\n</code></pre>"},{"location":"algorithm_reference/algorithm_selection_guide/#rebase-parameters","title":"REBASE Parameters","text":"<pre><code># Quick refinement\nmax_iterations=3, improvement_threshold=0.2\n\n# Balanced\nmax_iterations=5, improvement_threshold=0.1\n\n# Thorough refinement\nmax_iterations=10, improvement_threshold=0.05\n</code></pre>"},{"location":"algorithm_reference/algorithm_selection_guide/#combining-algorithms","title":"Combining Algorithms","text":"<p>You can use different algorithms for different stages:</p> <pre><code># Generate diverse initial solutions with BestOfN\nfrom plangen.algorithms import BestOfN\nbest_of_n = BestOfN(n_plans=5, llm_interface=model)\ninitial_plans, scores, _ = best_of_n.run(problem)\n\n# Refine best plan with REBASE\nfrom plangen.algorithms import REBASE\nrebase = REBASE(max_iterations=5, llm_interface=model)\nrefined_plan, final_score, _ = rebase.run(\n    problem,\n    initial_solution=initial_plans[0]\n)\n</code></pre>"},{"location":"algorithm_reference/algorithm_selection_guide/#debugging-and-visualization","title":"Debugging and Visualization","text":"<p>Enable visualization to understand algorithm behavior:</p> <pre><code>from plangen.visualization import GraphRenderer\nfrom plangen.algorithms import TreeOfThought\n\n# Create visualizer\nrenderer = GraphRenderer(output_dir=\"./viz\")\n\n# Create algorithm and add visualizer\ntot = TreeOfThought(branching_factor=3, max_depth=5, llm_interface=model)\ntot.add_observer(renderer)\n\n# Run and visualize\nresult, score, metadata = tot.run(problem)\n# Check ./viz directory for visualizations\n</code></pre>"},{"location":"algorithm_reference/algorithm_selection_guide/#recommendations-by-problem-type","title":"Recommendations by Problem Type","text":"Problem Type Primary Choice Alternative Why Scheduling BestOfN Mixture Fast, multiple options Algorithm Design TreeOfThought REBASE Need exploration Math Problems REBASE TreeOfThought Need refinement Code Generation REBASE TreeOfThought Iterative debugging Creative Tasks TreeOfThought BestOfN Multiple approaches Optimization REBASE TreeOfThought Iterative improvement Planning TreeOfThought BestOfN Consider alternatives Q&amp;A BestOfN Mixture Quick answers Unknown Mixture BestOfN Auto-selection"},{"location":"algorithm_reference/algorithm_selection_guide/#starting-point","title":"Starting Point","text":"<p>If you're unsure, start with these defaults:</p> <pre><code># For most problems\nresult = plangen.solve(problem, algorithm=\"mixture\")\n\n# If mixture is too slow\nresult = plangen.solve(problem, algorithm=\"best_of_n\", n_plans=5)\n\n# If quality is critical and time allows\nresult = plangen.solve(problem, algorithm=\"rebase\", max_iterations=5)\n</code></pre>"},{"location":"algorithm_reference/algorithm_selection_guide/#next-steps","title":"Next Steps","text":"<ul> <li>Read detailed algorithm references: BestOfN, TreeOfThought, REBASE, Mixture</li> <li>See Examples for practical use cases</li> <li>Check Visualization for debugging tools</li> </ul>"},{"location":"algorithm_reference/best_of_n/","title":"BestOfN","text":"<p>The BestOfN algorithm is one of the simplest and most effective planning algorithms in PlanGEN. It works by generating multiple plans (solutions) independently and selecting the best one based on verification scores.</p>"},{"location":"algorithm_reference/best_of_n/#overview","title":"Overview","text":"<ol> <li>Generate N plans for solving the problem</li> <li>Verify each plan against the constraints</li> <li>Select the best plan based on verification scores</li> </ol>"},{"location":"algorithm_reference/best_of_n/#when-to-use","title":"When to Use","text":"<p>BestOfN is well-suited for:</p> <ul> <li>Simple to moderately complex problems</li> <li>When you want to explore diverse solutions</li> <li>When you need a quick solution without complex reasoning paths</li> <li>When parallelization is available (to speed up generation)</li> </ul>"},{"location":"algorithm_reference/best_of_n/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>n_plans</code> int 5 Number of plans to generate <code>sampling_strategy</code> str \"diverse\" Strategy for generating plans (\"diverse\" or \"adaptive\") <code>parallel</code> bool True Whether to generate plans in parallel <code>llm_interface</code> ModelProtocol None The language model interface to use"},{"location":"algorithm_reference/best_of_n/#sampling-strategies","title":"Sampling Strategies","text":""},{"location":"algorithm_reference/best_of_n/#diverse","title":"Diverse","text":"<p>The \"diverse\" sampling strategy aims to generate a diverse set of plans by increasing the temperature and encouraging the model to explore different approaches.</p>"},{"location":"algorithm_reference/best_of_n/#adaptive","title":"Adaptive","text":"<p>The \"adaptive\" sampling strategy adjusts the plan generation based on previous verification results, learning from successful and unsuccessful plans to improve subsequent plans.</p>"},{"location":"algorithm_reference/best_of_n/#usage-example","title":"Usage Example","text":""},{"location":"algorithm_reference/best_of_n/#with-public-api","title":"With Public API","text":"<pre><code>from plangen import PlanGen\n\n# Create a PlanGen instance\nplangen = PlanGen.create()\n\n# Solve a problem using BestOfN\nresult = plangen.solve(\n    problem=\"Schedule a meeting for 3 people with the following constraints...\",\n    algorithm=\"best_of_n\",\n    n_plans=5,\n    sampling_strategy=\"diverse\",\n    parallel=True\n)\n\n# Access the selected solution\nselected_solution = result[\"selected_solution\"]\n</code></pre>"},{"location":"algorithm_reference/best_of_n/#with-algorithm-class","title":"With Algorithm Class","text":"<pre><code>from plangen import Algorithm, PlanGen\n\n# Create a PlanGen instance\nplangen = PlanGen.create()\n\n# Create a BestOfN algorithm instance\nbest_of_n = Algorithm.create(\n    algorithm_type=\"best_of_n\",\n    model=plangen._plangen.model,\n    n_plans=5,\n    sampling_strategy=\"diverse\",\n    parallel=True\n)\n\n# Run the algorithm\nproblem = \"Schedule a meeting for 3 people with the following constraints...\"\nbest_plan, score, metadata = best_of_n.run(problem)\n\nprint(f\"Best plan (score {score}):\\n{best_plan}\")\n</code></pre>"},{"location":"algorithm_reference/best_of_n/#advanced-configuration","title":"Advanced Configuration","text":"<p>For more advanced configuration, you can use the internal API:</p> <pre><code>from plangen.algorithms import BestOfN\nfrom plangen.models import OpenAIModelInterface\n\n# Create a model interface\nmodel = OpenAIModelInterface(model_name=\"gpt-4o\")\n\n# Create a BestOfN algorithm instance with custom parameters\nbest_of_n = BestOfN(\n    n_plans=10,\n    sampling_strategy=\"adaptive\",\n    parallel=True,\n    llm_interface=model,\n    temperature_range=(0.5, 0.9),\n    max_retries=3\n)\n\n# Run the algorithm\nproblem = \"Schedule a meeting for 3 people with the following constraints...\"\nbest_plan, score, metadata = best_of_n.run(problem)\n</code></pre>"},{"location":"algorithm_reference/best_of_n/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Time Complexity: O(N) where N is the number of plans</li> <li>Space Complexity: O(N) where N is the number of plans</li> <li>API Calls: 2N API calls (N for generation, N for verification)</li> <li>Parallelization: Can be parallelized for faster generation</li> </ul>"},{"location":"algorithm_reference/best_of_n/#advantages","title":"Advantages","text":"<ul> <li>Simple and effective</li> <li>Easy to understand and implement</li> <li>Highly parallelizable</li> <li>Flexible sampling strategies</li> </ul>"},{"location":"algorithm_reference/best_of_n/#limitations","title":"Limitations","text":"<ul> <li>No interaction between plans</li> <li>No refinement of plans</li> <li>Limited exploration of the solution space</li> <li>May require a large N for complex problems</li> </ul>"},{"location":"algorithm_reference/mixture_of_algorithms/","title":"Mixture of Algorithms","text":"<p>MixtureOfAlgorithms is a meta-algorithm that automatically selects and switches between different planning algorithms (BestOfN, TreeOfThought, REBASE) based on problem characteristics and intermediate results.</p>"},{"location":"algorithm_reference/mixture_of_algorithms/#overview","title":"Overview","text":"<p>MixtureOfAlgorithms acts as an intelligent orchestrator:</p> <ol> <li>Analyzes problem characteristics</li> <li>Selects the most appropriate algorithm</li> <li>Monitors performance during execution</li> <li>Can switch algorithms if needed</li> <li>Returns the best solution found</li> </ol>"},{"location":"algorithm_reference/mixture_of_algorithms/#how-it-works","title":"How It Works","text":"<pre><code>flowchart TD\n    A[Problem] --&gt; B[Analyze Characteristics]\n    B --&gt; C{Simple?}\n    C --&gt;|Yes| D[BestOfN]\n    C --&gt;|No| E{Need Exploration?}\n    E --&gt;|Yes| F[TreeOfThought]\n    E --&gt;|No| G{Need Refinement?}\n    G --&gt;|Yes| H[REBASE]\n    G --&gt;|No| D\n    D --&gt; I[Execute Selected Algorithm]\n    F --&gt; I\n    H --&gt; I\n    I --&gt; J[Monitor Results]\n    J --&gt; K{Poor Results?}\n    K --&gt;|Yes| L[Switch Algorithm]\n    L --&gt; B\n    K --&gt;|No| M[Return Solution]\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#when-to-use","title":"When to Use","text":"<p>MixtureOfAlgorithms is ideal for:</p> <ul> <li>Unknown problem types where the best approach isn't clear</li> <li>Production systems needing automatic optimization</li> <li>Diverse problem sets with varying characteristics</li> <li>Prototyping to find the best algorithm for later optimization</li> </ul> <p>Best for:</p> <ul> <li>Applications with varied problems</li> <li>When you want optimal results automatically</li> <li>Rapid development and testing</li> </ul>"},{"location":"algorithm_reference/mixture_of_algorithms/#parameters","title":"Parameters","text":""},{"location":"algorithm_reference/mixture_of_algorithms/#core-parameters","title":"Core Parameters","text":"Parameter Type Default Description <code>max_algorithm_switches</code> int 2 Maximum number of algorithm changes <code>llm_interface</code> ModelProtocol Required Language model interface"},{"location":"algorithm_reference/mixture_of_algorithms/#advanced-parameters","title":"Advanced Parameters","text":"Parameter Type Default Description <code>selection_strategy</code> str \"adaptive\" How to select algorithm <code>performance_threshold</code> float 0.6 Minimum acceptable score <code>enable_switching</code> bool True Allow mid-execution switches"},{"location":"algorithm_reference/mixture_of_algorithms/#usage-examples","title":"Usage Examples","text":""},{"location":"algorithm_reference/mixture_of_algorithms/#basic-usage","title":"Basic Usage","text":"<pre><code>from plangen import PlanGen\n\nplangen = PlanGen.create()\n\n# Automatically selects best algorithm\nresult = plangen.solve(\n    problem=\"Your problem here\",\n    algorithm=\"mixture\"\n)\n\nprint(result[\"selected_solution\"])\nprint(f\"Used algorithm: {result['metadata']['algorithm_used']}\")\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#with-algorithm-class","title":"With Algorithm Class","text":"<pre><code>from plangen.algorithms import MixtureOfAlgorithms\nfrom plangen.models import OpenAIModelInterface\n\n# Create model\nmodel = OpenAIModelInterface(model_name=\"gpt-4o\")\n\n# Create mixture algorithm\nmixture = MixtureOfAlgorithms(\n    max_algorithm_switches=3,\n    llm_interface=model\n)\n\n# Run on problem\nproblem = \"Design a distributed caching system\"\nsolution, score, metadata = mixture.run(problem)\n\nprint(f\"Solution (score: {score}):\\n{solution}\")\nprint(f\"\\nAlgorithm path: {metadata['algorithm_path']}\")\nprint(f\"Switches made: {metadata['num_switches']}\")\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#with-custom-parameters","title":"With Custom Parameters","text":"<pre><code>from plangen.algorithms import MixtureOfAlgorithms\n\nmixture = MixtureOfAlgorithms(\n    max_algorithm_switches=2,\n    performance_threshold=0.7,  # Switch if score &lt; 0.7\n    enable_switching=True,\n    llm_interface=model\n)\n\nsolution, score, metadata = mixture.run(problem)\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#selection-logic","title":"Selection Logic","text":""},{"location":"algorithm_reference/mixture_of_algorithms/#problem-characteristics","title":"Problem Characteristics","text":"<p>MixtureOfAlgorithms analyzes:</p> <ol> <li>Complexity: Simple vs. complex problems</li> <li>Structure: Single-step vs. multi-step</li> <li>Constraints: Number and type of constraints</li> <li>Domain: Recognized problem types</li> </ol>"},{"location":"algorithm_reference/mixture_of_algorithms/#algorithm-selection-rules","title":"Algorithm Selection Rules","text":"<pre><code>if problem.is_simple():\n    return BestOfN(n_plans=5)\nelif problem.needs_exploration():\n    return TreeOfThought(branching_factor=3, max_depth=5)\nelif problem.needs_refinement():\n    return REBASE(max_iterations=5)\nelse:\n    return BestOfN(n_plans=5)  # Default\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#switching-triggers","title":"Switching Triggers","text":"<p>Algorithm switching occurs when:</p> <ul> <li>Current algorithm produces low scores</li> <li>Better algorithm identified for problem type</li> <li>Stuck in local optimum</li> <li>Maximum iterations reached without good result</li> </ul>"},{"location":"algorithm_reference/mixture_of_algorithms/#use-case-examples","title":"Use Case Examples","text":""},{"location":"algorithm_reference/mixture_of_algorithms/#unknown-problem-type","title":"Unknown Problem Type","text":"<pre><code># Let MixtureOfAlgorithms choose\nmixture = MixtureOfAlgorithms(llm_interface=model)\n\nproblems = [\n    \"Schedule a meeting...\",  # \u2192 Likely BestOfN\n    \"Design an algorithm...\", # \u2192 Likely TreeOfThought\n    \"Write and debug code...\", # \u2192 Likely REBASE\n]\n\nfor problem in problems:\n    solution, score, metadata = mixture.run(problem)\n    print(f\"Problem: {problem[:30]}...\")\n    print(f\"Used: {metadata['algorithm_used']}\")\n    print(f\"Score: {score}\\n\")\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#production-system","title":"Production System","text":"<pre><code># Handle diverse user queries\nmixture = MixtureOfAlgorithms(\n    max_algorithm_switches=2,\n    performance_threshold=0.75,\n    llm_interface=model\n)\n\ndef solve_user_problem(user_input):\n    \"\"\"Automatically handles any problem type.\"\"\"\n    solution, score, metadata = mixture.run(user_input)\n\n    return {\n        'solution': solution,\n        'confidence': score,\n        'method': metadata['algorithm_used'],\n        'switches': metadata.get('num_switches', 0)\n    }\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#benchmarking","title":"Benchmarking","text":"<pre><code># Test all algorithms on same problem\nfrom plangen.algorithms import BestOfN, TreeOfThought, REBASE\n\nproblem = \"Your test problem\"\n\n# Manual testing\nalgorithms = {\n    'BestOfN': BestOfN(n_plans=5, llm_interface=model),\n    'TreeOfThought': TreeOfThought(llm_interface=model),\n    'REBASE': REBASE(llm_interface=model),\n}\n\nresults = {}\nfor name, alg in algorithms.items():\n    solution, score, metadata = alg.run(problem)\n    results[name] = score\n\n# Automatic selection\nmixture = MixtureOfAlgorithms(llm_interface=model)\nsolution, score, metadata = mixture.run(problem)\n\nprint(\"Manual results:\", results)\nprint(f\"Mixture selected: {metadata['algorithm_used']} (score: {score})\")\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#understanding-the-output","title":"Understanding the Output","text":""},{"location":"algorithm_reference/mixture_of_algorithms/#metadata-structure","title":"Metadata Structure","text":"<pre><code>metadata = {\n    'algorithm': 'mixture_of_algorithms',\n    'algorithm_used': 'tree_of_thought',  # Final algorithm\n    'algorithm_path': [\n        {'algorithm': 'best_of_n', 'score': 0.65, 'reason': 'initial_selection'},\n        {'algorithm': 'tree_of_thought', 'score': 0.89, 'reason': 'poor_initial_score'}\n    ],\n    'num_switches': 1,\n    'selection_reasoning': 'Problem requires multi-step reasoning',\n    'total_time': 45.2,\n    'time_by_algorithm': {\n        'best_of_n': 15.3,\n        'tree_of_thought': 29.9\n    }\n}\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#algorithm-path","title":"Algorithm Path","text":"<p>The <code>algorithm_path</code> shows the sequence of algorithms used:</p> <pre><code>[\n    {'algorithm': 'best_of_n', 'score': 0.6, 'reason': 'initial_selection'},\n    {'algorithm': 'rebase', 'score': 0.85, 'reason': 'needs_refinement'},\n    {'algorithm': 'rebase', 'score': 0.92, 'reason': 'continue_refining'}\n]\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"algorithm_reference/mixture_of_algorithms/#time-complexity","title":"Time Complexity","text":"<p>Depends on selected algorithm(s):</p> <ul> <li>Best case: Single BestOfN execution</li> <li>Average case: Single TreeOfThought or REBASE</li> <li>Worst case: Multiple algorithm switches</li> </ul>"},{"location":"algorithm_reference/mixture_of_algorithms/#api-calls","title":"API Calls","text":"<p>Variable based on:</p> <ul> <li>Number of algorithm switches</li> <li>Algorithms selected</li> <li>Problem complexity</li> </ul> <pre><code># Example cost calculation\nBestOfN: ~10 calls\nSwitch analysis: ~2 calls\nTreeOfThought: ~30 calls\nTotal with one switch: ~42 calls\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#comparison-with-direct-algorithm-use","title":"Comparison with Direct Algorithm Use","text":"Aspect Mixture Direct Algorithm Ease of use Very Easy Requires knowledge Performance Good Can be optimal Flexibility High Low Predictability Medium High Cost Variable Predictable Control Less More"},{"location":"algorithm_reference/mixture_of_algorithms/#advanced-features","title":"Advanced Features","text":""},{"location":"algorithm_reference/mixture_of_algorithms/#custom-selection-strategy","title":"Custom Selection Strategy","text":"<pre><code>class CustomMixture(MixtureOfAlgorithms):\n    def analyze_problem(self, problem):\n        \"\"\"Custom problem analysis.\"\"\"\n        analysis = super().analyze_problem(problem)\n\n        # Add custom heuristics\n        if 'schedule' in problem.lower():\n            analysis['recommended'] = 'best_of_n'\n        elif 'algorithm' in problem.lower():\n            analysis['recommended'] = 'tree_of_thought'\n        elif 'code' in problem.lower():\n            analysis['recommended'] = 'rebase'\n\n        return analysis\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#custom-switching-logic","title":"Custom Switching Logic","text":"<pre><code>class AdaptiveMixture(MixtureOfAlgorithms):\n    def should_switch_algorithm(self, current_algorithm, current_score, iteration):\n        \"\"\"Custom switching logic.\"\"\"\n        # Switch if score below threshold\n        if current_score &lt; self.performance_threshold:\n            return True\n\n        # Switch if not improving\n        if iteration &gt; 3 and current_score &lt; 0.7:\n            return True\n\n        # Don't switch if doing well\n        if current_score &gt;= 0.9:\n            return False\n\n        return super().should_switch_algorithm(\n            current_algorithm, current_score, iteration\n        )\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#best-practices","title":"Best Practices","text":""},{"location":"algorithm_reference/mixture_of_algorithms/#1-start-with-default-settings","title":"1. Start with Default Settings","text":"<pre><code># Good for most cases\nmixture = MixtureOfAlgorithms(llm_interface=model)\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#2-limit-algorithm-switches","title":"2. Limit Algorithm Switches","text":"<pre><code># Avoid excessive switching\nmixture = MixtureOfAlgorithms(\n    max_algorithm_switches=2,  # Usually sufficient\n    llm_interface=model\n)\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#3-set-performance-thresholds","title":"3. Set Performance Thresholds","text":"<pre><code># Define acceptable performance\nmixture = MixtureOfAlgorithms(\n    performance_threshold=0.75,  # Switch if below\n    llm_interface=model\n)\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#4-monitor-algorithm-selection","title":"4. Monitor Algorithm Selection","text":"<pre><code>result, score, metadata = mixture.run(problem)\n\n# Log selection for analysis\nprint(f\"Selected: {metadata['algorithm_used']}\")\nprint(f\"Reason: {metadata['selection_reasoning']}\")\nprint(f\"Switches: {metadata['num_switches']}\")\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#advantages","title":"Advantages","text":"<ol> <li>Automatic optimization: No manual algorithm selection</li> <li>Adaptive: Adjusts to problem characteristics</li> <li>Robust: Recovers from poor initial choices</li> <li>Simple API: Easy to use for beginners</li> <li>Production-ready: Handles diverse problems</li> </ol>"},{"location":"algorithm_reference/mixture_of_algorithms/#limitations","title":"Limitations","text":"<ol> <li>Less predictable: Performance varies by problem</li> <li>Higher cost: May use multiple algorithms</li> <li>Debugging harder: More complex execution path</li> <li>Overhead: Algorithm selection adds latency</li> <li>Less control: Automatic decisions may not be optimal</li> </ol>"},{"location":"algorithm_reference/mixture_of_algorithms/#when-to-use-direct-algorithms-instead","title":"When to Use Direct Algorithms Instead","text":"<p>Use specific algorithms when:</p> <ul> <li>Performance is critical: Know the best algorithm</li> <li>Cost matters: Need predictable API usage</li> <li>Debugging needed: Simpler execution path</li> <li>Domain-specific: Have expertise in problem type</li> <li>Benchmarking: Testing specific approaches</li> </ul>"},{"location":"algorithm_reference/mixture_of_algorithms/#troubleshooting","title":"Troubleshooting","text":""},{"location":"algorithm_reference/mixture_of_algorithms/#too-many-switches","title":"Too Many Switches","text":"<pre><code># Limit switches\nmixture = MixtureOfAlgorithms(\n    max_algorithm_switches=1,\n    llm_interface=model\n)\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#poor-algorithm-selection","title":"Poor Algorithm Selection","text":"<pre><code># Adjust threshold\nmixture = MixtureOfAlgorithms(\n    performance_threshold=0.7,  # More lenient\n    llm_interface=model\n)\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#high-cost","title":"High Cost","text":"<pre><code># Use simpler algorithms\nmixture = MixtureOfAlgorithms(\n    max_algorithm_switches=0,  # Disable switching\n    llm_interface=model\n)\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#example-complete-usage","title":"Example: Complete Usage","text":"<pre><code>from plangen import PlanGen\nfrom plangen.algorithms import MixtureOfAlgorithms\nfrom plangen.visualization import GraphRenderer\n\n# Create model and visualizer\nplangen = PlanGen.create()\nmodel = plangen._plangen.model\nrenderer = GraphRenderer(output_dir=\"./mixture_viz\")\n\n# Create mixture algorithm\nmixture = MixtureOfAlgorithms(\n    max_algorithm_switches=2,\n    performance_threshold=0.75,\n    llm_interface=model\n)\n\n# Add observer\nmixture.add_observer(renderer)\n\n# Solve problem\nproblem = \"Design a real-time chat application with 100k users\"\nsolution, score, metadata = mixture.run(problem)\n\n# Analyze results\nprint(f\"Solution (score {score}):\\n{solution}\")\nprint(f\"\\nAlgorithm journey:\")\nfor step in metadata['algorithm_path']:\n    print(f\"  - {step['algorithm']}: {step['score']:.2f} ({step['reason']})\")\n\nprint(f\"\\nTotal time: {metadata['total_time']:.1f}s\")\nprint(f\"Switches made: {metadata['num_switches']}\")\n</code></pre>"},{"location":"algorithm_reference/mixture_of_algorithms/#next-steps","title":"Next Steps","text":"<ul> <li>Read Algorithm Selection Guide for manual selection</li> <li>See BestOfN, TreeOfThought, REBASE for details on individual algorithms</li> <li>Check Examples for more use cases</li> </ul>"},{"location":"algorithm_reference/rebase/","title":"REBASE","text":"<p>REBASE (Recursive Refinement Based on Assessment and Selection Enhancement) is an iterative algorithm that generates an initial solution and continuously refines it based on verification feedback until convergence or maximum iterations.</p>"},{"location":"algorithm_reference/rebase/#overview","title":"Overview","text":"<p>REBASE follows an iterative improvement loop:</p> <ol> <li>Generate initial solution</li> <li>Verify and get feedback</li> <li>Refine based on feedback</li> <li>Repeat until improvement threshold or max iterations</li> </ol>"},{"location":"algorithm_reference/rebase/#how-it-works","title":"How It Works","text":"<pre><code>flowchart TD\n    A[Initial Solution] --&gt; B[Verify]\n    B --&gt; C[Score: 0.6]\n    C --&gt; D[Refine based on feedback]\n    D --&gt; E[Refined Solution]\n    E --&gt; F[Verify]\n    F --&gt; G[Score: 0.8]\n    G --&gt; H[Refine based on feedback]\n    H --&gt; I[Final Solution]\n    I --&gt; J[Verify]\n    J --&gt; K[Score: 0.95 \u2713]\n</code></pre>"},{"location":"algorithm_reference/rebase/#when-to-use","title":"When to Use","text":"<p>REBASE is ideal for:</p> <ul> <li>Problems requiring refinement through iteration</li> <li>Optimization tasks where incremental improvement is valuable</li> <li>Complex solutions that are unlikely to be perfect initially</li> <li>Quality-critical applications where time allows for refinement</li> </ul> <p>Avoid for:</p> <ul> <li>Simple problems solvable in one pass</li> <li>Time-sensitive applications</li> <li>Problems with binary correctness (either right or wrong)</li> </ul>"},{"location":"algorithm_reference/rebase/#parameters","title":"Parameters","text":""},{"location":"algorithm_reference/rebase/#core-parameters","title":"Core Parameters","text":"Parameter Type Default Description <code>max_iterations</code> int 5 Maximum refinement iterations <code>improvement_threshold</code> float 0.1 Minimum score improvement to continue <code>llm_interface</code> ModelProtocol Required Language model interface"},{"location":"algorithm_reference/rebase/#advanced-parameters","title":"Advanced Parameters","text":"Parameter Type Default Description <code>min_score</code> float 0.8 Target minimum score <code>patience</code> int 2 Iterations without improvement before stopping <code>feedback_format</code> str \"detailed\" Level of feedback detail"},{"location":"algorithm_reference/rebase/#usage-examples","title":"Usage Examples","text":""},{"location":"algorithm_reference/rebase/#basic-usage","title":"Basic Usage","text":"<pre><code>from plangen import PlanGen\n\nplangen = PlanGen.create()\n\nresult = plangen.solve(\n    problem=\"Write a Python function to implement binary search\",\n    algorithm=\"rebase\",\n    max_iterations=5,\n    improvement_threshold=0.1\n)\n\nprint(result[\"selected_solution\"])\n</code></pre>"},{"location":"algorithm_reference/rebase/#with-algorithm-class","title":"With Algorithm Class","text":"<pre><code>from plangen.algorithms import REBASE\nfrom plangen.models import OpenAIModelInterface\n\n# Create model\nmodel = OpenAIModelInterface(model_name=\"gpt-4o\")\n\n# Create algorithm\nrebase = REBASE(\n    max_iterations=10,\n    improvement_threshold=0.05,\n    llm_interface=model\n)\n\n# Run algorithm\nproblem = \"Prove that the sum of first n natural numbers is n(n+1)/2\"\nbest_plan, score, metadata = rebase.run(problem)\n\nprint(f\"Final solution (score: {score}):\\n{best_plan}\")\nprint(f\"\\nIterations: {metadata['iterations']}\")\nprint(f\"Score history: {metadata['score_history']}\")\n</code></pre>"},{"location":"algorithm_reference/rebase/#with-custom-initial-solution","title":"With Custom Initial Solution","text":"<pre><code>from plangen.algorithms import REBASE\n\n# Create algorithm\nrebase = REBASE(\n    max_iterations=5,\n    improvement_threshold=0.1,\n    llm_interface=model\n)\n\n# Provide initial solution\ninitial_solution = \"\"\"\ndef binary_search(arr, target):\n    left, right = 0, len(arr)\n    while left &lt; right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n    return -1\n\"\"\"\n\nproblem = \"Write a correct binary search implementation\"\nrefined_solution, score, metadata = rebase.run(\n    problem,\n    initial_solution=initial_solution\n)\n</code></pre>"},{"location":"algorithm_reference/rebase/#with-visualization","title":"With Visualization","text":"<pre><code>from plangen.algorithms import REBASE\nfrom plangen.visualization import GraphRenderer\n\n# Create visualizer\nrenderer = GraphRenderer(output_dir=\"./rebase_visualizations\")\n\n# Create algorithm\nrebase = REBASE(\n    max_iterations=10,\n    improvement_threshold=0.05,\n    llm_interface=model\n)\n\n# Add observer\nrebase.add_observer(renderer)\n\n# Run algorithm\nproblem = \"Design a cache eviction policy\"\nsolution, score, metadata = rebase.run(problem)\n\n# Check ./rebase_visualizations for:\n# - iteration_scores.png (score progression)\n# - feedback_summary.txt (feedback at each iteration)\n</code></pre>"},{"location":"algorithm_reference/rebase/#configuration-strategies","title":"Configuration Strategies","text":""},{"location":"algorithm_reference/rebase/#quick-refinement","title":"Quick Refinement","text":"<p>Good for: Simple improvements, time-sensitive tasks</p> <pre><code>rebase = REBASE(\n    max_iterations=3,\n    improvement_threshold=0.2,  # Stop after small improvement\n    llm_interface=model\n)\n</code></pre>"},{"location":"algorithm_reference/rebase/#thorough-refinement","title":"Thorough Refinement","text":"<p>Good for: Quality-critical tasks, complex problems</p> <pre><code>rebase = REBASE(\n    max_iterations=10,\n    improvement_threshold=0.05,  # Continue for small improvements\n    min_score=0.95,              # High quality target\n    llm_interface=model\n)\n</code></pre>"},{"location":"algorithm_reference/rebase/#balanced-refinement","title":"Balanced Refinement","text":"<p>Good for: Most general problems</p> <pre><code>rebase = REBASE(\n    max_iterations=5,\n    improvement_threshold=0.1,\n    llm_interface=model\n)\n</code></pre>"},{"location":"algorithm_reference/rebase/#use-case-examples","title":"Use Case Examples","text":""},{"location":"algorithm_reference/rebase/#code-generation-with-bug-fixes","title":"Code Generation with Bug Fixes","text":"<pre><code>problem = \"\"\"\nWrite a Python function to validate email addresses.\nHandle edge cases like:\n- Multiple @ symbols\n- Missing domain\n- Invalid characters\n- Empty strings\n\"\"\"\n\nrebase = REBASE(\n    max_iterations=7,\n    improvement_threshold=0.08,\n    llm_interface=model\n)\n\ncode, score, metadata = rebase.run(problem)\nprint(f\"Generated after {metadata['iterations']} iterations\")\n</code></pre>"},{"location":"algorithm_reference/rebase/#mathematical-proof-refinement","title":"Mathematical Proof Refinement","text":"<pre><code>problem = \"\"\"\nProve by induction that 1 + 2 + 3 + ... + n = n(n+1)/2\nInclude:\n- Base case\n- Inductive hypothesis\n- Inductive step\n- Conclusion\n\"\"\"\n\nrebase = REBASE(\n    max_iterations=5,\n    improvement_threshold=0.1,\n    llm_interface=model\n)\n\nproof, score, metadata = rebase.run(problem)\n</code></pre>"},{"location":"algorithm_reference/rebase/#essay-writing-and-editing","title":"Essay Writing and Editing","text":"<pre><code>problem = \"\"\"\nWrite a 500-word essay on the impact of AI on education.\nInclude:\n- Clear thesis statement\n- Supporting arguments\n- Examples\n- Conclusion\n\"\"\"\n\nrebase = REBASE(\n    max_iterations=6,\n    improvement_threshold=0.1,\n    llm_interface=model\n)\n\nessay, score, metadata = rebase.run(problem)\n</code></pre>"},{"location":"algorithm_reference/rebase/#understanding-the-output","title":"Understanding the Output","text":""},{"location":"algorithm_reference/rebase/#metadata-structure","title":"Metadata Structure","text":"<pre><code>metadata = {\n    'algorithm': 'rebase',\n    'iterations': 5,\n    'initial_score': 0.60,\n    'final_score': 0.92,\n    'score_history': [0.60, 0.72, 0.85, 0.90, 0.92],\n    'feedback_history': [\n        'Missing edge case handling',\n        'Improved but efficiency could be better',\n        'Good solution, minor style issues',\n        'Nearly perfect, small documentation gap',\n        'Excellent solution'\n    ],\n    'improvements': [\n        {'iteration': 1, 'delta': 0.12},\n        {'iteration': 2, 'delta': 0.13},\n        {'iteration': 3, 'delta': 0.05},\n        {'iteration': 4, 'delta': 0.02}  # Below threshold, stopped\n    ],\n    'generation_time': 35.2,\n    'verification_time': 28.5,\n    'total_time': 63.7,\n    'stop_reason': 'improvement_threshold'\n}\n</code></pre>"},{"location":"algorithm_reference/rebase/#stop-reasons","title":"Stop Reasons","text":"<ul> <li><code>improvement_threshold</code>: Improvement below threshold</li> <li><code>max_iterations</code>: Reached maximum iterations</li> <li><code>min_score_reached</code>: Achieved target score</li> <li><code>no_improvement</code>: No improvement for <code>patience</code> iterations</li> </ul>"},{"location":"algorithm_reference/rebase/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"algorithm_reference/rebase/#time-complexity","title":"Time Complexity","text":"<p><code>O(k)</code> where <code>k</code> = number of iterations</p>"},{"location":"algorithm_reference/rebase/#api-calls","title":"API Calls","text":"<p>Approximately: <code>2 * iterations</code> calls</p> <ul> <li>One for refinement generation</li> <li>One for verification</li> </ul>"},{"location":"algorithm_reference/rebase/#actual-cost-example","title":"Actual Cost Example","text":"<pre><code># Configuration\nmax_iterations = 5\n\n# Best case (converges early)\niterations = 3\ntotal_calls = 3 * 2 = 6\n\n# Worst case (all iterations)\niterations = 5\ntotal_calls = 5 * 2 = 10\n</code></pre>"},{"location":"algorithm_reference/rebase/#comparison-with-other-algorithms","title":"Comparison with Other Algorithms","text":"Aspect REBASE BestOfN TreeOfThought Approach Iterative Parallel Tree search Refinement Yes No Minimal Convergence Yes N/A N/A Best for Optimization Speed Exploration Speed Slow Fast Medium Quality Excellent Good Excellent"},{"location":"algorithm_reference/rebase/#iteration-patterns","title":"Iteration Patterns","text":""},{"location":"algorithm_reference/rebase/#typical-iteration-pattern","title":"Typical Iteration Pattern","text":"<pre><code>Iteration 1: Score 0.60 \u2192 Fix critical issues\nIteration 2: Score 0.75 \u2192 Address major problems  \nIteration 3: Score 0.85 \u2192 Improve edge cases\nIteration 4: Score 0.90 \u2192 Polish and optimize\nIteration 5: Score 0.92 \u2192 Final refinements\n</code></pre>"},{"location":"algorithm_reference/rebase/#convergence-pattern","title":"Convergence Pattern","text":"<pre><code>xychart-beta\n    title \"REBASE Score Convergence\"\n    x-axis \"Iteration\" [1, 2, 3, 4, 5]\n    y-axis \"Score\" 0 --&gt; 1\n    line [0.45, 0.65, 0.80, 0.90, 0.92]\n</code></pre>"},{"location":"algorithm_reference/rebase/#advanced-features","title":"Advanced Features","text":""},{"location":"algorithm_reference/rebase/#custom-refinement-prompts","title":"Custom Refinement Prompts","text":"<pre><code>class CustomREBASE(REBASE):\n    def generate_refinement_prompt(self, problem, current_solution, feedback):\n        \"\"\"Custom refinement prompt.\"\"\"\n        return f\"\"\"\n        Problem: {problem}\n\n        Current Solution:\n        {current_solution}\n\n        Issues Found:\n        {feedback}\n\n        Your task:\n        1. Address each issue mentioned\n        2. Maintain correct parts of the solution\n        3. Improve overall quality\n        4. Add comments explaining changes\n\n        Provide the refined solution:\n        \"\"\"\n</code></pre>"},{"location":"algorithm_reference/rebase/#adaptive-threshold","title":"Adaptive Threshold","text":"<pre><code>class AdaptiveREBASE(REBASE):\n    def should_continue(self, iteration, current_score, previous_score):\n        \"\"\"Adaptive stopping condition.\"\"\"\n        improvement = current_score - previous_score\n\n        # Dynamic threshold based on iteration\n        threshold = self.improvement_threshold * (1 - iteration / self.max_iterations)\n\n        return improvement &gt;= threshold and iteration &lt; self.max_iterations\n</code></pre>"},{"location":"algorithm_reference/rebase/#best-practices","title":"Best Practices","text":""},{"location":"algorithm_reference/rebase/#1-set-appropriate-max-iterations","title":"1. Set Appropriate Max Iterations","text":"<pre><code># Simple problems\nmax_iterations = 3\n\n# Moderate problems\nmax_iterations = 5\n\n# Complex problems\nmax_iterations = 10\n</code></pre>"},{"location":"algorithm_reference/rebase/#2-use-reasonable-thresholds","title":"2. Use Reasonable Thresholds","text":"<pre><code># Strict (stop early)\nimprovement_threshold = 0.2\n\n# Balanced\nimprovement_threshold = 0.1\n\n# Lenient (refine thoroughly)\nimprovement_threshold = 0.05\n</code></pre>"},{"location":"algorithm_reference/rebase/#3-provide-good-initial-solutions","title":"3. Provide Good Initial Solutions","text":"<pre><code># Generate better initial solution\ninitial = plangen.generate_plan(problem, constraints)\n\n# Refine it with REBASE\nrebase = REBASE(max_iterations=5, llm_interface=model)\nrefined, score, metadata = rebase.run(problem, initial_solution=initial)\n</code></pre>"},{"location":"algorithm_reference/rebase/#4-monitor-convergence","title":"4. Monitor Convergence","text":"<pre><code>result, score, metadata = rebase.run(problem)\n\n# Check if converged properly\nif metadata['stop_reason'] == 'improvement_threshold':\n    print(\"Converged successfully\")\nelif metadata['stop_reason'] == 'max_iterations':\n    print(\"May need more iterations\")\n\n# Analyze score progression\nimport matplotlib.pyplot as plt\nplt.plot(metadata['score_history'])\nplt.xlabel('Iteration')\nplt.ylabel('Score')\nplt.title('REBASE Convergence')\nplt.show()\n</code></pre>"},{"location":"algorithm_reference/rebase/#troubleshooting","title":"Troubleshooting","text":""},{"location":"algorithm_reference/rebase/#not-converging","title":"Not Converging","text":"<ul> <li>Increase <code>max_iterations</code></li> <li>Decrease <code>improvement_threshold</code></li> <li>Check if problem is suitable for iterative refinement</li> </ul> <pre><code># More lenient configuration\nrebase = REBASE(\n    max_iterations=15,\n    improvement_threshold=0.03,\n    llm_interface=model\n)\n</code></pre>"},{"location":"algorithm_reference/rebase/#slow-performance","title":"Slow Performance","text":"<ul> <li>Reduce <code>max_iterations</code></li> <li>Increase <code>improvement_threshold</code></li> <li>Use faster model</li> </ul> <pre><code># Faster configuration\nrebase = REBASE(\n    max_iterations=3,\n    improvement_threshold=0.2,\n    llm_interface=faster_model\n)\n</code></pre>"},{"location":"algorithm_reference/rebase/#score-oscillating","title":"Score Oscillating","text":"<ul> <li>Add patience parameter</li> <li>Use verification smoothing</li> <li>Check verification consistency</li> </ul> <pre><code>rebase = REBASE(\n    max_iterations=10,\n    improvement_threshold=0.1,\n    patience=3,  # Stop after 3 iterations without improvement\n    llm_interface=model\n)\n</code></pre>"},{"location":"algorithm_reference/rebase/#combining-with-other-algorithms","title":"Combining with Other Algorithms","text":""},{"location":"algorithm_reference/rebase/#rebase-after-bestofn","title":"REBASE after BestOfN","text":"<pre><code># Generate diverse initial solutions\nbest_of_n = BestOfN(n_plans=5, llm_interface=model)\ninitial_plan, _, _ = best_of_n.run(problem)\n\n# Refine the best one\nrebase = REBASE(max_iterations=5, llm_interface=model)\nrefined_plan, score, _ = rebase.run(problem, initial_solution=initial_plan)\n</code></pre>"},{"location":"algorithm_reference/rebase/#rebase-with-treeofthought-initial","title":"REBASE with TreeOfThought Initial","text":"<pre><code># Explore with TreeOfThought\ntot = TreeOfThought(branching_factor=3, max_depth=4, llm_interface=model)\nexplored_plan, _, _ = tot.run(problem)\n\n# Refine with REBASE\nrebase = REBASE(max_iterations=5, llm_interface=model)\nfinal_plan, score, _ = rebase.run(problem, initial_solution=explored_plan)\n</code></pre>"},{"location":"algorithm_reference/rebase/#next-steps","title":"Next Steps","text":"<ul> <li>See Algorithm Selection Guide for choosing algorithms</li> <li>Read BestOfN for simpler alternative</li> <li>Check TreeOfThought for exploration-focused approach</li> <li>Explore Visualization to track refinement progress</li> </ul>"},{"location":"algorithm_reference/tree_of_thought/","title":"TreeOfThought","text":"<p>The TreeOfThought algorithm explores multiple reasoning paths in a tree structure, allowing for branching, evaluation, and backtracking to find the best solution.</p>"},{"location":"algorithm_reference/tree_of_thought/#overview","title":"Overview","text":"<p>TreeOfThought builds a search tree where:</p> <ol> <li>Each node represents a reasoning step or partial solution</li> <li>Multiple branches explore different approaches</li> <li>Nodes are evaluated and pruned based on promise</li> <li>The best path leads to the final solution</li> </ol>"},{"location":"algorithm_reference/tree_of_thought/#how-it-works","title":"How It Works","text":"<pre><code>graph TD\n    A[Initial Problem] --&gt; B1[Branch 1]\n    A --&gt; B2[Branch 2]\n    A --&gt; B3[Branch 3]\n    B1 --&gt; S1[S1]\n    B1 --&gt; S2[S2]\n    B1 --&gt; S3[S3]\n    B2 --&gt; S4[S4]\n    B2 --&gt; S5[S5]\n    B2 --&gt; S6[S6]\n    B3 --&gt; S7[S7]\n    B3 --&gt; S8[S8]\n    B3 --&gt; S9[S9]\n</code></pre> <p>Process:</p> <ol> <li>Generate Branches: Create multiple reasoning paths from current node</li> <li>Evaluate: Score each branch based on promise/validity</li> <li>Prune: Keep only top branches (beam search)</li> <li>Expand: Continue from promising branches</li> <li>Repeat: Until max depth or solution found</li> </ol>"},{"location":"algorithm_reference/tree_of_thought/#when-to-use","title":"When to Use","text":"<p>TreeOfThought excels at:</p> <ul> <li>Complex reasoning tasks requiring multiple steps</li> <li>Problems with multiple valid approaches to explore</li> <li>Multi-step planning where early decisions affect later ones</li> <li>Tasks benefiting from backtracking when approaches fail</li> </ul> <p>Avoid for:</p> <ul> <li>Simple, straightforward problems</li> <li>When speed is critical</li> <li>Problems with clear single solution path</li> </ul>"},{"location":"algorithm_reference/tree_of_thought/#parameters","title":"Parameters","text":""},{"location":"algorithm_reference/tree_of_thought/#core-parameters","title":"Core Parameters","text":"Parameter Type Default Description <code>branching_factor</code> int 3 Number of branches from each node <code>max_depth</code> int 5 Maximum tree depth <code>beam_width</code> int 2 Number of best branches to keep at each level <code>llm_interface</code> ModelProtocol Required Language model interface"},{"location":"algorithm_reference/tree_of_thought/#advanced-parameters","title":"Advanced Parameters","text":"Parameter Type Default Description <code>evaluation_strategy</code> str \"llm\" How to evaluate nodes (\"llm\" or \"heuristic\") <code>pruning_threshold</code> float 0.3 Minimum score to keep a branch <code>domain_template</code> str None Domain-specific reasoning template"},{"location":"algorithm_reference/tree_of_thought/#usage-examples","title":"Usage Examples","text":""},{"location":"algorithm_reference/tree_of_thought/#basic-usage","title":"Basic Usage","text":"<pre><code>from plangen import PlanGen\n\nplangen = PlanGen.create()\n\nresult = plangen.solve(\n    problem=\"Design an algorithm to find the kth largest element in an array\",\n    algorithm=\"tree_of_thought\",\n    branching_factor=3,\n    max_depth=5,\n    beam_width=2\n)\n\nprint(result[\"selected_solution\"])\n</code></pre>"},{"location":"algorithm_reference/tree_of_thought/#with-algorithm-class","title":"With Algorithm Class","text":"<pre><code>from plangen.algorithms import TreeOfThought\nfrom plangen.models import OpenAIModelInterface\n\n# Create model\nmodel = OpenAIModelInterface(model_name=\"gpt-4o\")\n\n# Create algorithm\ntot = TreeOfThought(\n    branching_factor=4,\n    max_depth=6,\n    beam_width=3,\n    llm_interface=model\n)\n\n# Run algorithm\nproblem = \"Plan a 7-day trip to Japan with a budget of $3000\"\nbest_plan, score, metadata = tot.run(problem)\n\nprint(f\"Best plan (score: {score}):\\n{best_plan}\")\nprint(f\"\\nMetadata: {metadata}\")\n</code></pre>"},{"location":"algorithm_reference/tree_of_thought/#with-visualization","title":"With Visualization","text":"<pre><code>from plangen.algorithms import TreeOfThought\nfrom plangen.visualization import GraphRenderer\nfrom plangen.models import OpenAIModelInterface\n\n# Create model\nmodel = OpenAIModelInterface(model_name=\"gpt-4o\")\n\n# Create visualizer\nrenderer = GraphRenderer(output_dir=\"./tree_visualizations\")\n\n# Create algorithm\ntot = TreeOfThought(\n    branching_factor=3,\n    max_depth=5,\n    beam_width=2,\n    llm_interface=model\n)\n\n# Add observer for visualization\ntot.add_observer(renderer)\n\n# Run algorithm\nproblem = \"Design a system for managing a library\"\nbest_plan, score, metadata = tot.run(problem)\n\n# Check ./tree_visualizations for generated graphs\n</code></pre>"},{"location":"algorithm_reference/tree_of_thought/#configuration-strategies","title":"Configuration Strategies","text":""},{"location":"algorithm_reference/tree_of_thought/#shallow-wide-exploration","title":"Shallow, Wide Exploration","text":"<p>Good for: Exploring many alternatives quickly</p> <pre><code>tot = TreeOfThought(\n    branching_factor=5,  # Many branches\n    max_depth=3,         # Shallow depth\n    beam_width=3,        # Keep more alternatives\n    llm_interface=model\n)\n</code></pre>"},{"location":"algorithm_reference/tree_of_thought/#deep-narrow-exploration","title":"Deep, Narrow Exploration","text":"<p>Good for: Following promising paths deeply</p> <pre><code>tot = TreeOfThought(\n    branching_factor=2,  # Fewer branches\n    max_depth=8,         # Deep exploration\n    beam_width=1,        # Focus on best path\n    llm_interface=model\n)\n</code></pre>"},{"location":"algorithm_reference/tree_of_thought/#balanced-exploration","title":"Balanced Exploration","text":"<p>Good for: Most general problems</p> <pre><code>tot = TreeOfThought(\n    branching_factor=3,\n    max_depth=5,\n    beam_width=2,\n    llm_interface=model\n)\n</code></pre>"},{"location":"algorithm_reference/tree_of_thought/#use-case-examples","title":"Use Case Examples","text":""},{"location":"algorithm_reference/tree_of_thought/#algorithm-design","title":"Algorithm Design","text":"<pre><code>problem = \"\"\"\nDesign an efficient algorithm to find the longest palindromic substring.\nConsider time and space complexity trade-offs.\n\"\"\"\n\ntot = TreeOfThought(\n    branching_factor=3,  # Try different approaches\n    max_depth=5,         # Detailed reasoning\n    beam_width=2,        # Keep best approaches\n    llm_interface=model\n)\n\nsolution, score, metadata = tot.run(problem)\n</code></pre>"},{"location":"algorithm_reference/tree_of_thought/#planning-task","title":"Planning Task","text":"<pre><code>problem = \"\"\"\nPlan a 3-day hackathon event for 100 participants.\nInclude schedule, venue setup, and contingency plans.\n\"\"\"\n\ntot = TreeOfThought(\n    branching_factor=4,  # Many planning options\n    max_depth=6,         # Detailed planning\n    beam_width=2,        # Keep good alternatives\n    llm_interface=model\n)\n\nplan, score, metadata = tot.run(problem)\n</code></pre>"},{"location":"algorithm_reference/tree_of_thought/#creative-writing","title":"Creative Writing","text":"<pre><code>problem = \"\"\"\nWrite a short science fiction story about AI and humanity.\nInclude plot development, character arcs, and themes.\n\"\"\"\n\ntot = TreeOfThought(\n    branching_factor=5,  # Explore plot directions\n    max_depth=4,         # Story structure\n    beam_width=3,        # Keep creative options\n    llm_interface=model\n)\n\nstory, score, metadata = tot.run(problem)\n</code></pre>"},{"location":"algorithm_reference/tree_of_thought/#understanding-the-output","title":"Understanding the Output","text":""},{"location":"algorithm_reference/tree_of_thought/#metadata-structure","title":"Metadata Structure","text":"<pre><code>metadata = {\n    'algorithm': 'tree_of_thought',\n    'total_nodes': 45,\n    'total_branches': 135,\n    'explored_depth': 5,\n    'pruned_branches': 90,\n    'best_path': [\n        {'depth': 0, 'node_id': 'root', 'score': 0.0},\n        {'depth': 1, 'node_id': 'n1_2', 'score': 0.85},\n        {'depth': 2, 'node_id': 'n2_5', 'score': 0.90},\n        {'depth': 3, 'node_id': 'n3_8', 'score': 0.95},\n        {'depth': 4, 'node_id': 'n4_12', 'score': 0.92},\n        {'depth': 5, 'node_id': 'n5_15', 'score': 0.88}\n    ],\n    'generation_time': 45.3,\n    'evaluation_time': 32.1,\n    'total_time': 77.4\n}\n</code></pre>"},{"location":"algorithm_reference/tree_of_thought/#visualization-output","title":"Visualization Output","text":"<p>When using a GraphRenderer observer, TreeOfThought generates:</p> <ul> <li>tree_structure.png: Visual representation of the search tree</li> <li>path_scores.png: Score evolution along explored paths</li> <li>pruning_summary.txt: Details on pruned branches</li> </ul>"},{"location":"algorithm_reference/tree_of_thought/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"algorithm_reference/tree_of_thought/#time-complexity","title":"Time Complexity","text":"<p><code>O(b^d * k)</code> where:</p> <ul> <li><code>b</code> = branching_factor</li> <li><code>d</code> = max_depth</li> <li><code>k</code> = beam_width</li> </ul>"},{"location":"algorithm_reference/tree_of_thought/#api-calls","title":"API Calls","text":"<p>Approximately: <code>branching_factor * max_depth * 2</code> calls</p> <ul> <li>One for generation</li> <li>One for evaluation</li> </ul>"},{"location":"algorithm_reference/tree_of_thought/#actual-cost-example","title":"Actual Cost Example","text":"<pre><code># Configuration\nbranching_factor = 3\nmax_depth = 5\nbeam_width = 2\n\n# Estimated API calls\ngeneration_calls = 3 * 5 = 15\nevaluation_calls = 3 * 5 = 15\ntotal_calls \u2248 30\n\n# With pruning, actual calls are typically lower\n</code></pre>"},{"location":"algorithm_reference/tree_of_thought/#comparison-with-other-algorithms","title":"Comparison with Other Algorithms","text":"Aspect TreeOfThought BestOfN REBASE Exploration Structured Independent Sequential Backtracking Yes No No Refinement Minimal No Yes Best for Complex reasoning Quick results Optimization Speed Medium Fast Slow API calls High Medium Medium-High"},{"location":"algorithm_reference/tree_of_thought/#advanced-features","title":"Advanced Features","text":""},{"location":"algorithm_reference/tree_of_thought/#custom-evaluation","title":"Custom Evaluation","text":"<pre><code>class CustomTreeOfThought(TreeOfThought):\n    def evaluate_node(self, node, problem, constraints):\n        \"\"\"Custom node evaluation logic.\"\"\"\n        # Use domain-specific heuristics\n        base_score = super().evaluate_node(node, problem, constraints)\n\n        # Apply custom scoring\n        if self._meets_custom_criteria(node):\n            base_score *= 1.2\n\n        return min(1.0, base_score)\n\n    def _meets_custom_criteria(self, node):\n        # Your custom logic\n        return True\n</code></pre>"},{"location":"algorithm_reference/tree_of_thought/#domain-templates","title":"Domain Templates","text":"<pre><code># Algorithm design template\nalgorithm_template = \"\"\"\nAt each step, consider:\n1. Data structures needed\n2. Time complexity implications\n3. Space complexity trade-offs\n4. Edge cases to handle\n\nCurrent reasoning: {current_node}\nNext step: {branch}\n\"\"\"\n\ntot = TreeOfThought(\n    branching_factor=3,\n    max_depth=5,\n    llm_interface=model,\n    domain_template=algorithm_template\n)\n</code></pre>"},{"location":"algorithm_reference/tree_of_thought/#best-practices","title":"Best Practices","text":""},{"location":"algorithm_reference/tree_of_thought/#1-start-with-balanced-parameters","title":"1. Start with Balanced Parameters","text":"<pre><code># Good starting point\ntot = TreeOfThought(\n    branching_factor=3,\n    max_depth=5,\n    beam_width=2,\n    llm_interface=model\n)\n</code></pre>"},{"location":"algorithm_reference/tree_of_thought/#2-use-visualization-for-debugging","title":"2. Use Visualization for Debugging","text":"<pre><code>renderer = GraphRenderer(output_dir=\"./debug_viz\")\ntot.add_observer(renderer)\n# Run and examine visualizations\n</code></pre>"},{"location":"algorithm_reference/tree_of_thought/#3-adjust-based-on-problem","title":"3. Adjust Based on Problem","text":"<pre><code># For exploration-heavy problems\nbranching_factor = 4  # More options\nbeam_width = 3        # Keep more paths\n\n# For depth-first problems  \nbranching_factor = 2  # Fewer options\nmax_depth = 8         # Deeper exploration\n</code></pre>"},{"location":"algorithm_reference/tree_of_thought/#4-monitor-performance","title":"4. Monitor Performance","text":"<pre><code>import time\n\nstart = time.time()\nsolution, score, metadata = tot.run(problem)\nduration = time.time() - start\n\nprint(f\"Solved in {duration:.1f}s\")\nprint(f\"API calls: ~{metadata['total_nodes'] * 2}\")\nprint(f\"Explored: {metadata['total_nodes']} nodes\")\n</code></pre>"},{"location":"algorithm_reference/tree_of_thought/#troubleshooting","title":"Troubleshooting","text":""},{"location":"algorithm_reference/tree_of_thought/#too-slow","title":"Too Slow","text":"<ul> <li>Reduce <code>branching_factor</code></li> <li>Reduce <code>max_depth</code></li> <li>Reduce <code>beam_width</code></li> </ul> <pre><code># Faster configuration\ntot = TreeOfThought(\n    branching_factor=2,\n    max_depth=3,\n    beam_width=1,\n    llm_interface=model\n)\n</code></pre>"},{"location":"algorithm_reference/tree_of_thought/#poor-quality","title":"Poor Quality","text":"<ul> <li>Increase <code>branching_factor</code> for more exploration</li> <li>Increase <code>max_depth</code> for deeper reasoning</li> <li>Increase <code>beam_width</code> to keep more alternatives</li> </ul> <pre><code># Higher quality configuration\ntot = TreeOfThought(\n    branching_factor=4,\n    max_depth=7,\n    beam_width=3,\n    llm_interface=model\n)\n</code></pre>"},{"location":"algorithm_reference/tree_of_thought/#all-branches-pruned","title":"All Branches Pruned","text":"<ul> <li>Lower <code>pruning_threshold</code></li> <li>Adjust prompts to encourage valid reasoning</li> </ul>"},{"location":"algorithm_reference/tree_of_thought/#next-steps","title":"Next Steps","text":"<ul> <li>See Algorithm Selection Guide to compare with other algorithms</li> <li>Read BestOfN for a simpler alternative</li> <li>Check REBASE for iterative refinement</li> <li>Explore Visualization for debugging tools</li> </ul>"},{"location":"api_reference/","title":"API Reference","text":"<p>This section provides a complete reference for all public classes, methods, and functions in the PlanGEN framework.</p>"},{"location":"api_reference/#public-api","title":"Public API","text":"<ul> <li>PlanGen - Main entry point for the PlanGEN framework</li> <li>Algorithm - Interface for different planning algorithms</li> <li>Visualization - Tools for visualizing planning processes</li> <li>Verifiers - Factory for domain-specific verification tools</li> </ul>"},{"location":"api_reference/#internal-api-for-contributors","title":"Internal API (for contributors)","text":"<ul> <li>Agents - Specialized agents for different tasks</li> <li>Models - Model interfaces for different LLM providers</li> <li>Prompts - Prompt management and templating</li> <li>Verification - Verification strategies and mechanisms</li> </ul>"},{"location":"api_reference/agents/","title":"Agents","text":"<p>Internal agents for the PlanGEN workflow pipeline.</p>"},{"location":"api_reference/agents/#overview","title":"Overview","text":"<p>PlanGEN uses specialized agents for each step of the planning process. These are internal components\u2014use the high-level PlanGen API instead.</p>"},{"location":"api_reference/agents/#classes","title":"Classes","text":""},{"location":"api_reference/agents/#constraintagent","title":"ConstraintAgent","text":"<p>Extracts constraints from problem statements.</p> <pre><code>from plangen.agents import ConstraintAgent\n\nagent = ConstraintAgent(model_name=\"gpt-4o\")\nconstraints = agent.run(problem=\"Schedule a meeting...\")\n</code></pre>"},{"location":"api_reference/agents/#solutionagent","title":"SolutionAgent","text":"<p>Generates candidate solutions that satisfy constraints.</p> <pre><code>from plangen.agents import SolutionAgent\n\nagent = SolutionAgent(model_name=\"gpt-4o\")\nsolution = agent.run(problem=\"...\", constraints=[\"...\"])\n</code></pre>"},{"location":"api_reference/agents/#verificationagent","title":"VerificationAgent","text":"<p>Validates solutions against constraints and provides feedback.</p> <pre><code>from plangen.agents import VerificationAgent\n\nagent = VerificationAgent(model_name=\"gpt-4o\")\nresult = agent.run(problem=\"...\", solution=\"...\", constraints=[\"...\"])\n</code></pre>"},{"location":"api_reference/agents/#selectionagent","title":"SelectionAgent","text":"<p>Selects the best solution from candidates based on verification scores.</p> <pre><code>from plangen.agents import SelectionAgent\n\nagent = SelectionAgent()\nbest = agent.run(solutions=[...], scores=[...])\n</code></pre>"},{"location":"api_reference/agents/#base-class","title":"Base Class","text":"<p>All agents inherit from <code>BaseAgent</code>:</p> <pre><code>class BaseAgent(ABC):\n    def __init__(\n        self,\n        llm_interface: LLMInterface | None = None,\n        model_name: str = \"gpt-4o\",\n        temperature: float = 0.7,\n        system_message: str | None = None,\n    ) -&gt; None: ...\n\n    @abstractmethod\n    def run(self, *args, **kwargs) -&gt; object: ...\n</code></pre>"},{"location":"api_reference/agents/#see-also","title":"See Also","text":"<ul> <li>PlanGen - High-level API (recommended)</li> <li>Models - LLM interfaces used by agents</li> </ul>"},{"location":"api_reference/algorithm/","title":"Algorithm API Reference","text":"<p>This page documents the Algorithm classes and interfaces in PlanGEN.</p>"},{"location":"api_reference/algorithm/#algorithm-factory","title":"Algorithm Factory","text":""},{"location":"api_reference/algorithm/#algorithmcreate","title":"<code>Algorithm.create</code>","text":"<pre><code>@classmethod\ndef create(\n    cls,\n    algorithm_type: str,\n    model: ModelProtocol,\n    **kwargs\n) -&gt; BaseAlgorithm\n</code></pre> <p>Factory method to create algorithm instances.</p> <p>Parameters:</p> <ul> <li><code>algorithm_type</code>: Type of algorithm ('best_of_n', 'tree_of_thought', 'rebase', 'mixture')</li> <li><code>model</code>: Language model interface</li> <li><code>**kwargs</code>: Algorithm-specific parameters</li> </ul> <p>Returns:</p> <ul> <li>Configured algorithm instance</li> </ul> <p>Example:</p> <pre><code>from plangen import Algorithm, PlanGen\n\nplangen = PlanGen.create()\nalgorithm = Algorithm.create(\n    algorithm_type=\"best_of_n\",\n    model=plangen._plangen.model,\n    n_plans=5,\n    parallel=True\n)\n</code></pre>"},{"location":"api_reference/algorithm/#basealgorithm","title":"BaseAlgorithm","text":"<p>Base class for all planning algorithms.</p>"},{"location":"api_reference/algorithm/#methods","title":"Methods","text":""},{"location":"api_reference/algorithm/#run","title":"<code>run</code>","text":"<pre><code>def run(\n    self,\n    problem: str,\n    constraints: Optional[List[str]] = None,\n    **kwargs\n) -&gt; Tuple[str, float, Dict[str, Any]]\n</code></pre> <p>Execute the planning algorithm.</p> <p>Parameters:</p> <ul> <li><code>problem</code>: Problem statement</li> <li><code>constraints</code>: Optional list of constraints (extracted automatically if not provided)</li> <li><code>**kwargs</code>: Additional algorithm-specific parameters</li> </ul> <p>Returns:</p> <ul> <li>Tuple of (best_plan, score, metadata)</li> </ul> <p>Example:</p> <pre><code>best_plan, score, metadata = algorithm.run(\n    problem=\"Schedule a meeting...\",\n    constraints=[\"30 minutes\", \"Monday only\"]\n)\n</code></pre>"},{"location":"api_reference/algorithm/#add_observer","title":"<code>add_observer</code>","text":"<pre><code>def add_observer(self, observer: ObserverProtocol) -&gt; None\n</code></pre> <p>Add an observer for visualization or logging.</p> <p>Parameters:</p> <ul> <li><code>observer</code>: Observer implementing ObserverProtocol</li> </ul> <p>Example:</p> <pre><code>from plangen.visualization import GraphRenderer\n\nrenderer = GraphRenderer(output_dir=\"./viz\")\nalgorithm.add_observer(renderer)\n</code></pre>"},{"location":"api_reference/algorithm/#bestofn","title":"BestOfN","text":"<p>Generates multiple plans and selects the best one.</p>"},{"location":"api_reference/algorithm/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    n_plans: int = 5,\n    sampling_strategy: str = \"diverse\",\n    parallel: bool = True,\n    llm_interface: ModelProtocol = None,\n    **kwargs\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>n_plans</code>: Number of plans to generate</li> <li><code>sampling_strategy</code>: Strategy for generation ('diverse' or 'adaptive')</li> <li><code>parallel</code>: Whether to generate in parallel</li> <li><code>llm_interface</code>: Language model interface</li> </ul> <p>Example:</p> <pre><code>from plangen.algorithms import BestOfN\nfrom plangen.models import OpenAIModelInterface\n\nmodel = OpenAIModelInterface(model_name=\"gpt-4o\")\nalgorithm = BestOfN(\n    n_plans=10,\n    sampling_strategy=\"diverse\",\n    parallel=True,\n    llm_interface=model\n)\n</code></pre>"},{"location":"api_reference/algorithm/#treeofthought","title":"TreeOfThought","text":"<p>Explores multiple reasoning paths in a tree structure.</p>"},{"location":"api_reference/algorithm/#constructor_1","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    branching_factor: int = 3,\n    max_depth: int = 5,\n    beam_width: int = 2,\n    llm_interface: ModelProtocol = None,\n    **kwargs\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>branching_factor</code>: Number of branches per node</li> <li><code>max_depth</code>: Maximum tree depth</li> <li><code>beam_width</code>: Number of best branches to keep at each level</li> <li><code>llm_interface</code>: Language model interface</li> </ul> <p>Example:</p> <pre><code>from plangen.algorithms import TreeOfThought\n\nalgorithm = TreeOfThought(\n    branching_factor=4,\n    max_depth=6,\n    beam_width=3,\n    llm_interface=model\n)\n</code></pre>"},{"location":"api_reference/algorithm/#rebase","title":"REBASE","text":"<p>Iteratively refines solutions based on feedback.</p>"},{"location":"api_reference/algorithm/#constructor_2","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    max_iterations: int = 5,\n    improvement_threshold: float = 0.1,\n    llm_interface: ModelProtocol = None,\n    **kwargs\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>max_iterations</code>: Maximum number of refinement iterations</li> <li><code>improvement_threshold</code>: Minimum improvement to continue</li> <li><code>llm_interface</code>: Language model interface</li> </ul> <p>Example:</p> <pre><code>from plangen.algorithms import REBASE\n\nalgorithm = REBASE(\n    max_iterations=10,\n    improvement_threshold=0.05,\n    llm_interface=model\n)\n</code></pre>"},{"location":"api_reference/algorithm/#mixtureofalgorithms","title":"MixtureOfAlgorithms","text":"<p>Dynamically selects the best algorithm for the problem.</p>"},{"location":"api_reference/algorithm/#constructor_3","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    max_algorithm_switches: int = 2,\n    llm_interface: ModelProtocol = None,\n    **kwargs\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>max_algorithm_switches</code>: Maximum number of algorithm switches allowed</li> <li><code>llm_interface</code>: Language model interface</li> </ul> <p>Example:</p> <pre><code>from plangen.algorithms import MixtureOfAlgorithms\n\nalgorithm = MixtureOfAlgorithms(\n    max_algorithm_switches=3,\n    llm_interface=model\n)\n</code></pre>"},{"location":"api_reference/algorithm/#common-metadata","title":"Common Metadata","text":"<p>All algorithms return metadata with run information:</p> <pre><code>metadata = {\n    'algorithm': 'best_of_n',\n    'num_plans_generated': 5,\n    'generation_time': 12.5,\n    'verification_time': 8.3,\n    'total_time': 20.8,\n    'all_scores': [0.9, 0.85, 0.95, 0.8, 0.88],\n    'selected_index': 2,\n    # Algorithm-specific metadata\n}\n</code></pre>"},{"location":"api_reference/algorithm/#observer-protocol","title":"Observer Protocol","text":"<p>For custom observers:</p> <pre><code>from typing import Protocol\n\nclass ObserverProtocol(Protocol):\n    def notify(self, event: str, data: Dict[str, Any]) -&gt; None:\n        \"\"\"Receive notifications from algorithms.\"\"\"\n        pass\n</code></pre>"},{"location":"api_reference/algorithm/#complete-example","title":"Complete Example","text":"<pre><code>from plangen import PlanGen\nfrom plangen.algorithms import BestOfN, TreeOfThought, REBASE\nfrom plangen.visualization import GraphRenderer\n\n# Create model\nplangen = PlanGen.create()\nmodel = plangen._plangen.model\n\n# Create visualizer\nrenderer = GraphRenderer(output_dir=\"./visualizations\")\n\n# Try different algorithms\nproblem = \"Your problem statement here\"\n\n# BestOfN\nbest_of_n = BestOfN(n_plans=5, llm_interface=model)\nbest_of_n.add_observer(renderer)\nplan1, score1, meta1 = best_of_n.run(problem)\n\n# TreeOfThought\ntot = TreeOfThought(branching_factor=3, max_depth=5, llm_interface=model)\ntot.add_observer(renderer)\nplan2, score2, meta2 = tot.run(problem)\n\n# REBASE\nrebase = REBASE(max_iterations=5, llm_interface=model)\nrebase.add_observer(renderer)\nplan3, score3, meta3 = rebase.run(problem)\n\n# Compare results\nprint(f\"BestOfN: {score1}\")\nprint(f\"TreeOfThought: {score2}\")\nprint(f\"REBASE: {score3}\")\n</code></pre>"},{"location":"api_reference/algorithm/#see-also","title":"See Also","text":"<ul> <li>Algorithm Selection Guide</li> <li>Algorithm References</li> <li>Visualization</li> </ul>"},{"location":"api_reference/models/","title":"Models","text":"<p>LLM model interfaces for PlanGEN.</p>"},{"location":"api_reference/models/#overview","title":"Overview","text":"<p>PlanGEN supports multiple LLM providers through a common interface. For most use cases, use the high-level PlanGen factory methods instead.</p>"},{"location":"api_reference/models/#classes","title":"Classes","text":""},{"location":"api_reference/models/#basemodelinterface","title":"BaseModelInterface","text":"<p>Abstract base class defining the model interface contract.</p> <pre><code>class BaseModelInterface(ABC):\n    @abstractmethod\n    def generate(\n        self,\n        prompt: str,\n        system_message: str | None = None,\n        temperature: float | None = None,\n        max_tokens: int | None = None,\n    ) -&gt; str: ...\n\n    @abstractmethod\n    def batch_generate(\n        self,\n        prompts: list[str],\n        system_message: str | None = None,\n        temperature: float | None = None,\n        max_tokens: int | None = None,\n    ) -&gt; list[str]: ...\n</code></pre>"},{"location":"api_reference/models/#openaimodelinterface","title":"OpenAIModelInterface","text":"<p>OpenAI API implementation.</p> <pre><code>from plangen.models import OpenAIModelInterface\n\nmodel = OpenAIModelInterface(model_name=\"gpt-4o\", temperature=0.7)\nresponse = model.generate(\"What is 2+2?\")\n</code></pre> <p>Requires <code>OPENAI_API_KEY</code> environment variable.</p>"},{"location":"api_reference/models/#bedrockmodelinterface","title":"BedrockModelInterface","text":"<p>AWS Bedrock implementation.</p> <pre><code>from plangen.models import BedrockModelInterface\n\nmodel = BedrockModelInterface(\n    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n    region_name=\"us-east-1\"\n)\nresponse = model.generate(\"What is 2+2?\")\n</code></pre> <p>Requires AWS credentials configured.</p>"},{"location":"api_reference/models/#custom-models","title":"Custom Models","text":"<p>Implement <code>BaseModelInterface</code> to add support for other providers:</p> <pre><code>from plangen.models import BaseModelInterface\n\nclass MyModelInterface(BaseModelInterface):\n    def generate(self, prompt, **kwargs) -&gt; str:\n        # Your implementation\n        pass\n\n    def batch_generate(self, prompts, **kwargs) -&gt; list[str]:\n        return [self.generate(p, **kwargs) for p in prompts]\n</code></pre>"},{"location":"api_reference/models/#see-also","title":"See Also","text":"<ul> <li>PlanGen - High-level API with model factory methods</li> </ul>"},{"location":"api_reference/plangen/","title":"PlanGen","text":"<p>The <code>PlanGen</code> class is the main entry point for the PlanGEN framework, providing a simplified interface for solving problems using large language models.</p> <pre><code>from plangen import PlanGen\n</code></pre>"},{"location":"api_reference/plangen/#factory-methods","title":"Factory Methods","text":""},{"location":"api_reference/plangen/#plangencreate","title":"<code>PlanGen.create</code>","text":"<pre><code>@classmethod\ndef create(\n    cls,\n    model: Optional[str] = \"gpt-4o\",\n    temperature: float = 0.7,\n    max_tokens: Optional[int] = None,\n    api_key: Optional[str] = None,\n    **kwargs,\n) -&gt; \"PlanGen\"\n</code></pre> <p>Create a new PlanGen instance with simplified configuration.</p> <p>Parameters:</p> <ul> <li><code>model</code>: Model identifier string ('gpt-4o', 'claude-3-sonnet', etc.)</li> <li><code>temperature</code>: Temperature for generation (0.0-1.0)</li> <li><code>max_tokens</code>: Maximum tokens to generate (None for model default)</li> <li><code>api_key</code>: Optional API key (if not provided, uses environment variables)</li> <li><code>**kwargs</code>: Additional parameters passed to the model</li> </ul> <p>Returns:</p> <ul> <li>Configured PlanGen instance</li> </ul> <p>Example:</p> <pre><code># Create with default model (gpt-4o)\nplangen = PlanGen.create()\n\n# Create with custom parameters\nplangen = PlanGen.create(\n    model=\"gpt-4-turbo\",\n    temperature=0.5,\n    max_tokens=2048\n)\n</code></pre>"},{"location":"api_reference/plangen/#plangenwith_model","title":"<code>PlanGen.with_model</code>","text":"<pre><code>@classmethod\ndef with_model(cls, model: ModelProtocol) -&gt; \"PlanGen\"\n</code></pre> <p>Create a PlanGen instance with a custom model implementation.</p> <p>Parameters:</p> <ul> <li><code>model</code>: Custom model instance implementing the ModelProtocol</li> </ul> <p>Returns:</p> <ul> <li>Configured PlanGen instance</li> </ul> <p>Example:</p> <pre><code>from my_custom_models import MyCustomModel\n\n# Create a custom model\nmodel = MyCustomModel()\n\n# Create PlanGen with custom model\nplangen = PlanGen.with_model(model)\n</code></pre>"},{"location":"api_reference/plangen/#plangenwith_openai","title":"<code>PlanGen.with_openai</code>","text":"<pre><code>@classmethod\ndef with_openai(\n    cls,\n    model_name: str = \"gpt-4o\",\n    api_key: Optional[str] = None,\n    **kwargs,\n) -&gt; \"PlanGen\"\n</code></pre> <p>Create a PlanGen instance with OpenAI model.</p> <p>Parameters:</p> <ul> <li><code>model_name</code>: OpenAI model name</li> <li><code>api_key</code>: OpenAI API key (if not provided, uses OPENAI_API_KEY)</li> <li><code>**kwargs</code>: Additional parameters passed to the model</li> </ul> <p>Returns:</p> <ul> <li>Configured PlanGen instance with OpenAI model</li> </ul> <p>Example:</p> <pre><code># Create with default OpenAI model (gpt-4o)\nplangen = PlanGen.with_openai()\n\n# Create with custom OpenAI model and parameters\nplangen = PlanGen.with_openai(\n    model_name=\"gpt-3.5-turbo\",\n    temperature=0.8,\n    max_tokens=512\n)\n</code></pre>"},{"location":"api_reference/plangen/#plangenwith_bedrock","title":"<code>PlanGen.with_bedrock</code>","text":"<pre><code>@classmethod\ndef with_bedrock(\n    cls,\n    model_id: str = \"anthropic.claude-3-sonnet-20240229-v1:0\",\n    region: str = \"us-east-1\",\n    **kwargs,\n) -&gt; \"PlanGen\"\n</code></pre> <p>Create a PlanGen instance with AWS Bedrock model.</p> <p>Parameters:</p> <ul> <li><code>model_id</code>: Bedrock model ID</li> <li><code>region</code>: AWS region</li> <li><code>**kwargs</code>: Additional parameters passed to the model</li> </ul> <p>Returns:</p> <ul> <li>Configured PlanGen instance with Bedrock model</li> </ul> <p>Example:</p> <pre><code># Create with default Bedrock model (Claude 3 Sonnet)\nplangen = PlanGen.with_bedrock()\n\n# Create with custom Bedrock model and parameters\nplangen = PlanGen.with_bedrock(\n    model_id=\"anthropic.claude-3-opus-20240229-v1:0\",\n    region=\"us-west-2\",\n    temperature=0.6\n)\n</code></pre>"},{"location":"api_reference/plangen/#methods","title":"Methods","text":""},{"location":"api_reference/plangen/#solve","title":"<code>solve</code>","text":"<pre><code>def solve(\n    self,\n    problem: str,\n    algorithm: str = \"default\",\n    verifier: Optional[VerifierProtocol] = None,\n    **algorithm_params,\n) -&gt; Dict[str, Any]\n</code></pre> <p>Solve a problem using the PlanGEN workflow.</p> <p>Parameters:</p> <ul> <li><code>problem</code>: Problem statement to solve</li> <li><code>algorithm</code>: Algorithm to use ('default', 'best_of_n', 'tree_of_thought', 'rebase', 'mixture')</li> <li><code>verifier</code>: Optional custom verifier for specialized verification</li> <li><code>**algorithm_params</code>: Additional parameters for the specific algorithm</li> </ul> <p>Returns:</p> <ul> <li>Dictionary with the solution and intermediate results</li> </ul> <p>Example:</p> <pre><code># Solve a problem using the default workflow\nresult = plangen.solve(\"Design an algorithm to find the kth largest element in an unsorted array.\")\n\n# Solve a problem using a specific algorithm\nresult = plangen.solve(\n    \"Design an algorithm to find the kth largest element in an unsorted array.\",\n    algorithm=\"best_of_n\",\n    n_plans=5\n)\n</code></pre>"},{"location":"api_reference/plangen/#generate_plan","title":"<code>generate_plan</code>","text":"<pre><code>def generate_plan(\n    self,\n    problem: str,\n    constraints: Optional[List[str]] = None,\n    **kwargs,\n) -&gt; str\n</code></pre> <p>Generate a single plan for the given problem.</p> <p>Parameters:</p> <ul> <li><code>problem</code>: Problem statement</li> <li><code>constraints</code>: Optional list of constraints (extracted automatically if not provided)</li> <li><code>**kwargs</code>: Additional parameters for generation</li> </ul> <p>Returns:</p> <ul> <li>Generated plan</li> </ul> <p>Example:</p> <pre><code># Generate a plan with automatically extracted constraints\nplan = plangen.generate_plan(\"Find an algorithm to sort a list of numbers.\")\n\n# Generate a plan with predefined constraints\nconstraints = [\n    \"The algorithm should have O(n log n) time complexity\",\n    \"The algorithm should be stable\",\n    \"The algorithm should use O(1) extra space\"\n]\nplan = plangen.generate_plan(\"Find an algorithm to sort a list of numbers.\", constraints)\n</code></pre>"},{"location":"api_reference/plangen/#extract_constraints","title":"<code>extract_constraints</code>","text":"<pre><code>def extract_constraints(self, problem: str) -&gt; List[str]\n</code></pre> <p>Extract constraints from a problem statement.</p> <p>Parameters:</p> <ul> <li><code>problem</code>: Problem statement</li> </ul> <p>Returns:</p> <ul> <li>List of extracted constraints</li> </ul> <p>Example:</p> <pre><code>constraints = plangen.extract_constraints(\n    \"Schedule a 30-minute meeting for 3 people. Alexander is busy from 9-10am and 2-3pm.\"\n)\nprint(constraints)\n# ['Meeting duration: 30 minutes', 'Alexander is busy from 9-10am', 'Alexander is busy from 2-3pm']\n</code></pre>"},{"location":"api_reference/plangen/#verify_plan","title":"<code>verify_plan</code>","text":"<pre><code>def verify_plan(\n    self,\n    problem: str,\n    plan: str,\n    constraints: Optional[List[str]] = None,\n    verifier: Optional[VerifierProtocol] = None,\n) -&gt; Tuple[str, float]\n</code></pre> <p>Verify a plan against constraints.</p> <p>Parameters:</p> <ul> <li><code>problem</code>: Problem statement</li> <li><code>plan</code>: Plan to verify</li> <li><code>constraints</code>: Optional list of constraints (extracted automatically if not provided)</li> <li><code>verifier</code>: Optional custom verifier</li> </ul> <p>Returns:</p> <ul> <li>Tuple of (feedback, score)</li> </ul> <p>Example:</p> <pre><code># Verify a plan with automatically extracted constraints\nfeedback, score = plangen.verify_plan(\n    \"Find an algorithm to sort a list of numbers.\",\n    \"I will use QuickSort to sort the list. QuickSort works by...\"\n)\nprint(f\"Score: {score}, Feedback: {feedback}\")\n\n# Verify with a custom verifier\nfrom plangen import Verifiers\nverifier = Verifiers.math()\nfeedback, score = plangen.verify_plan(\n    \"Solve the equation 2x + 3 = 7.\",\n    \"x = 2\",\n    verifier=verifier\n)\n</code></pre>"},{"location":"api_reference/prompts/","title":"Prompts","text":"<p>Prompt management and templating for PlanGEN.</p>"},{"location":"api_reference/prompts/#overview","title":"Overview","text":"<p>PlanGEN uses Jinja2 templates for prompt management. The <code>PromptManager</code> handles loading, rendering, and customization of prompts.</p>"},{"location":"api_reference/prompts/#classes","title":"Classes","text":""},{"location":"api_reference/prompts/#promptmanager","title":"PromptManager","text":"<pre><code>from plangen.prompts import PromptManager\n\nmanager = PromptManager()\n</code></pre>"},{"location":"api_reference/prompts/#methods","title":"Methods","text":"<p><code>render(template_name: str, **kwargs) -&gt; str</code></p> <p>Render a prompt template with variables.</p> <pre><code>prompt = manager.render(\"constraint_extraction\", problem=\"Schedule a meeting\")\n</code></pre> <p><code>get_system_message(agent_type: str) -&gt; str</code></p> <p>Get the system message for an agent type.</p> <pre><code>system_msg = manager.get_system_message(\"constraint\")\n</code></pre> <p><code>get_prompt(prompt_type: str, **kwargs) -&gt; str</code></p> <p>Get a rendered prompt by type.</p> <pre><code>prompt = manager.get_prompt(\"verification\", solution=\"...\", constraints=[\"...\"])\n</code></pre> <p><code>update_prompt(prompt_name: str, prompt_text: str) -&gt; None</code></p> <p>Override a prompt template at runtime.</p> <pre><code>manager.update_prompt(\"constraint_extraction\", \"Your custom template: {{ problem }}\")\n</code></pre>"},{"location":"api_reference/prompts/#template-location","title":"Template Location","text":"<p>Default templates are in <code>plangen/prompts/templates/</code>. Templates use Jinja2 syntax with <code>.j2</code> extension.</p>"},{"location":"api_reference/prompts/#see-also","title":"See Also","text":"<ul> <li>Agents - Agents that use prompt templates</li> </ul>"},{"location":"api_reference/verification/","title":"Verification","text":"<p>Verification strategies and mechanisms for PlanGEN.</p>"},{"location":"api_reference/verification/#overview","title":"Overview","text":"<p>PlanGEN uses a pluggable verification system. Domain-specific verifiers can validate solutions using specialized logic.</p>"},{"location":"api_reference/verification/#classes","title":"Classes","text":""},{"location":"api_reference/verification/#baseverifier","title":"BaseVerifier","text":"<p>Abstract base class for all verifiers.</p> <pre><code>class BaseVerifier(ABC):\n    @abstractmethod\n    def verify_solution(\n        self,\n        problem_statement: str,\n        solution: str,\n        constraints: list[str],\n    ) -&gt; VerificationResult | dict: ...\n\n    @abstractmethod\n    def is_applicable(self, problem_statement: str) -&gt; bool: ...\n\n    @abstractmethod\n    def extract_domain_constraints(\n        self,\n        problem_statement: str,\n        general_constraints: list[str],\n    ) -&gt; list[str]: ...\n</code></pre>"},{"location":"api_reference/verification/#mathverifier","title":"MathVerifier","text":"<p>Specialized verifier for mathematical problems.</p> <pre><code>from plangen.verification import MathVerifier\n\nverifier = MathVerifier()\nif verifier.is_applicable(problem):\n    result = verifier.verify_solution(problem, solution, constraints)\n</code></pre>"},{"location":"api_reference/verification/#verifierfactory","title":"VerifierFactory","text":"<p>Creates verifiers based on problem domain.</p> <pre><code>from plangen.verification import VerifierFactory\n\nverifier = VerifierFactory.create(\"math\")\n# or\nverifier = VerifierFactory.auto_detect(problem_statement)\n</code></pre>"},{"location":"api_reference/verification/#verification-result","title":"Verification Result","text":"<pre><code>class VerificationResult(TypedDict):\n    is_valid: bool      # Whether solution satisfies constraints\n    score: float        # Quality score (0-100)\n    reason: str         # Explanation of result\n    feedback: str       # Detailed feedback for improvement\n</code></pre>"},{"location":"api_reference/verification/#custom-verifiers","title":"Custom Verifiers","text":"<p>Implement <code>BaseVerifier</code> for domain-specific validation:</p> <pre><code>from plangen.verification import BaseVerifier\n\nclass SchedulingVerifier(BaseVerifier):\n    def verify_solution(self, problem, solution, constraints):\n        # Check time conflicts, capacity limits, etc.\n        return {\"is_valid\": True, \"score\": 85, \"reason\": \"...\", \"feedback\": \"...\"}\n\n    def is_applicable(self, problem):\n        return \"schedule\" in problem.lower() or \"meeting\" in problem.lower()\n\n    def extract_domain_constraints(self, problem, general_constraints):\n        # Extract time windows, resource limits, etc.\n        return general_constraints + [\"No double-booking\"]\n</code></pre>"},{"location":"api_reference/verification/#see-also","title":"See Also","text":"<ul> <li>Verifiers - High-level verifier factory API</li> <li>PlanGen - Using verifiers with the main API</li> </ul>"},{"location":"api_reference/verifiers/","title":"Verifiers API Reference","text":"<p>This page documents the verifier classes and interfaces in PlanGEN.</p>"},{"location":"api_reference/verifiers/#verifiers-factory","title":"Verifiers Factory","text":""},{"location":"api_reference/verifiers/#verifiers-class","title":"<code>Verifiers</code> Class","text":"<p>Factory class for creating common verifiers.</p> <pre><code>from plangen import Verifiers\n</code></pre>"},{"location":"api_reference/verifiers/#calendar","title":"<code>calendar()</code>","text":"<pre><code>@staticmethod\ndef calendar() -&gt; BaseVerifier\n</code></pre> <p>Create a calendar scheduling verifier.</p> <p>Returns:</p> <ul> <li>Verifier specialized for calendar scheduling problems</li> </ul> <p>Example:</p> <pre><code>verifier = Verifiers.calendar()\nplangen.solve(problem, verifier=verifier)\n</code></pre>"},{"location":"api_reference/verifiers/#math","title":"<code>math()</code>","text":"<pre><code>@staticmethod\ndef math() -&gt; BaseVerifier\n</code></pre> <p>Create a mathematical problem verifier.</p> <p>Returns:</p> <ul> <li>Verifier specialized for math problems</li> </ul> <p>Example:</p> <pre><code>verifier = Verifiers.math()\nplangen.solve(problem, verifier=verifier)\n</code></pre>"},{"location":"api_reference/verifiers/#algorithm","title":"<code>algorithm()</code>","text":"<pre><code>@staticmethod\ndef algorithm() -&gt; BaseVerifier\n</code></pre> <p>Create an algorithm design verifier.</p> <p>Returns:</p> <ul> <li>Verifier specialized for algorithm design problems</li> </ul> <p>Example:</p> <pre><code>verifier = Verifiers.algorithm()\nplangen.solve(problem, verifier=verifier)\n</code></pre>"},{"location":"api_reference/verifiers/#baseverifier","title":"BaseVerifier","text":"<p>Base class for all verifiers.</p>"},{"location":"api_reference/verifiers/#methods","title":"Methods","text":""},{"location":"api_reference/verifiers/#verify_solution","title":"<code>verify_solution</code>","text":"<pre><code>def verify_solution(\n    self,\n    problem: str,\n    solution: str,\n    constraints: List[str]\n) -&gt; Tuple[str, float]\n</code></pre> <p>Verify a solution against constraints.</p> <p>Parameters:</p> <ul> <li><code>problem</code>: Problem statement</li> <li><code>solution</code>: Proposed solution</li> <li><code>constraints</code>: List of constraints</li> </ul> <p>Returns:</p> <ul> <li>Tuple of (feedback: str, score: float)</li> </ul> <p>Example:</p> <pre><code>feedback, score = verifier.verify_solution(\n    problem=\"Schedule a meeting...\",\n    solution=\"Monday 10:00-10:30\",\n    constraints=[\"30 minutes\", \"3 participants\"]\n)\n</code></pre>"},{"location":"api_reference/verifiers/#is_applicable","title":"<code>is_applicable</code>","text":"<pre><code>def is_applicable(self, problem: str) -&gt; bool\n</code></pre> <p>Check if verifier applies to the problem.</p> <p>Parameters:</p> <ul> <li><code>problem</code>: Problem statement</li> </ul> <p>Returns:</p> <ul> <li>True if verifier is applicable, False otherwise</li> </ul> <p>Example:</p> <pre><code>if verifier.is_applicable(problem):\n    feedback, score = verifier.verify_solution(problem, solution, constraints)\n</code></pre>"},{"location":"api_reference/verifiers/#extract_domain_constraints","title":"<code>extract_domain_constraints</code>","text":"<pre><code>def extract_domain_constraints(self, problem: str) -&gt; List[str]\n</code></pre> <p>Extract domain-specific constraints from problem.</p> <p>Parameters:</p> <ul> <li><code>problem</code>: Problem statement</li> </ul> <p>Returns:</p> <ul> <li>List of domain-specific constraints</li> </ul> <p>Example:</p> <pre><code>constraints = verifier.extract_domain_constraints(problem)\n</code></pre>"},{"location":"api_reference/verifiers/#custom-verifier","title":"Custom Verifier","text":"<p>Create custom verifiers by subclassing BaseVerifier:</p> <pre><code>from typing import List, Tuple\nfrom plangen.verification.base_verifier import BaseVerifier\n\nclass CustomVerifier(BaseVerifier):\n    \"\"\"Custom domain-specific verifier.\"\"\"\n\n    def verify_solution(\n        self,\n        problem: str,\n        solution: str,\n        constraints: List[str]\n    ) -&gt; Tuple[str, float]:\n        \"\"\"Verify solution with custom logic.\"\"\"\n        feedback = []\n        score = 1.0\n\n        # Custom verification logic\n        for constraint in constraints:\n            if not self._check_constraint(solution, constraint):\n                feedback.append(f\"Failed: {constraint}\")\n                score -= 0.2\n\n        return \"\\n\".join(feedback) or \"Valid\", max(0.0, score)\n\n    def _check_constraint(self, solution: str, constraint: str) -&gt; bool:\n        \"\"\"Check specific constraint.\"\"\"\n        # Implementation\n        return True\n\n    def is_applicable(self, problem: str) -&gt; bool:\n        \"\"\"Check if verifier applies.\"\"\"\n        return \"your_domain_keyword\" in problem.lower()\n\n    def extract_domain_constraints(self, problem: str) -&gt; List[str]:\n        \"\"\"Extract domain constraints.\"\"\"\n        # Implementation\n        return []\n\n# Usage\ncustom_verifier = CustomVerifier()\nresult = plangen.solve(problem, verifier=custom_verifier)\n</code></pre>"},{"location":"api_reference/verifiers/#verifierfactory","title":"VerifierFactory","text":"<p>Factory for registering and retrieving verifiers.</p>"},{"location":"api_reference/verifiers/#register","title":"<code>register</code>","text":"<pre><code>@classmethod\ndef register(cls, name: str, verifier_class: Type[BaseVerifier]) -&gt; None\n</code></pre> <p>Register a custom verifier.</p> <p>Parameters:</p> <ul> <li><code>name</code>: Name for the verifier</li> <li><code>verifier_class</code>: Verifier class</li> </ul> <p>Example:</p> <pre><code>from plangen.verification import VerifierFactory\n\nVerifierFactory.register(\"custom\", CustomVerifier)\n</code></pre>"},{"location":"api_reference/verifiers/#get","title":"<code>get</code>","text":"<pre><code>@classmethod\ndef get(cls, name: str) -&gt; BaseVerifier\n</code></pre> <p>Get a registered verifier by name.</p> <p>Parameters:</p> <ul> <li><code>name</code>: Verifier name</li> </ul> <p>Returns:</p> <ul> <li>Verifier instance</li> </ul> <p>Example:</p> <pre><code>verifier = VerifierFactory.get(\"custom\")\n</code></pre>"},{"location":"api_reference/verifiers/#get_applicable","title":"<code>get_applicable</code>","text":"<pre><code>@classmethod\ndef get_applicable(cls, problem: str) -&gt; Optional[BaseVerifier]\n</code></pre> <p>Get the first applicable verifier for a problem.</p> <p>Parameters:</p> <ul> <li><code>problem</code>: Problem statement</li> </ul> <p>Returns:</p> <ul> <li>Applicable verifier or None</li> </ul> <p>Example:</p> <pre><code>verifier = VerifierFactory.get_applicable(problem)\nif verifier:\n    feedback, score = verifier.verify_solution(problem, solution, constraints)\n</code></pre>"},{"location":"api_reference/verifiers/#complete-example","title":"Complete Example","text":"<pre><code>from plangen import PlanGen, Verifiers\nfrom plangen.verification.base_verifier import BaseVerifier\nfrom typing import List, Tuple\n\n# Use built-in verifier\nplangen = PlanGen.create()\ncalendar_verifier = Verifiers.calendar()\n\nresult = plangen.solve(\n    \"Schedule a meeting...\",\n    verifier=calendar_verifier\n)\n\n# Create custom verifier\nclass EmailVerifier(BaseVerifier):\n    def verify_solution(\n        self, \n        problem: str,\n        solution: str,\n        constraints: List[str]\n    ) -&gt; Tuple[str, float]:\n        # Custom logic for email validation\n        score = 1.0\n        feedback = []\n\n        if \"@\" not in solution:\n            feedback.append(\"Missing @ symbol\")\n            score -= 0.5\n\n        if \".\" not in solution.split(\"@\")[-1]:\n            feedback.append(\"Invalid domain\")\n            score -= 0.3\n\n        return \"\\n\".join(feedback) or \"Valid email\", max(0.0, score)\n\n    def is_applicable(self, problem: str) -&gt; bool:\n        return \"email\" in problem.lower()\n\n# Use custom verifier\nemail_verifier = EmailVerifier()\nresult = plangen.solve(\n    \"Validate this email address...\",\n    verifier=email_verifier\n)\n</code></pre>"},{"location":"api_reference/verifiers/#see-also","title":"See Also","text":"<ul> <li>Verification Guide</li> <li>Custom Verification Example</li> <li>Algorithm API</li> </ul>"},{"location":"api_reference/visualization/","title":"Visualization API Reference","text":"<p>This page documents the visualization classes and interfaces in PlanGEN.</p>"},{"location":"api_reference/visualization/#graphrenderer","title":"GraphRenderer","text":"<p>The main visualization class for rendering algorithm execution.</p>"},{"location":"api_reference/visualization/#constructor","title":"Constructor","text":"<pre><code>def __init__(\n    self,\n    output_dir: str = \"./visualizations\",\n    format: str = \"png\",\n    dpi: int = 100,\n    figsize: tuple = (10, 8)\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>output_dir</code>: Directory to save visualizations</li> <li><code>format</code>: Output format ('png', 'svg', 'pdf')</li> <li><code>dpi</code>: Resolution for raster formats</li> <li><code>figsize</code>: Figure size as (width, height) in inches</li> </ul> <p>Example:</p> <pre><code>from plangen.visualization import GraphRenderer\n\nrenderer = GraphRenderer(\n    output_dir=\"./my_visualizations\",\n    format=\"png\",\n    dpi=300,\n    figsize=(12, 8)\n)\n</code></pre>"},{"location":"api_reference/visualization/#methods","title":"Methods","text":""},{"location":"api_reference/visualization/#notify","title":"<code>notify</code>","text":"<pre><code>def notify(self, event: str, data: Dict[str, Any]) -&gt; None\n</code></pre> <p>Receive notifications from algorithms during execution.</p> <p>Parameters:</p> <ul> <li><code>event</code>: Event type (e.g., \"plan_generated\", \"plan_verified\")</li> <li><code>data</code>: Event data dictionary</li> </ul> <p>Example:</p> <pre><code># Typically called automatically by algorithms\nalgorithm.add_observer(renderer)\n</code></pre>"},{"location":"api_reference/visualization/#render_tree","title":"<code>render_tree</code>","text":"<pre><code>def render_tree(\n    self,\n    tree_data: Dict[str, Any],\n    filename: str = \"tree_structure.png\"\n) -&gt; None\n</code></pre> <p>Render a tree structure visualization.</p> <p>Parameters:</p> <ul> <li><code>tree_data</code>: Tree structure data</li> <li><code>filename</code>: Output filename</li> </ul>"},{"location":"api_reference/visualization/#render_scores","title":"<code>render_scores</code>","text":"<pre><code>def render_scores(\n    self,\n    scores: List[float],\n    filename: str = \"scores.png\"\n) -&gt; None\n</code></pre> <p>Render score progression plot.</p> <p>Parameters:</p> <ul> <li><code>scores</code>: List of scores over time</li> <li><code>filename</code>: Output filename</li> </ul>"},{"location":"api_reference/visualization/#observerprotocol","title":"ObserverProtocol","text":"<p>Protocol for implementing custom observers.</p> <pre><code>from typing import Protocol, Dict, Any\n\nclass ObserverProtocol(Protocol):\n    \"\"\"Protocol for algorithm observers.\"\"\"\n\n    def notify(self, event: str, data: Dict[str, Any]) -&gt; None:\n        \"\"\"Receive notifications from algorithms.\"\"\"\n        ...\n</code></pre>"},{"location":"api_reference/visualization/#custom-observer-example","title":"Custom Observer Example","text":"<pre><code>from typing import Dict, Any\nimport json\n\nclass JsonLogger:\n    \"\"\"Observer that logs events to JSON.\"\"\"\n\n    def __init__(self, output_file: str):\n        self.output_file = output_file\n        self.events = []\n\n    def notify(self, event: str, data: Dict[str, Any]) -&gt; None:\n        \"\"\"Log event to list.\"\"\"\n        self.events.append({\n            'event': event,\n            'timestamp': time.time(),\n            'data': data\n        })\n\n    def save(self) -&gt; None:\n        \"\"\"Save events to file.\"\"\"\n        with open(self.output_file, 'w') as f:\n            json.dump(self.events, f, indent=2)\n\n# Usage\nlogger = JsonLogger(\"events.json\")\nalgorithm.add_observer(logger)\nsolution, score, metadata = algorithm.run(problem)\nlogger.save()\n</code></pre>"},{"location":"api_reference/visualization/#visualization-types","title":"Visualization Types","text":""},{"location":"api_reference/visualization/#tree-visualizations","title":"Tree Visualizations","text":"<p>Used with TreeOfThought algorithm to show exploration paths.</p>"},{"location":"api_reference/visualization/#score-plots","title":"Score Plots","text":"<p>Used with REBASE to show iterative improvement.</p>"},{"location":"api_reference/visualization/#comparison-charts","title":"Comparison Charts","text":"<p>Used with BestOfN to compare generated plans.</p>"},{"location":"api_reference/visualization/#see-also","title":"See Also","text":"<ul> <li>Visualization Guide</li> <li>Algorithm API</li> <li>Examples</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>This section provides code examples for various use cases of PlanGEN.</p>"},{"location":"examples/#basic-examples","title":"Basic Examples","text":"<ul> <li>Simple Example - A simple example of using PlanGEN with the default settings</li> <li>OpenAI Example - An example of using PlanGEN with OpenAI models</li> <li>Bedrock Example - An example of using PlanGEN with AWS Bedrock models</li> </ul>"},{"location":"examples/#algorithm-examples","title":"Algorithm Examples","text":"<ul> <li>BestOfN Example - An example of using the BestOfN algorithm</li> <li>TreeOfThought Example - An example of using the TreeOfThought algorithm</li> <li>REBASE Example - An example of using the REBASE algorithm</li> <li>MixtureOfAlgorithms Example - An example of using the MixtureOfAlgorithms algorithm</li> </ul>"},{"location":"examples/#advanced-examples","title":"Advanced Examples","text":"<ul> <li>Custom Verification - An example of using custom verification strategies</li> <li>Visualization Example - An example of visualizing the planning process</li> <li>Custom Prompts - An example of customizing prompts</li> </ul>"},{"location":"examples/bedrock_example/","title":"AWS Bedrock Example","text":"<p>Example using AWS Bedrock models with PlanGEN.</p> <pre><code>from plangen import PlanGen\n\n# Create PlanGen with Bedrock\nplangen = PlanGen.with_bedrock(\n    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n    region=\"us-east-1\"\n)\n\n# Solve problem\nresult = plangen.solve(\"Your problem here\")\nprint(result[\"selected_solution\"])\n</code></pre>"},{"location":"examples/best_of_n_example/","title":"BestOfN Algorithm Example","text":"<p>Example using the BestOfN algorithm.</p> <pre><code>from plangen import PlanGen\n\nplangen = PlanGen.create()\n\nresult = plangen.solve(\n    problem=\"Your problem\",\n    algorithm=\"best_of_n\",\n    n_plans=5,\n    parallel=True\n)\n</code></pre>"},{"location":"examples/custom_prompts_example/","title":"Custom Prompts Example","text":"<p>Example customizing prompts.</p> <pre><code>from plangen.prompts import PromptManager\n\nprompt_manager = PromptManager()\nprompt_manager.update_prompt(\n    \"constraint_extraction\",\n    \"Your custom prompt template\"\n)\n</code></pre>"},{"location":"examples/custom_verification/","title":"Custom Verification Example","text":"<p>Example creating custom verifiers.</p> <pre><code>from plangen import PlanGen\nfrom plangen.verification.base_verifier import BaseVerifier\n\nclass CustomVerifier(BaseVerifier):\n    def verify_solution(self, problem, solution, constraints):\n        # Custom logic\n        return \"Feedback\", 0.9\n\nplangen = PlanGen.create()\nverifier = CustomVerifier()\nresult = plangen.solve(problem, verifier=verifier)\n</code></pre>"},{"location":"examples/mixture_of_algorithms_example/","title":"Mixture of Algorithms Example","text":"<p>Example using automatic algorithm selection.</p> <pre><code>from plangen import PlanGen\n\nplangen = PlanGen.create()\n\nresult = plangen.solve(\n    problem=\"Your problem\",\n    algorithm=\"mixture\"\n)\n\nprint(f\"Used: {result['metadata']['algorithm_used']}\")\n</code></pre>"},{"location":"examples/openai_example/","title":"OpenAI Example","text":"<p>Complete example using OpenAI models with PlanGEN.</p> <pre><code>from plangen import PlanGen\n\n# Create PlanGen with OpenAI\nplangen = PlanGen.with_openai(\n    model_name=\"gpt-4o\",\n    temperature=0.7,\n    max_tokens=2048\n)\n\n# Define problem\nproblem = \"\"\"\nSchedule a 30-minute meeting for Alexander, Elizabeth, and Walter on Monday.\nAlexander: Busy 9:30-10:00, 10:30-11:00, 12:30-13:00, 14:30-15:00, 16:00-17:00\nElizabeth: Busy 9:00-9:30, 11:30-12:30, 13:00-14:30\nWalter: Busy 9:00-14:30, 15:30-17:00\nFind earliest available time.\n\"\"\"\n\n# Solve\nresult = plangen.solve(problem)\nprint(result[\"selected_solution\"])\n</code></pre> <p>See Simple Example for more details.</p>"},{"location":"examples/rebase_example/","title":"REBASE Algorithm Example","text":"<p>Example using REBASE algorithm.</p> <pre><code>from plangen import PlanGen\n\nplangen = PlanGen.create()\n\nresult = plangen.solve(\n    problem=\"Your problem\",\n    algorithm=\"rebase\",\n    max_iterations=5\n)\n</code></pre>"},{"location":"examples/simple_example/","title":"Simple Example","text":"<p>This example demonstrates the basic usage of PlanGEN using the new public API.</p>"},{"location":"examples/simple_example/#full-example","title":"Full Example","text":"<pre><code>\"\"\"\nSimple example of using PlanGEN with the new public API\n\"\"\"\n\nimport json\nimport os\nfrom dotenv import load_dotenv\n\nfrom plangen import PlanGen\n\n# Load environment variables from .env file\nload_dotenv()\n\n\ndef main():\n    \"\"\"Run a simple example of PlanGEN with the new public API.\"\"\"\n    # Create a PlanGen instance\n    # This will automatically use OpenAI if OPENAI_API_KEY is set,\n    # or try to fall back to AWS Bedrock if AWS credentials are available\n    plangen = PlanGen.create()\n\n    # You can also explicitly specify the model:\n    # plangen = PlanGen.with_openai(model_name=\"gpt-4o\")\n    # plangen = PlanGen.with_bedrock(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\")\n\n    # Define a problem\n    problem = \"\"\"\n    Design an algorithm to find the kth largest element in an unsorted array.\n    For example, given [3, 2, 1, 5, 6, 4] and k = 2, the output should be 5.\n    \"\"\"\n\n    print(f\"Problem: {problem}\")\n    print(\"\\nSolving problem...\")\n\n    # Solve the problem\n    result = plangen.solve(problem)\n\n    # Print the extracted constraints\n    print(\"\\n=== Extracted Constraints ===\")\n    constraints = result.get(\"constraints\", [])\n    for i, constraint in enumerate(constraints, 1):\n        print(f\"{i}. {constraint}\")\n\n    # Print the selected solution\n    print(\"\\n=== Selected Solution ===\")\n    print(result.get(\"selected_solution\", \"No solution found\"))\n\n    # Save the results to a file\n    with open(\"plangen_results.json\", \"w\") as f:\n        json.dump(result, f, indent=2)\n\n    print(\"\\nResults saved to plangen_results.json\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/simple_example/#step-by-step-explanation","title":"Step-by-Step Explanation","text":"<ol> <li>Import the necessary modules:</li> </ol> <pre><code>import json\nimport os\nfrom dotenv import load_dotenv\nfrom plangen import PlanGen\n</code></pre> <ol> <li>Load environment variables:</li> </ol> <pre><code>load_dotenv()\n</code></pre> <p>This loads API keys from a <code>.env</code> file if one exists.</p> <ol> <li>Create a PlanGen instance:</li> </ol> <pre><code>plangen = PlanGen.create()\n</code></pre> <p>This creates a PlanGen instance with default settings. It will automatically use the OpenAI API if an API key is available, or fall back to AWS Bedrock if AWS credentials are available.</p> <ol> <li>Define a problem:</li> </ol> <pre><code>problem = \"\"\"\nDesign an algorithm to find the kth largest element in an unsorted array.\nFor example, given [3, 2, 1, 5, 6, 4] and k = 2, the output should be 5.\n\"\"\"\n</code></pre> <ol> <li>Solve the problem:</li> </ol> <pre><code>result = plangen.solve(problem)\n</code></pre> <p>This will:    - Extract constraints from the problem    - Generate multiple solutions    - Verify the solutions    - Select the best solution</p> <ol> <li>Access the results:</li> </ol> <pre><code>constraints = result.get(\"constraints\", [])\nselected_solution = result.get(\"selected_solution\", \"No solution found\")\n</code></pre> <ol> <li>Save the results:</li> </ol> <pre><code>with open(\"plangen_results.json\", \"w\") as f:\n    json.dump(result, f, indent=2)\n</code></pre>"},{"location":"examples/simple_example/#alternative-model-configurations","title":"Alternative Model Configurations","text":"<p>You can create a PlanGen instance with a specific model:</p> <pre><code># Using OpenAI\nplangen = PlanGen.with_openai(\n    model_name=\"gpt-4o\",\n    temperature=0.7,\n    max_tokens=1024\n)\n\n# Using AWS Bedrock\nplangen = PlanGen.with_bedrock(\n    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n    region=\"us-east-1\",\n    temperature=0.7,\n    max_tokens=1024\n)\n</code></pre>"},{"location":"examples/simple_example/#using-a-specific-algorithm","title":"Using a Specific Algorithm","text":"<p>You can also specify which algorithm to use when solving a problem:</p> <pre><code>result = plangen.solve(\n    problem,\n    algorithm=\"best_of_n\",\n    n_plans=5,\n    sampling_strategy=\"diverse\"\n)\n</code></pre> <p>Available algorithms are:</p> <ul> <li><code>\"best_of_n\"</code> - Generates multiple plans and selects the best one</li> <li><code>\"tree_of_thought\"</code> - Explores multiple reasoning paths in a tree structure</li> <li><code>\"rebase\"</code> - Uses recursive refinement to improve plans</li> <li><code>\"mixture\"</code> - Dynamically selects the best algorithm for the problem</li> </ul>"},{"location":"examples/tree_of_thought_example/","title":"TreeOfThought Algorithm Example","text":"<p>Example using TreeOfThought algorithm.</p> <pre><code>from plangen import PlanGen\n\nplangen = PlanGen.create()\n\nresult = plangen.solve(\n    problem=\"Your problem\",\n    algorithm=\"tree_of_thought\",\n    branching_factor=3,\n    max_depth=5\n)\n</code></pre>"},{"location":"examples/visualization_example/","title":"Visualization Example","text":"<p>Example using visualization tools.</p> <pre><code>from plangen.visualization import GraphRenderer\nfrom plangen.algorithms import TreeOfThought\n\nrenderer = GraphRenderer(output_dir=\"./viz\")\nalgorithm = TreeOfThought(llm_interface=model)\nalgorithm.add_observer(renderer)\n\nsolution, score, metadata = algorithm.run(problem)\n</code></pre>"},{"location":"user_guide/","title":"User Guide","text":"<p>This guide will help you get started with PlanGEN and walk you through common use cases and best practices.</p>"},{"location":"user_guide/#contents","title":"Contents","text":"<ul> <li>Installation - How to install PlanGEN and set up your environment</li> <li>Quick Start - A quick introduction to PlanGEN</li> <li>Configuration - How to configure PlanGEN</li> <li>Models - Working with different LLM providers</li> <li>Custom Prompts - How to customize the prompts used by PlanGEN</li> <li>Verification - How to verify solutions</li> <li>Visualization - How to visualize the planning process</li> </ul>"},{"location":"user_guide/configuration/","title":"Configuration Guide","text":"<p>This guide explains how to configure PlanGEN for different use cases and environments.</p>"},{"location":"user_guide/configuration/#environment-variables","title":"Environment Variables","text":"<p>PlanGEN uses environment variables for API keys and configuration. Create a <code>.env</code> file in your project root:</p> <pre><code># OpenAI Configuration\nOPENAI_API_KEY=your-openai-api-key-here\nOPENAI_MODEL=gpt-4o  # Optional, defaults to gpt-4o\n\n# AWS Bedrock Configuration (if using Bedrock)\nAWS_ACCESS_KEY_ID=your-aws-access-key\nAWS_SECRET_ACCESS_KEY=your-aws-secret-key\nAWS_DEFAULT_REGION=us-east-1\n\n# Optional: Logging Configuration\nLOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL\n</code></pre> <p>Load environment variables in your code:</p> <pre><code>from dotenv import load_dotenv\n\nload_dotenv()  # Load from .env file\n</code></pre>"},{"location":"user_guide/configuration/#model-configuration","title":"Model Configuration","text":""},{"location":"user_guide/configuration/#temperature","title":"Temperature","text":"<p>Temperature controls the randomness of the model's output:</p> <ul> <li>Low temperature (0.1-0.3): More deterministic and focused output</li> <li>Medium temperature (0.5-0.7): Balanced creativity and consistency (recommended)</li> <li>High temperature (0.8-1.0): More creative and diverse output</li> </ul> <pre><code>from plangen import PlanGen\n\n# Low temperature for deterministic output\nplangen = PlanGen.create(temperature=0.2)\n\n# High temperature for creative solutions\nplangen = PlanGen.create(temperature=0.9)\n</code></pre>"},{"location":"user_guide/configuration/#max-tokens","title":"Max Tokens","text":"<p>Control the maximum length of generated responses:</p> <pre><code>from plangen import PlanGen\n\n# Shorter responses for simple problems\nplangen = PlanGen.create(max_tokens=512)\n\n# Longer responses for complex problems\nplangen = PlanGen.create(max_tokens=4096)\n</code></pre>"},{"location":"user_guide/configuration/#model-selection","title":"Model Selection","text":"<p>Choose the appropriate model for your use case:</p> <pre><code>from plangen import PlanGen\n\n# Fast and cost-effective\nplangen = PlanGen.with_openai(model_name=\"gpt-3.5-turbo\")\n\n# Best performance\nplangen = PlanGen.with_openai(model_name=\"gpt-4o\")\n\n# For AWS Bedrock\nplangen = PlanGen.with_bedrock(\n    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n    region=\"us-east-1\"\n)\n</code></pre>"},{"location":"user_guide/configuration/#algorithm-configuration","title":"Algorithm Configuration","text":""},{"location":"user_guide/configuration/#default-algorithm","title":"Default Algorithm","text":"<p>Configure the default algorithm for your application:</p> <pre><code>from plangen import PlanGen\n\nplangen = PlanGen.create()\n\n# The solve method uses BestOfN by default\nresult = plangen.solve(problem)\n</code></pre>"},{"location":"user_guide/configuration/#custom-algorithm-parameters","title":"Custom Algorithm Parameters","text":"<p>Each algorithm has specific parameters:</p> <pre><code>from plangen import PlanGen\n\nplangen = PlanGen.create()\n\n# BestOfN configuration\nresult = plangen.solve(\n    problem,\n    algorithm=\"best_of_n\",\n    n_plans=10,  # Generate 10 plans\n    sampling_strategy=\"diverse\",  # Use diverse sampling\n    parallel=True  # Generate in parallel\n)\n\n# TreeOfThought configuration\nresult = plangen.solve(\n    problem,\n    algorithm=\"tree_of_thought\",\n    branching_factor=3,  # 3 branches per node\n    max_depth=5,  # Maximum depth of 5\n    beam_width=2  # Keep top 2 branches\n)\n\n# REBASE configuration\nresult = plangen.solve(\n    problem,\n    algorithm=\"rebase\",\n    max_iterations=10,  # Maximum 10 refinement iterations\n    improvement_threshold=0.1  # Stop if improvement &lt; 0.1\n)\n</code></pre>"},{"location":"user_guide/configuration/#prompt-configuration","title":"Prompt Configuration","text":""},{"location":"user_guide/configuration/#custom-prompts","title":"Custom Prompts","text":"<p>Customize the prompts used by PlanGEN:</p> <pre><code>from plangen import PlanGen\nfrom plangen.prompts import PromptManager\n\n# Create a custom prompt manager\nprompt_manager = PromptManager()\n\n# Update constraint extraction prompt\nprompt_manager.update_prompt(\n    \"constraint_extraction\",\n    \"\"\"You are an expert at analyzing problems and identifying constraints.\n\n    Problem: {problem}\n\n    List all constraints, requirements, and limitations in the problem.\n    Format your response as a numbered list.\"\"\"\n)\n\n# Create PlanGen with custom prompts\nplangen = PlanGen.with_model(model)\nplangen._plangen.prompt_manager = prompt_manager\n</code></pre>"},{"location":"user_guide/configuration/#verification-configuration","title":"Verification Configuration","text":""},{"location":"user_guide/configuration/#custom-verifiers","title":"Custom Verifiers","text":"<p>Use domain-specific verifiers for better validation:</p> <pre><code>from plangen import PlanGen, Verifiers\n\nplangen = PlanGen.create()\n\n# Use a calendar scheduling verifier\nverifier = Verifiers.calendar()\n\nresult = plangen.solve(\n    problem=\"Schedule a meeting...\",\n    verifier=verifier\n)\n</code></pre>"},{"location":"user_guide/configuration/#verification-thresholds","title":"Verification Thresholds","text":"<p>Configure verification score thresholds:</p> <pre><code>from plangen.algorithms import BestOfN\n\nbest_of_n = BestOfN(\n    n_plans=5,\n    min_score_threshold=0.7,  # Only accept plans with score &gt;= 0.7\n    llm_interface=model\n)\n</code></pre>"},{"location":"user_guide/configuration/#visualization-configuration","title":"Visualization Configuration","text":""},{"location":"user_guide/configuration/#enable-visualization","title":"Enable Visualization","text":"<p>Configure visualization for debugging and analysis:</p> <pre><code>from plangen.visualization import GraphRenderer\nfrom plangen.algorithms import TreeOfThought\n\n# Create a graph renderer\nrenderer = GraphRenderer(output_dir=\"./visualizations\")\n\n# Create algorithm with visualization\ntot = TreeOfThought(\n    branching_factor=3,\n    max_depth=5,\n    llm_interface=model\n)\n\n# Register the renderer as an observer\ntot.add_observer(renderer)\n\n# Run the algorithm (visualization will be generated)\nbest_plan, score, metadata = tot.run(problem)\n</code></pre>"},{"location":"user_guide/configuration/#logging-configuration","title":"Logging Configuration","text":""},{"location":"user_guide/configuration/#basic-logging","title":"Basic Logging","text":"<p>Configure Python logging for PlanGEN:</p> <pre><code>import logging\n\n# Set logging level\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\n# Get PlanGEN logger\nlogger = logging.getLogger('plangen')\nlogger.setLevel(logging.DEBUG)\n</code></pre>"},{"location":"user_guide/configuration/#advanced-logging","title":"Advanced Logging","text":"<p>Configure logging to file:</p> <pre><code>import logging\nfrom logging.handlers import RotatingFileHandler\n\n# Create file handler\nhandler = RotatingFileHandler(\n    'plangen.log',\n    maxBytes=10485760,  # 10MB\n    backupCount=5\n)\nhandler.setFormatter(\n    logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n)\n\n# Add handler to PlanGEN logger\nlogger = logging.getLogger('plangen')\nlogger.addHandler(handler)\nlogger.setLevel(logging.DEBUG)\n</code></pre>"},{"location":"user_guide/configuration/#performance-configuration","title":"Performance Configuration","text":""},{"location":"user_guide/configuration/#caching","title":"Caching","text":"<p>Enable caching for improved performance:</p> <pre><code>from plangen import PlanGen\nfrom functools import lru_cache\n\n# Cache constraint extraction results\n@lru_cache(maxsize=128)\ndef cached_extract_constraints(problem):\n    plangen = PlanGen.create()\n    return tuple(plangen.extract_constraints(problem))\n</code></pre>"},{"location":"user_guide/configuration/#parallel-processing","title":"Parallel Processing","text":"<p>Enable parallel plan generation:</p> <pre><code>from plangen import PlanGen\n\nplangen = PlanGen.create()\n\n# Enable parallel processing for BestOfN\nresult = plangen.solve(\n    problem,\n    algorithm=\"best_of_n\",\n    n_plans=10,\n    parallel=True  # Generate plans in parallel\n)\n</code></pre>"},{"location":"user_guide/configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Use environment variables for sensitive information like API keys</li> <li>Start with default parameters and adjust based on results</li> <li>Choose the right model for your use case (cost vs. performance)</li> <li>Enable logging for production environments</li> <li>Use custom verifiers for domain-specific problems</li> <li>Configure appropriate timeouts for API calls</li> <li>Monitor API usage and costs</li> <li>Test configurations with simple problems first</li> </ol>"},{"location":"user_guide/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Models in detail</li> <li>Explore Custom Prompts</li> <li>See Verification for advanced verification strategies</li> </ul>"},{"location":"user_guide/custom_prompts/","title":"Custom Prompts Guide","text":"<p>PlanGEN uses Jinja2 templates for all LLM interactions. This guide explains how to customize prompts to improve performance for your specific use cases.</p>"},{"location":"user_guide/custom_prompts/#understanding-the-prompt-system","title":"Understanding the Prompt System","text":"<p>PlanGEN uses prompts for different stages of the planning workflow:</p> <ol> <li>Constraint Extraction: Identify constraints from problem statements</li> <li>Solution Generation: Generate candidate solutions</li> <li>Verification: Evaluate solutions against constraints</li> <li>Selection: Choose the best solution from candidates</li> </ol>"},{"location":"user_guide/custom_prompts/#prompt-manager","title":"Prompt Manager","text":"<p>The <code>PromptManager</code> class manages all prompts:</p> <pre><code>from plangen.prompts import PromptManager\n\n# Create a prompt manager\nprompt_manager = PromptManager()\n\n# Get current prompt template\nconstraint_prompt = prompt_manager.get_prompt(\"constraint_extraction\")\nprint(constraint_prompt)\n</code></pre>"},{"location":"user_guide/custom_prompts/#customizing-prompts","title":"Customizing Prompts","text":""},{"location":"user_guide/custom_prompts/#method-1-using-update_prompt","title":"Method 1: Using update_prompt","text":"<pre><code>from plangen.prompts import PromptManager\nfrom plangen import PlanGen\nfrom plangen.models import OpenAIModelInterface\n\n# Create a custom prompt manager\nprompt_manager = PromptManager()\n\n# Update constraint extraction prompt\nprompt_manager.update_prompt(\n    \"constraint_extraction\",\n    \"\"\"You are an expert at analyzing scheduling problems.\n\n    Problem: {problem}\n\n    Identify all constraints including:\n    - Time constraints\n    - Resource constraints\n    - Person availability\n    - Meeting requirements\n\n    List each constraint clearly and concisely.\"\"\"\n)\n\n# Create model and use custom prompts\nmodel = OpenAIModelInterface(model_name=\"gpt-4o\")\nplangen = PlanGen.with_model(model)\nplangen._plangen.prompt_manager = prompt_manager\n\n# Use PlanGen with custom prompts\nresult = plangen.solve(problem)\n</code></pre>"},{"location":"user_guide/custom_prompts/#method-2-using-custom-template-files","title":"Method 2: Using Custom Template Files","text":"<p>Create a custom template directory:</p> <pre><code>my_prompts/\n\u251c\u2500\u2500 constraint_extraction.j2\n\u251c\u2500\u2500 solution_generation.j2\n\u251c\u2500\u2500 verification.j2\n\u2514\u2500\u2500 selection.j2\n</code></pre> <p>Load custom templates:</p> <pre><code>from plangen.prompts import PromptManager\n\n# Load custom templates from directory\nprompt_manager = PromptManager(template_dir=\"my_prompts\")\n</code></pre>"},{"location":"user_guide/custom_prompts/#prompt-templates","title":"Prompt Templates","text":""},{"location":"user_guide/custom_prompts/#constraint-extraction","title":"Constraint Extraction","text":"<p>Default template:</p> <pre><code>You are an expert at analyzing problems and extracting constraints.\n\nProblem: {{ problem }}\n\nAnalyze the problem and identify all constraints, requirements, and limitations.\nList each constraint on a separate line.\n</code></pre> <p>Custom example for scheduling:</p> <pre><code>You are an expert scheduler with deep knowledge of time management.\n\nProblem: {{ problem }}\n\nExtract all constraints from this scheduling problem:\n1. Time windows and availability\n2. Duration requirements\n3. Resource conflicts\n4. Priority requirements\n5. Any other limitations\n\nFormat: List each constraint as a bullet point.\n</code></pre>"},{"location":"user_guide/custom_prompts/#solution-generation","title":"Solution Generation","text":"<p>Default template:</p> <pre><code>You are an expert problem solver.\n\nProblem: {{ problem }}\n\n{% if constraints %}\nConstraints:\n{% for constraint in constraints %}\n- {{ constraint }}\n{% endfor %}\n{% endif %}\n\nGenerate a detailed solution that satisfies all constraints.\n</code></pre> <p>Custom example with emphasis on creativity:</p> <pre><code>You are a creative problem solver who thinks outside the box.\n\nProblem: {{ problem }}\n\n{% if constraints %}\nRequirements:\n{% for constraint in constraints %}\n\u2713 {{ constraint }}\n{% endfor %}\n{% endif %}\n\nThink creatively and propose an innovative solution. Consider:\n- Alternative approaches\n- Edge cases\n- Potential optimizations\n\nProvide a detailed, step-by-step solution.\n</code></pre>"},{"location":"user_guide/custom_prompts/#verification","title":"Verification","text":"<p>Default template:</p> <pre><code>You are an expert at verifying solutions.\n\nProblem: {{ problem }}\nSolution: {{ solution }}\n\n{% if constraints %}\nConstraints:\n{% for constraint in constraints %}\n- {{ constraint }}\n{% endfor %}\n{% endif %}\n\nVerify if the solution satisfies all constraints.\nProvide feedback and a score from 0.0 to 1.0.\n</code></pre> <p>Custom example with detailed rubric:</p> <pre><code>You are a meticulous solution verifier.\n\nProblem: {{ problem }}\nProposed Solution: {{ solution }}\n\n{% if constraints %}\nRequirements:\n{% for constraint in constraints %}\n{{ loop.index }}. {{ constraint }}\n{% endfor %}\n{% endif %}\n\nEvaluate the solution:\n\nFor each requirement:\n- Check if it's satisfied (Yes/No)\n- Provide specific feedback\n- Note any issues\n\nCalculate score:\n- 1.0: All requirements perfectly satisfied\n- 0.8-0.9: Minor issues or improvements needed\n- 0.6-0.7: Some requirements not met\n- 0.4-0.5: Multiple problems\n- 0.0-0.3: Fundamental issues\n\nFormat your response as:\nScore: [0.0-1.0]\nFeedback: [Detailed explanation]\n</code></pre>"},{"location":"user_guide/custom_prompts/#selection","title":"Selection","text":"<p>Default template:</p> <pre><code>You are an expert at comparing and selecting solutions.\n\nProblem: {{ problem }}\n\n{% for solution in solutions %}\nSolution {{ loop.index }} (Score: {{ solution.score }}):\n{{ solution.text }}\n\nFeedback: {{ solution.feedback }}\n---\n{% endfor %}\n\nSelect the best solution and explain your reasoning.\n</code></pre>"},{"location":"user_guide/custom_prompts/#domain-specific-prompt-examples","title":"Domain-Specific Prompt Examples","text":""},{"location":"user_guide/custom_prompts/#calendar-scheduling","title":"Calendar Scheduling","text":"<pre><code>prompt_manager.update_prompt(\n    \"constraint_extraction\",\n    \"\"\"You are an expert calendar scheduling assistant.\n\nProblem: {{ problem }}\n\nExtract scheduling constraints:\n1. PARTICIPANTS: List all participants\n2. DURATION: Meeting duration\n3. TIME_WINDOW: Available time range\n4. BUSY_TIMES: Each participant's busy slots\n5. PREFERENCES: Any scheduling preferences\n6. REQUIREMENTS: Additional requirements (earliest time, etc.)\n\nBe precise with times and participants.\"\"\"\n)\n\nprompt_manager.update_prompt(\n    \"solution_generation\",\n    \"\"\"You are an expert at finding optimal meeting times.\n\nProblem: {{ problem }}\n\n{% if constraints %}\nConstraints:\n{% for constraint in constraints %}\n\u2022 {{ constraint }}\n{% endfor %}\n{% endif %}\n\nFind the optimal meeting time:\n1. Check all participants' availability\n2. Find overlapping free slots\n3. Apply any preferences (e.g., earliest time)\n4. Propose the time in format: \"Day HH:MM-HH:MM\"\n\nExplain your reasoning step by step.\"\"\"\n)\n</code></pre>"},{"location":"user_guide/custom_prompts/#algorithm-design","title":"Algorithm Design","text":"<pre><code>prompt_manager.update_prompt(\n    \"constraint_extraction\",\n    \"\"\"You are an expert in algorithm design and complexity analysis.\n\nProblem: {{ problem }}\n\nIdentify algorithmic constraints:\n1. TIME_COMPLEXITY: Required time complexity\n2. SPACE_COMPLEXITY: Required space complexity\n3. INPUT_FORMAT: Input data format and constraints\n4. OUTPUT_FORMAT: Required output format\n5. EDGE_CASES: Special cases to handle\n6. CONSTRAINTS: Any additional requirements\n\nBe specific about Big-O notation.\"\"\"\n)\n\nprompt_manager.update_prompt(\n    \"solution_generation\",\n    \"\"\"You are an expert algorithm designer.\n\nProblem: {{ problem }}\n\n{% if constraints %}\nRequirements:\n{% for constraint in constraints %}\n- {{ constraint }}\n{% endfor %}\n{% endif %}\n\nDesign an algorithm:\n1. Describe the approach\n2. Provide pseudocode\n3. Analyze time complexity\n4. Analyze space complexity\n5. Explain why it meets all requirements\n\nBe thorough and precise.\"\"\"\n)\n</code></pre>"},{"location":"user_guide/custom_prompts/#mathematical-problems","title":"Mathematical Problems","text":"<pre><code>prompt_manager.update_prompt(\n    \"verification\",\n    \"\"\"You are a mathematics professor with expertise in proof verification.\n\nProblem: {{ problem }}\nSolution: {{ solution }}\n\n{% if constraints %}\nRequirements:\n{% for constraint in constraints %}\n- {{ constraint }}\n{% endfor %}\n{% endif %}\n\nVerify the solution:\n1. Check mathematical correctness\n2. Verify all steps in the derivation\n3. Confirm the final answer\n4. Check for computational errors\n\nScore:\n- 1.0: Mathematically sound and correct\n- 0.8: Correct with minor notation issues\n- 0.6: Right approach but calculation errors\n- 0.4: Conceptual errors\n- 0.2: Fundamentally wrong\n\nProvide detailed mathematical feedback.\"\"\"\n)\n</code></pre>"},{"location":"user_guide/custom_prompts/#best-practices","title":"Best Practices","text":""},{"location":"user_guide/custom_prompts/#1-be-specific","title":"1. Be Specific","text":"<p>\u274c Bad: \"List the constraints\" \u2705 Good: \"List all time constraints, resource constraints, and participant availability\"</p>"},{"location":"user_guide/custom_prompts/#2-use-clear-formatting","title":"2. Use Clear Formatting","text":"<pre><code># Good: Clear structure\nProblem: {{ problem }}\n\nConstraints:\n{% for constraint in constraints %}\n{{ loop.index }}. {{ constraint }}\n{% endfor %}\n\nYour task: Generate a solution\n</code></pre>"},{"location":"user_guide/custom_prompts/#3-provide-examples","title":"3. Provide Examples","text":"<pre><code>Generate a solution in this format:\n\nExample:\nTime: Monday 10:00-10:30\nParticipants: Alice, Bob, Carol\nReasoning: This is the earliest available slot for all participants.\n\nYour solution:\n</code></pre>"},{"location":"user_guide/custom_prompts/#4-set-clear-expectations","title":"4. Set Clear Expectations","text":"<pre><code>Provide a score from 0.0 to 1.0 where:\n- 1.0 means perfect compliance with all constraints\n- 0.5 means partial compliance\n- 0.0 means no constraints are satisfied\n\nScore: [your score]\nExplanation: [detailed reasoning]\n</code></pre>"},{"location":"user_guide/custom_prompts/#5-include-context","title":"5. Include Context","text":"<pre><code>You are an expert {{ domain }} specialist with {{ years }} years of experience.\nYour expertise includes:\n- {{ skill_1 }}\n- {{ skill_2 }}\n- {{ skill_3 }}\n\nProblem: {{ problem }}\n</code></pre>"},{"location":"user_guide/custom_prompts/#testing-custom-prompts","title":"Testing Custom Prompts","text":"<p>Always test custom prompts with various inputs:</p> <pre><code>from plangen import PlanGen\n\n# Create PlanGen with custom prompts\nplangen = create_custom_plangen()\n\n# Test cases\ntest_problems = [\n    \"Simple scheduling problem...\",\n    \"Complex scheduling problem...\",\n    \"Edge case problem...\",\n]\n\nfor problem in test_problems:\n    print(f\"\\nTesting: {problem[:50]}...\")\n    result = plangen.solve(problem)\n    print(f\"Score: {result['selected_solution']['score']}\")\n    print(f\"Solution: {result['selected_solution']['solution'][:100]}...\")\n</code></pre>"},{"location":"user_guide/custom_prompts/#advanced-techniques","title":"Advanced Techniques","text":""},{"location":"user_guide/custom_prompts/#conditional-prompts","title":"Conditional Prompts","text":"<pre><code>{% if domain == \"scheduling\" %}\nYou are a calendar scheduling expert.\n{% elif domain == \"algorithms\" %}\nYou are a computer science algorithm expert.\n{% else %}\nYou are a general problem-solving expert.\n{% endif %}\n\nProblem: {{ problem }}\n</code></pre>"},{"location":"user_guide/custom_prompts/#chain-of-thought-prompting","title":"Chain-of-Thought Prompting","text":"<pre><code>Think through this problem step by step:\n\nProblem: {{ problem }}\n\n1. First, understand what is being asked\n2. Then, identify all constraints\n3. Next, consider possible approaches\n4. Finally, select and explain the best solution\n\nLet's begin with step 1...\n</code></pre>"},{"location":"user_guide/custom_prompts/#few-shot-examples","title":"Few-Shot Examples","text":"<pre><code>Here are examples of good solutions:\n\nExample 1:\nProblem: Schedule a 30-min meeting for 2 people...\nSolution: Monday 10:00-10:30 because...\n\nExample 2:\nProblem: Schedule a 1-hour meeting for 3 people...\nSolution: Tuesday 14:00-15:00 because...\n\nNow solve this problem:\nProblem: {{ problem }}\n</code></pre>"},{"location":"user_guide/custom_prompts/#prompt-engineering-tips","title":"Prompt Engineering Tips","text":"<ol> <li>Start simple: Begin with default prompts and refine</li> <li>Be explicit: State exactly what you want</li> <li>Use examples: Show the model what good output looks like</li> <li>Test iteratively: Make small changes and test</li> <li>Domain-specific: Customize for your problem domain</li> <li>Clear metrics: Define scoring criteria explicitly</li> <li>Error handling: Include instructions for edge cases</li> </ol>"},{"location":"user_guide/custom_prompts/#next-steps","title":"Next Steps","text":"<ul> <li>See Verification for custom verification strategies</li> <li>Explore Examples for complete examples</li> <li>Learn about Configuration for additional settings</li> </ul>"},{"location":"user_guide/installation/","title":"Installation Guide","text":"<p>This guide will help you install PlanGEN and set up your environment.</p>"},{"location":"user_guide/installation/#requirements","title":"Requirements","text":"<p>PlanGEN requires:</p> <ul> <li>Python 3.9 or later (up to 3.12)</li> <li>Dependencies listed in <code>pyproject.toml</code></li> </ul>"},{"location":"user_guide/installation/#installation-options","title":"Installation Options","text":""},{"location":"user_guide/installation/#using-poetry-recommended","title":"Using Poetry (Recommended)","text":"<p>Poetry is the recommended way to install PlanGEN, as it handles dependency management and virtual environments automatically.</p> <ol> <li>First, install Poetry if you don't have it already:</li> </ol> <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/yourusername/plangen.git\ncd plangen\n</code></pre> <ol> <li>Install dependencies and create a virtual environment:</li> </ol> <pre><code>poetry install\n</code></pre> <ol> <li>Activate the virtual environment:</li> </ol> <pre><code>poetry shell\n</code></pre>"},{"location":"user_guide/installation/#using-pip","title":"Using pip","text":"<p>Alternatively, you can install PlanGEN using pip:</p> <ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/yourusername/plangen.git\ncd plangen\n</code></pre> <ol> <li>Create a virtual environment:</li> </ol> <pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre> <ol> <li>Install the package in development mode:</li> </ol> <pre><code>pip install -e .\n</code></pre>"},{"location":"user_guide/installation/#setting-up-api-keys","title":"Setting Up API Keys","text":"<p>PlanGEN supports multiple LLM providers. Depending on which provider you plan to use, you'll need to set up the appropriate API keys:</p>"},{"location":"user_guide/installation/#openai","title":"OpenAI","text":"<p>Set your OpenAI API key as an environment variable:</p> <pre><code>export OPENAI_API_KEY=your-api-key-here\n</code></pre> <p>For persistent configuration, add it to your <code>.env</code> file:</p> <pre><code>OPENAI_API_KEY=your-api-key-here\n</code></pre>"},{"location":"user_guide/installation/#aws-bedrock","title":"AWS Bedrock","text":"<p>For AWS Bedrock, you need to set up your AWS credentials. The recommended way is to use the AWS CLI:</p> <pre><code>aws configure\n</code></pre> <p>Alternatively, you can set environment variables or use a <code>.env</code> file:</p> <pre><code>AWS_ACCESS_KEY_ID=your-access-key\nAWS_SECRET_ACCESS_KEY=your-secret-key\nAWS_REGION=us-east-1\n</code></pre>"},{"location":"user_guide/installation/#verifying-installation","title":"Verifying Installation","text":"<p>To verify that PlanGEN is installed correctly, run one of the example scripts:</p> <pre><code>python examples/simple_example.py\n</code></pre> <p>If everything is set up correctly, you should see output from PlanGEN solving a simple problem.</p>"},{"location":"user_guide/installation/#next-steps","title":"Next Steps","text":"<p>Now that you have PlanGEN installed, you can:</p> <ul> <li>Follow the Quick Start Guide to learn how to use PlanGEN</li> <li>Explore the Examples to see how to use PlanGEN for various use cases</li> <li>Read the API Reference for detailed information about PlanGEN's API</li> </ul>"},{"location":"user_guide/models/","title":"Models Guide","text":"<p>PlanGEN supports multiple language model backends through a unified interface. This guide explains how to use different models and configure them for your needs.</p>"},{"location":"user_guide/models/#supported-models","title":"Supported Models","text":"<p>PlanGEN currently supports:</p> <ul> <li>OpenAI Models: GPT-4, GPT-4 Turbo, GPT-3.5 Turbo</li> <li>AWS Bedrock Models: Claude 3 (Opus, Sonnet, Haiku), Amazon Titan</li> </ul>"},{"location":"user_guide/models/#openai-models","title":"OpenAI Models","text":""},{"location":"user_guide/models/#setup","title":"Setup","text":"<p>Install the required dependency (already included in PlanGEN):</p> <pre><code>pip install openai\n</code></pre> <p>Set your API key:</p> <pre><code>export OPENAI_API_KEY=\"your-api-key-here\"\n</code></pre> <p>Or in a <code>.env</code> file:</p> <pre><code>OPENAI_API_KEY=your-api-key-here\n</code></pre>"},{"location":"user_guide/models/#using-openai-models","title":"Using OpenAI Models","text":"<pre><code>from plangen import PlanGen\n\n# Use default model (gpt-4o)\nplangen = PlanGen.with_openai()\n\n# Use specific model\nplangen = PlanGen.with_openai(\n    model_name=\"gpt-4-turbo\",\n    temperature=0.7,\n    max_tokens=2048\n)\n\n# Use GPT-3.5 for faster, cheaper responses\nplangen = PlanGen.with_openai(\n    model_name=\"gpt-3.5-turbo\",\n    temperature=0.5\n)\n</code></pre>"},{"location":"user_guide/models/#available-openai-models","title":"Available OpenAI Models","text":"Model Description Best For <code>gpt-4o</code> Latest GPT-4 optimized model Best overall performance <code>gpt-4-turbo</code> Fast GPT-4 model Complex reasoning tasks <code>gpt-4</code> Original GPT-4 Highest quality, slower <code>gpt-3.5-turbo</code> Fast and cost-effective Simple tasks, prototyping"},{"location":"user_guide/models/#openai-model-parameters","title":"OpenAI Model Parameters","text":"<pre><code>plangen = PlanGen.with_openai(\n    model_name=\"gpt-4o\",\n    temperature=0.7,          # Randomness (0.0-2.0)\n    max_tokens=2048,          # Maximum response length\n    top_p=1.0,                # Nucleus sampling (0.0-1.0)\n    frequency_penalty=0.0,    # Reduce repetition (-2.0-2.0)\n    presence_penalty=0.0,     # Encourage new topics (-2.0-2.0)\n)\n</code></pre>"},{"location":"user_guide/models/#aws-bedrock-models","title":"AWS Bedrock Models","text":""},{"location":"user_guide/models/#setup_1","title":"Setup","text":"<p>Install AWS SDK (already included in PlanGEN):</p> <pre><code>pip install boto3\n</code></pre> <p>Configure AWS credentials:</p> <pre><code>aws configure\n</code></pre> <p>Or set environment variables:</p> <pre><code>export AWS_ACCESS_KEY_ID=\"your-access-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\nexport AWS_DEFAULT_REGION=\"us-east-1\"\n</code></pre>"},{"location":"user_guide/models/#using-bedrock-models","title":"Using Bedrock Models","text":"<pre><code>from plangen import PlanGen\n\n# Use default model (Claude 3 Sonnet)\nplangen = PlanGen.with_bedrock()\n\n# Use specific model\nplangen = PlanGen.with_bedrock(\n    model_id=\"anthropic.claude-3-opus-20240229-v1:0\",\n    region=\"us-east-1\",\n    temperature=0.7,\n    max_tokens=2048\n)\n\n# Use Claude 3 Haiku for faster responses\nplangen = PlanGen.with_bedrock(\n    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n    region=\"us-west-2\"\n)\n</code></pre>"},{"location":"user_guide/models/#available-bedrock-models","title":"Available Bedrock Models","text":"Model ID Description Best For <code>anthropic.claude-3-opus-20240229-v1:0</code> Highest capability Most complex tasks <code>anthropic.claude-3-sonnet-20240229-v1:0</code> Balanced performance General use (default) <code>anthropic.claude-3-haiku-20240307-v1:0</code> Fast and efficient Simple tasks, high throughput <code>amazon.titan-text-express-v1</code> AWS native model AWS-specific workloads"},{"location":"user_guide/models/#bedrock-model-parameters","title":"Bedrock Model Parameters","text":"<pre><code>plangen = PlanGen.with_bedrock(\n    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n    region=\"us-east-1\",\n    temperature=0.7,          # Randomness (0.0-1.0)\n    max_tokens=2048,          # Maximum response length\n    top_p=0.9,                # Nucleus sampling (0.0-1.0)\n    top_k=250,                # Top-K sampling\n)\n</code></pre>"},{"location":"user_guide/models/#custom-model-interface","title":"Custom Model Interface","text":"<p>You can implement your own model interface for other providers:</p> <pre><code>from typing import List, Dict, Any, Optional\nfrom plangen.models.base import ModelProtocol\n\nclass CustomModelInterface(ModelProtocol):\n    \"\"\"Custom model interface implementation.\"\"\"\n\n    def __init__(self, model_name: str, **kwargs):\n        self.model_name = model_name\n        self.kwargs = kwargs\n\n    def generate(\n        self,\n        system_message: str,\n        user_message: str,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&gt; str:\n        \"\"\"Generate a response from the model.\"\"\"\n        # Implement your model API call here\n        response = your_model_api.generate(\n            system=system_message,\n            user=user_message,\n            temperature=temperature or 0.7,\n            max_tokens=max_tokens or 1024,\n        )\n        return response.text\n\n    def batch_generate(\n        self,\n        prompts: List[Dict[str, str]],\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n    ) -&gt; List[str]:\n        \"\"\"Generate responses for multiple prompts.\"\"\"\n        return [\n            self.generate(\n                prompt[\"system\"],\n                prompt[\"user\"],\n                temperature,\n                max_tokens,\n            )\n            for prompt in prompts\n        ]\n\n# Use your custom model\nfrom plangen import PlanGen\n\nmodel = CustomModelInterface(model_name=\"my-model\")\nplangen = PlanGen.with_model(model)\n</code></pre>"},{"location":"user_guide/models/#model-selection-guidelines","title":"Model Selection Guidelines","text":""},{"location":"user_guide/models/#by-use-case","title":"By Use Case","text":"Use Case Recommended Model Reason Production applications GPT-4o, Claude 3 Sonnet Best balance of quality and speed Complex reasoning GPT-4 Turbo, Claude 3 Opus Highest capability High-throughput systems GPT-3.5 Turbo, Claude 3 Haiku Fast and cost-effective Prototyping GPT-3.5 Turbo Quick iteration, low cost AWS-native applications Claude 3 on Bedrock Better AWS integration"},{"location":"user_guide/models/#by-cost","title":"By Cost","text":"<p>From least to most expensive (per token):</p> <ol> <li>GPT-3.5 Turbo</li> <li>Claude 3 Haiku</li> <li>Claude 3 Sonnet</li> <li>GPT-4o</li> <li>GPT-4 Turbo</li> <li>Claude 3 Opus</li> <li>GPT-4</li> </ol>"},{"location":"user_guide/models/#by-performance","title":"By Performance","text":"<p>From fastest to slowest:</p> <ol> <li>GPT-3.5 Turbo</li> <li>Claude 3 Haiku</li> <li>GPT-4o</li> <li>Claude 3 Sonnet</li> <li>GPT-4 Turbo</li> <li>Claude 3 Opus</li> <li>GPT-4</li> </ol>"},{"location":"user_guide/models/#model-comparison","title":"Model Comparison","text":""},{"location":"user_guide/models/#openai-vs-aws-bedrock","title":"OpenAI vs. AWS Bedrock","text":"Feature OpenAI AWS Bedrock Setup complexity Simple Moderate (requires AWS) Pricing model Pay-per-token Pay-per-token Model variety GPT models only Multiple providers Integration Direct API Through AWS SDK Enterprise features Limited Advanced (VPC, IAM)"},{"location":"user_guide/models/#best-practices","title":"Best Practices","text":""},{"location":"user_guide/models/#1-start-with-balanced-models","title":"1. Start with Balanced Models","text":"<p>Begin with GPT-4o or Claude 3 Sonnet for general use:</p> <pre><code># Good default choice\nplangen = PlanGen.with_openai(model_name=\"gpt-4o\")\n</code></pre>"},{"location":"user_guide/models/#2-use-cheaper-models-for-simple-tasks","title":"2. Use Cheaper Models for Simple Tasks","text":"<p>For constraint extraction or simple verification:</p> <pre><code># Use cheaper model for simple operations\ncheap_plangen = PlanGen.with_openai(model_name=\"gpt-3.5-turbo\")\nconstraints = cheap_plangen.extract_constraints(problem)\n\n# Use powerful model for complex reasoning\npowerful_plangen = PlanGen.with_openai(model_name=\"gpt-4o\")\nresult = powerful_plangen.solve(problem, constraints=constraints)\n</code></pre>"},{"location":"user_guide/models/#3-adjust-temperature-by-task","title":"3. Adjust Temperature by Task","text":"<ul> <li>Low temperature (0.1-0.3): Verification, consistency</li> <li>Medium temperature (0.5-0.7): General planning</li> <li>High temperature (0.8-1.0): Creative solutions</li> </ul> <pre><code># Deterministic verification\nverifier = PlanGen.create(temperature=0.2)\n\n# Creative solution generation\ngenerator = PlanGen.create(temperature=0.8)\n</code></pre>"},{"location":"user_guide/models/#4-monitor-costs","title":"4. Monitor Costs","text":"<p>Track API usage and costs:</p> <pre><code>import logging\n\n# Enable logging to monitor API calls\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger('plangen.models')\n\n# Log will show API call details\n</code></pre>"},{"location":"user_guide/models/#5-implement-fallbacks","title":"5. Implement Fallbacks","text":"<p>Use multiple models for reliability:</p> <pre><code>def solve_with_fallback(problem):\n    try:\n        # Try primary model\n        plangen = PlanGen.with_openai(model_name=\"gpt-4o\")\n        return plangen.solve(problem)\n    except Exception as e:\n        # Fallback to alternative model\n        print(f\"Primary model failed: {e}, trying fallback...\")\n        plangen = PlanGen.with_bedrock()\n        return plangen.solve(problem)\n</code></pre>"},{"location":"user_guide/models/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user_guide/models/#api-key-issues","title":"API Key Issues","text":"<pre><code># Check if API key is set\nimport os\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    raise ValueError(\"OPENAI_API_KEY not set\")\n</code></pre>"},{"location":"user_guide/models/#rate-limiting","title":"Rate Limiting","text":"<pre><code>from tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=4, max=10)\n)\ndef solve_with_retry(problem):\n    plangen = PlanGen.create()\n    return plangen.solve(problem)\n</code></pre>"},{"location":"user_guide/models/#timeout-issues","title":"Timeout Issues","text":"<pre><code># Set shorter max_tokens for faster responses\nplangen = PlanGen.create(max_tokens=512)\n</code></pre>"},{"location":"user_guide/models/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Configuration for advanced settings</li> <li>Explore Custom Prompts to optimize model behavior</li> <li>See Examples for practical use cases</li> </ul>"},{"location":"user_guide/quickstart/","title":"Quick Start Guide","text":"<p>This guide will help you get started with PlanGEN quickly.</p>"},{"location":"user_guide/quickstart/#simple-example","title":"Simple Example","text":"<p>Here's a minimal example using the new public API:</p> <pre><code>from dotenv import load_dotenv\nfrom plangen import PlanGen\n\n# Load environment variables from .env file (containing API keys)\nload_dotenv()\n\n# Create a PlanGen instance (automatically uses OpenAI's gpt-4o by default)\nplangen = PlanGen.create()\n\n# Define a problem\nproblem = \"\"\"\nSchedule a 30-minute meeting for Alexander, Elizabeth, and Walter on Monday between 9:00 and 17:00.\nAlexander: Busy at 9:30-10:00, 10:30-11:00, 12:30-13:00, 14:30-15:00, 16:00-17:00.\nElizabeth: Busy at 9:00-9:30, 11:30-12:30, 13:00-14:30.\nWalter: Busy at 9:00-14:30, 15:30-17:00.\nFind an earliest time slot that works for all participants.\n\"\"\"\n\n# Solve the problem\nresult = plangen.solve(problem)\n\n# Print the selected solution\nprint(result[\"selected_solution\"])\n</code></pre>"},{"location":"user_guide/quickstart/#using-a-specific-model","title":"Using a Specific Model","text":"<p>You can specify which model to use when creating a PlanGen instance:</p>"},{"location":"user_guide/quickstart/#openai","title":"OpenAI","text":"<pre><code>from plangen import PlanGen\n\n# Create a PlanGen instance with a specific OpenAI model\nplangen = PlanGen.with_openai(\n    model_name=\"gpt-4-turbo\",\n    temperature=0.7,\n    max_tokens=1024\n)\n\n# Solve a problem\nproblem = \"Your problem statement here\"\nresult = plangen.solve(problem)\n</code></pre>"},{"location":"user_guide/quickstart/#aws-bedrock","title":"AWS Bedrock","text":"<pre><code>from plangen import PlanGen\n\n# Create a PlanGen instance with a specific AWS Bedrock model\nplangen = PlanGen.with_bedrock(\n    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n    region=\"us-east-1\",\n    temperature=0.7,\n    max_tokens=1024\n)\n\n# Solve a problem\nproblem = \"Your problem statement here\"\nresult = plangen.solve(problem)\n</code></pre>"},{"location":"user_guide/quickstart/#using-a-specific-algorithm","title":"Using a Specific Algorithm","text":"<p>You can specify which planning algorithm to use when solving a problem:</p> <pre><code>from plangen import PlanGen\n\n# Create a PlanGen instance\nplangen = PlanGen.create()\n\n# Solve a problem using the BestOfN algorithm\nresult = plangen.solve(\n    problem=\"Your problem statement here\",\n    algorithm=\"best_of_n\",\n    n_plans=5,\n    sampling_strategy=\"diverse\",\n    parallel=True\n)\n</code></pre> <p>Available algorithms are:</p> <ul> <li><code>\"best_of_n\"</code> - Generates multiple plans and selects the best one</li> <li><code>\"tree_of_thought\"</code> - Explores multiple reasoning paths in a tree structure</li> <li><code>\"rebase\"</code> - Uses recursive refinement to improve plans</li> <li><code>\"mixture\"</code> - Dynamically selects the best algorithm for the problem</li> </ul>"},{"location":"user_guide/quickstart/#using-a-custom-verifier","title":"Using a Custom Verifier","text":"<p>For specialized verification of solutions, you can provide a custom verifier:</p> <pre><code>from plangen import PlanGen, Verifiers\n\n# Create a PlanGen instance\nplangen = PlanGen.create()\n\n# Create a domain-specific verifier\nverifier = Verifiers.calendar()\n\n# Solve a problem with the custom verifier\nresult = plangen.solve(\n    problem=\"Your calendar scheduling problem here\",\n    verifier=verifier\n)\n</code></pre>"},{"location":"user_guide/quickstart/#manual-process-control","title":"Manual Process Control","text":"<p>If you want more control over the individual steps of the planning process:</p> <pre><code>from plangen import PlanGen\n\n# Create a PlanGen instance\nplangen = PlanGen.create()\n\n# Extract constraints from a problem\nproblem = \"Your problem statement here\"\nconstraints = plangen.extract_constraints(problem)\nprint(f\"Extracted constraints: {constraints}\")\n\n# Generate a single plan\nplan = plangen.generate_plan(problem, constraints)\nprint(f\"Generated plan: {plan}\")\n\n# Verify the plan\nfeedback, score = plangen.verify_plan(problem, plan, constraints)\nprint(f\"Verification score: {score}\")\nprint(f\"Feedback: {feedback}\")\n</code></pre>"},{"location":"user_guide/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Check out the Examples for more advanced use cases</li> <li>Learn about Custom Prompts to customize the behavior of PlanGEN</li> <li>Explore the Algorithm Reference to understand the different planning algorithms</li> </ul>"},{"location":"user_guide/verification/","title":"Verification Guide","text":"<p>Verification is a critical component of PlanGEN that evaluates whether generated solutions satisfy the problem constraints. This guide explains how to use and customize verification.</p>"},{"location":"user_guide/verification/#overview","title":"Overview","text":"<p>PlanGEN's verification system:</p> <ol> <li>Checks if solutions meet all constraints</li> <li>Provides detailed feedback on issues</li> <li>Assigns numerical scores (0.0 to 1.0)</li> <li>Enables iterative refinement</li> </ol>"},{"location":"user_guide/verification/#built-in-verifiers","title":"Built-in Verifiers","text":""},{"location":"user_guide/verification/#base-verifier","title":"Base Verifier","text":"<p>The default verifier uses LLM-based verification:</p> <pre><code>from plangen import PlanGen\n\nplangen = PlanGen.create()\n\n# Verify a plan\nfeedback, score = plangen.verify_plan(\n    problem=\"Schedule a meeting...\",\n    plan=\"Meeting at 10:00-10:30\",\n    constraints=[\"Must be 30 minutes\", \"Must be on Monday\"]\n)\n\nprint(f\"Score: {score}\")\nprint(f\"Feedback: {feedback}\")\n</code></pre>"},{"location":"user_guide/verification/#domain-specific-verifiers","title":"Domain-Specific Verifiers","text":"<p>PlanGEN includes specialized verifiers for common domains:</p> <pre><code>from plangen import Verifiers\n\n# Calendar scheduling verifier\ncalendar_verifier = Verifiers.calendar()\n\n# Math problem verifier\nmath_verifier = Verifiers.math()\n\n# Algorithm design verifier\nalgorithm_verifier = Verifiers.algorithm()\n</code></pre>"},{"location":"user_guide/verification/#using-custom-verifiers","title":"Using Custom Verifiers","text":""},{"location":"user_guide/verification/#with-solve-method","title":"With solve() Method","text":"<pre><code>from plangen import PlanGen, Verifiers\n\nplangen = PlanGen.create()\nverifier = Verifiers.calendar()\n\nresult = plangen.solve(\n    problem=\"Schedule a meeting for 3 people...\",\n    verifier=verifier\n)\n</code></pre>"},{"location":"user_guide/verification/#with-verify_plan-method","title":"With verify_plan() Method","text":"<pre><code>from plangen import PlanGen, Verifiers\n\nplangen = PlanGen.create()\nverifier = Verifiers.calendar()\n\nfeedback, score = plangen.verify_plan(\n    problem=\"Schedule a meeting...\",\n    plan=\"Monday 10:00-10:30\",\n    verifier=verifier\n)\n</code></pre>"},{"location":"user_guide/verification/#creating-custom-verifiers","title":"Creating Custom Verifiers","text":""},{"location":"user_guide/verification/#basic-custom-verifier","title":"Basic Custom Verifier","text":"<pre><code>from typing import List, Tuple\nfrom plangen.verification.base_verifier import BaseVerifier\n\nclass CustomVerifier(BaseVerifier):\n    \"\"\"Custom verifier for specific problem domain.\"\"\"\n\n    def verify_solution(\n        self,\n        problem: str,\n        solution: str,\n        constraints: List[str]\n    ) -&gt; Tuple[str, float]:\n        \"\"\"\n        Verify a solution.\n\n        Args:\n            problem: The problem statement\n            solution: The proposed solution\n            constraints: List of constraints to check\n\n        Returns:\n            Tuple of (feedback, score)\n        \"\"\"\n        feedback = []\n        total_score = 0.0\n\n        # Check each constraint\n        for i, constraint in enumerate(constraints):\n            satisfied = self._check_constraint(solution, constraint)\n            if satisfied:\n                feedback.append(f\"\u2713 Constraint {i+1} satisfied\")\n                total_score += 1.0\n            else:\n                feedback.append(f\"\u2717 Constraint {i+1} not satisfied: {constraint}\")\n\n        # Calculate final score\n        score = total_score / len(constraints) if constraints else 0.0\n\n        return \"\\n\".join(feedback), score\n\n    def _check_constraint(self, solution: str, constraint: str) -&gt; bool:\n        \"\"\"Check if a specific constraint is satisfied.\"\"\"\n        # Implement constraint checking logic\n        return constraint.lower() in solution.lower()\n\n    def is_applicable(self, problem: str) -&gt; bool:\n        \"\"\"Check if this verifier applies to the problem.\"\"\"\n        return \"schedule\" in problem.lower()\n</code></pre>"},{"location":"user_guide/verification/#using-the-custom-verifier","title":"Using the Custom Verifier","text":"<pre><code>from plangen import PlanGen\n\n# Create custom verifier\nverifier = CustomVerifier()\n\n# Use with PlanGen\nplangen = PlanGen.create()\nresult = plangen.solve(\n    problem=\"Your problem here\",\n    verifier=verifier\n)\n</code></pre>"},{"location":"user_guide/verification/#verification-strategies","title":"Verification Strategies","text":""},{"location":"user_guide/verification/#1-rule-based-verification","title":"1. Rule-Based Verification","text":"<p>Check specific rules programmatically:</p> <pre><code>class RuleBasedVerifier(BaseVerifier):\n    \"\"\"Verifier that checks specific rules.\"\"\"\n\n    def verify_solution(self, problem, solution, constraints):\n        feedback = []\n        score = 1.0\n\n        # Check time format\n        if not self._has_valid_time_format(solution):\n            feedback.append(\"Invalid time format\")\n            score -= 0.3\n\n        # Check duration\n        if not self._has_correct_duration(solution, constraints):\n            feedback.append(\"Incorrect meeting duration\")\n            score -= 0.4\n\n        # Check participants\n        if not self._has_all_participants(solution, constraints):\n            feedback.append(\"Missing participants\")\n            score -= 0.3\n\n        score = max(0.0, score)\n        return \"\\n\".join(feedback) if feedback else \"All checks passed\", score\n\n    def _has_valid_time_format(self, solution):\n        import re\n        time_pattern = r'\\d{1,2}:\\d{2}'\n        return bool(re.search(time_pattern, solution))\n\n    def _has_correct_duration(self, solution, constraints):\n        # Implement duration checking logic\n        return True\n\n    def _has_all_participants(self, solution, constraints):\n        # Implement participant checking logic\n        return True\n</code></pre>"},{"location":"user_guide/verification/#2-llm-enhanced-verification","title":"2. LLM-Enhanced Verification","text":"<p>Combine rules with LLM verification:</p> <pre><code>class HybridVerifier(BaseVerifier):\n    \"\"\"Verifier combining rules and LLM.\"\"\"\n\n    def __init__(self, llm_interface):\n        self.llm = llm_interface\n\n    def verify_solution(self, problem, solution, constraints):\n        # First, check basic rules\n        rule_feedback, rule_score = self._check_rules(solution, constraints)\n\n        # If rules pass, use LLM for deeper verification\n        if rule_score &gt;= 0.7:\n            llm_feedback, llm_score = self._llm_verify(\n                problem, solution, constraints\n            )\n            final_score = (rule_score + llm_score) / 2\n            feedback = f\"Rules: {rule_feedback}\\nLLM: {llm_feedback}\"\n            return feedback, final_score\n\n        return rule_feedback, rule_score\n\n    def _check_rules(self, solution, constraints):\n        # Implement rule checking\n        return \"Rules passed\", 0.9\n\n    def _llm_verify(self, problem, solution, constraints):\n        # Use LLM for verification\n        prompt = f\"\"\"Verify this solution:\n        Problem: {problem}\n        Solution: {solution}\n        Constraints: {constraints}\n\n        Provide score (0.0-1.0) and feedback.\"\"\"\n\n        response = self.llm.generate(\"You are a verifier\", prompt)\n        # Parse response to extract score and feedback\n        return response, 0.85\n</code></pre>"},{"location":"user_guide/verification/#3-multi-criteria-verification","title":"3. Multi-Criteria Verification","text":"<p>Evaluate multiple aspects separately:</p> <pre><code>class MultiCriteriaVerifier(BaseVerifier):\n    \"\"\"Verifier that checks multiple criteria.\"\"\"\n\n    def verify_solution(self, problem, solution, constraints):\n        criteria_scores = {}\n\n        # Correctness\n        criteria_scores['correctness'] = self._check_correctness(\n            solution, constraints\n        )\n\n        # Completeness\n        criteria_scores['completeness'] = self._check_completeness(\n            solution, constraints\n        )\n\n        # Clarity\n        criteria_scores['clarity'] = self._check_clarity(solution)\n\n        # Efficiency\n        criteria_scores['efficiency'] = self._check_efficiency(solution)\n\n        # Calculate weighted score\n        weights = {\n            'correctness': 0.4,\n            'completeness': 0.3,\n            'clarity': 0.2,\n            'efficiency': 0.1,\n        }\n\n        final_score = sum(\n            criteria_scores[k] * weights[k]\n            for k in weights\n        )\n\n        feedback = self._format_feedback(criteria_scores)\n        return feedback, final_score\n\n    def _check_correctness(self, solution, constraints):\n        # Check if solution is correct\n        return 0.9\n\n    def _check_completeness(self, solution, constraints):\n        # Check if solution is complete\n        return 0.85\n\n    def _check_clarity(self, solution):\n        # Check if solution is clear\n        return 0.8\n\n    def _check_efficiency(self, solution):\n        # Check if solution is efficient\n        return 0.75\n\n    def _format_feedback(self, scores):\n        return \"\\n\".join(f\"{k}: {v:.2f}\" for k, v in scores.items())\n</code></pre>"},{"location":"user_guide/verification/#verification-best-practices","title":"Verification Best Practices","text":""},{"location":"user_guide/verification/#1-separate-concerns","title":"1. Separate Concerns","text":"<pre><code># Good: Separate verification logic\nclass SchedulingVerifier(BaseVerifier):\n    def verify_solution(self, problem, solution, constraints):\n        time_valid = self._verify_time(solution)\n        participants_valid = self._verify_participants(solution, constraints)\n        duration_valid = self._verify_duration(solution, constraints)\n\n        # Combine results\n        ...\n</code></pre>"},{"location":"user_guide/verification/#2-provide-actionable-feedback","title":"2. Provide Actionable Feedback","text":"<pre><code># Bad: Vague feedback\nfeedback = \"Solution has issues\"\n\n# Good: Specific feedback\nfeedback = \"\"\"Issues found:\n1. Time 9:30-10:00 conflicts with Alice's availability\n2. Meeting duration is 30 min but requirement is 60 min\n3. Location not specified\"\"\"\n</code></pre>"},{"location":"user_guide/verification/#3-use-appropriate-scoring","title":"3. Use Appropriate Scoring","text":"<pre><code># Define clear scoring rubric\ndef calculate_score(violations):\n    base_score = 1.0\n    critical_violations = [v for v in violations if v.critical]\n    minor_violations = [v for v in violations if not v.critical]\n\n    # Critical issues: -0.3 each\n    base_score -= len(critical_violations) * 0.3\n\n    # Minor issues: -0.1 each\n    base_score -= len(minor_violations) * 0.1\n\n    return max(0.0, base_score)\n</code></pre>"},{"location":"user_guide/verification/#4-handle-edge-cases","title":"4. Handle Edge Cases","text":"<pre><code>def verify_solution(self, problem, solution, constraints):\n    # Handle empty solution\n    if not solution or not solution.strip():\n        return \"Solution is empty\", 0.0\n\n    # Handle missing constraints\n    if not constraints:\n        return \"No constraints to verify against\", 0.5\n\n    # Normal verification\n    ...\n</code></pre>"},{"location":"user_guide/verification/#5-make-verifiers-reusable","title":"5. Make Verifiers Reusable","text":"<pre><code># Good: Configurable verifier\nclass ConfigurableVerifier(BaseVerifier):\n    def __init__(self, strict_mode=False, min_score=0.6):\n        self.strict_mode = strict_mode\n        self.min_score = min_score\n\n    def verify_solution(self, problem, solution, constraints):\n        feedback, score = self._verify(problem, solution, constraints)\n\n        if self.strict_mode and score &lt; self.min_score:\n            feedback += f\"\\nScore {score} below minimum {self.min_score}\"\n            score = 0.0\n\n        return feedback, score\n</code></pre>"},{"location":"user_guide/verification/#verification-examples","title":"Verification Examples","text":""},{"location":"user_guide/verification/#calendar-scheduling","title":"Calendar Scheduling","text":"<pre><code>class CalendarVerifier(BaseVerifier):\n    def verify_solution(self, problem, solution, constraints):\n        from datetime import datetime, timedelta\n\n        feedback = []\n        score = 1.0\n\n        # Extract time from solution\n        time_match = self._extract_time(solution)\n        if not time_match:\n            return \"No valid time found in solution\", 0.0\n\n        start_time = time_match['start']\n        end_time = time_match['end']\n\n        # Check duration\n        duration = (end_time - start_time).total_seconds() / 60\n        required_duration = self._extract_duration(constraints)\n        if abs(duration - required_duration) &gt; 5:\n            feedback.append(\n                f\"Duration {duration} min differs from required {required_duration} min\"\n            )\n            score -= 0.3\n\n        # Check availability\n        busy_times = self._extract_busy_times(constraints)\n        if self._has_conflicts(start_time, end_time, busy_times):\n            feedback.append(\"Time slot conflicts with busy times\")\n            score -= 0.5\n\n        return \"\\n\".join(feedback) if feedback else \"Valid\", max(0.0, score)\n</code></pre>"},{"location":"user_guide/verification/#mathematical-proofs","title":"Mathematical Proofs","text":"<pre><code>class MathVerifier(BaseVerifier):\n    def verify_solution(self, problem, solution, constraints):\n        feedback = []\n        score = 1.0\n\n        # Check if solution includes calculation steps\n        if not self._has_calculation_steps(solution):\n            feedback.append(\"Missing calculation steps\")\n            score -= 0.2\n\n        # Check final answer format\n        if not self._has_final_answer(solution):\n            feedback.append(\"Final answer not clearly stated\")\n            score -= 0.3\n\n        # Verify calculations (if possible)\n        if self._can_verify_calculations(solution):\n            if not self._calculations_correct(solution):\n                feedback.append(\"Calculation errors detected\")\n                score -= 0.5\n\n        return \"\\n\".join(feedback) if feedback else \"Correct\", max(0.0, score)\n</code></pre>"},{"location":"user_guide/verification/#integration-with-algorithms","title":"Integration with Algorithms","text":"<p>Verification works seamlessly with all PlanGEN algorithms:</p> <pre><code>from plangen import PlanGen\n\nverifier = CustomVerifier()\n\n# With BestOfN\nresult = plangen.solve(\n    problem,\n    algorithm=\"best_of_n\",\n    n_plans=5,\n    verifier=verifier\n)\n\n# With TreeOfThought\nresult = plangen.solve(\n    problem,\n    algorithm=\"tree_of_thought\",\n    verifier=verifier\n)\n\n# With REBASE\nresult = plangen.solve(\n    problem,\n    algorithm=\"rebase\",\n    verifier=verifier\n)\n</code></pre>"},{"location":"user_guide/verification/#next-steps","title":"Next Steps","text":"<ul> <li>See Examples for complete verification examples</li> <li>Learn about Visualization to see verification results</li> <li>Explore Configuration for verification settings</li> </ul>"},{"location":"user_guide/visualization/","title":"Visualization Guide","text":"<p>PlanGEN provides visualization capabilities to help you understand and debug the planning process. This guide explains how to use the visualization tools.</p>"},{"location":"user_guide/visualization/#overview","title":"Overview","text":"<p>Visualization in PlanGEN:</p> <ul> <li>Shows algorithm execution flow</li> <li>Displays score progression</li> <li>Illustrates reasoning paths</li> <li>Helps debug issues</li> <li>Enables analysis</li> </ul>"},{"location":"user_guide/visualization/#graphrenderer","title":"GraphRenderer","text":"<p>The <code>GraphRenderer</code> class is the main visualization tool:</p> <pre><code>from plangen.visualization import GraphRenderer\nfrom plangen.algorithms import TreeOfThought\nfrom plangen.models import OpenAIModelInterface\n\n# Create model\nmodel = OpenAIModelInterface(model_name=\"gpt-4o\")\n\n# Create renderer\nrenderer = GraphRenderer(output_dir=\"./visualizations\")\n\n# Create algorithm with visualization\nalgorithm = TreeOfThought(\n    branching_factor=3,\n    max_depth=5,\n    llm_interface=model\n)\n\n# Add observer\nalgorithm.add_observer(renderer)\n\n# Run algorithm (visualizations will be generated)\nproblem = \"Your problem here\"\nsolution, score, metadata = algorithm.run(problem)\n\n# Check ./visualizations directory for output\n</code></pre>"},{"location":"user_guide/visualization/#visualization-types","title":"Visualization Types","text":""},{"location":"user_guide/visualization/#tree-of-thought-visualization","title":"Tree of Thought Visualization","text":"<p>Shows the exploration tree with nodes and branches:</p> <pre><code>from plangen.visualization import GraphRenderer\nfrom plangen.algorithms import TreeOfThought\n\nrenderer = GraphRenderer(output_dir=\"./tot_viz\")\ntot = TreeOfThought(branching_factor=3, max_depth=5, llm_interface=model)\ntot.add_observer(renderer)\n\nsolution, score, metadata = tot.run(problem)\n# Generates: tree_structure.png, path_scores.png\n</code></pre>"},{"location":"user_guide/visualization/#bestofn-visualization","title":"BestOfN Visualization","text":"<p>Shows comparison of generated plans:</p> <pre><code>from plangen.visualization import GraphRenderer\nfrom plangen.algorithms import BestOfN\n\nrenderer = GraphRenderer(output_dir=\"./bon_viz\")\nbon = BestOfN(n_plans=5, llm_interface=model)\nbon.add_observer(renderer)\n\nsolution, score, metadata = bon.run(problem)\n# Generates: plan_comparison.png, score_distribution.png\n</code></pre>"},{"location":"user_guide/visualization/#rebase-visualization","title":"REBASE Visualization","text":"<p>Shows iterative refinement progress:</p> <pre><code>from plangen.visualization import GraphRenderer\nfrom plangen.algorithms import REBASE\n\nrenderer = GraphRenderer(output_dir=\"./rebase_viz\")\nrebase = REBASE(max_iterations=5, llm_interface=model)\nrebase.add_observer(renderer)\n\nsolution, score, metadata = rebase.run(problem)\n# Generates: iteration_scores.png, convergence_plot.png\n</code></pre>"},{"location":"user_guide/visualization/#configuration","title":"Configuration","text":""},{"location":"user_guide/visualization/#output-directory","title":"Output Directory","text":"<pre><code># Specify where visualizations are saved\nrenderer = GraphRenderer(output_dir=\"./my_visualizations\")\n</code></pre>"},{"location":"user_guide/visualization/#format-options","title":"Format Options","text":"<pre><code># Configure visualization format\nrenderer = GraphRenderer(\n    output_dir=\"./viz\",\n    format=\"png\",  # or \"svg\", \"pdf\"\n    dpi=300,       # resolution\n    figsize=(12, 8)  # figure size\n)\n</code></pre>"},{"location":"user_guide/visualization/#using-with-plangen-api","title":"Using with PlanGen API","text":"<pre><code>from plangen import PlanGen\nfrom plangen.visualization import GraphRenderer\n\n# Create PlanGen\nplangen = PlanGen.create()\n\n# For direct algorithm use, you need to access the internal algorithm\n# This is more advanced usage - typically used for debugging\n</code></pre>"},{"location":"user_guide/visualization/#best-practices","title":"Best Practices","text":""},{"location":"user_guide/visualization/#1-use-descriptive-output-directories","title":"1. Use Descriptive Output Directories","text":"<pre><code># Good: Descriptive names\nrenderer = GraphRenderer(output_dir=\"./problem_type_visualization\")\n\n# Bad: Generic names\nrenderer = GraphRenderer(output_dir=\"./viz\")\n</code></pre>"},{"location":"user_guide/visualization/#2-clear-old-visualizations","title":"2. Clear Old Visualizations","text":"<pre><code>import shutil\nimport os\n\n# Clear previous runs\nif os.path.exists(\"./visualizations\"):\n    shutil.rmtree(\"./visualizations\")\n\nrenderer = GraphRenderer(output_dir=\"./visualizations\")\n</code></pre>"},{"location":"user_guide/visualization/#3-save-metadata","title":"3. Save Metadata","text":"<pre><code>import json\n\nsolution, score, metadata = algorithm.run(problem)\n\n# Save metadata with visualizations\nwith open(\"./visualizations/metadata.json\", \"w\") as f:\n    json.dump(metadata, f, indent=2)\n</code></pre>"},{"location":"user_guide/visualization/#example-output","title":"Example Output","text":""},{"location":"user_guide/visualization/#tree-structure","title":"Tree Structure","text":"<p>The tree structure visualization shows:</p> <ul> <li>Nodes representing reasoning steps</li> <li>Edges showing branching decisions</li> <li>Colors indicating scores</li> <li>Pruned branches marked differently</li> </ul>"},{"location":"user_guide/visualization/#score-progression","title":"Score Progression","text":"<p>Score progression plots show:</p> <ul> <li>X-axis: Iteration or step number</li> <li>Y-axis: Score (0.0 to 1.0)</li> <li>Line showing improvement over time</li> <li>Markers for significant events</li> </ul>"},{"location":"user_guide/visualization/#comparison-charts","title":"Comparison Charts","text":"<p>For BestOfN, comparison charts show:</p> <ul> <li>Bar chart of all plan scores</li> <li>Selected plan highlighted</li> <li>Distribution of scores</li> <li>Statistical summaries</li> </ul>"},{"location":"user_guide/visualization/#advanced-features","title":"Advanced Features","text":""},{"location":"user_guide/visualization/#custom-observers","title":"Custom Observers","text":"<p>Create custom observers for specialized visualization:</p> <pre><code>from typing import Dict, Any\n\nclass CustomVisualizer:\n    def __init__(self, output_dir: str):\n        self.output_dir = output_dir\n        self.events = []\n\n    def notify(self, event: str, data: Dict[str, Any]) -&gt; None:\n        \"\"\"Receive notifications from algorithms.\"\"\"\n        self.events.append({'event': event, 'data': data})\n\n        if event == \"plan_generated\":\n            self._visualize_plan(data)\n        elif event == \"plan_verified\":\n            self._visualize_verification(data)\n\n    def _visualize_plan(self, data):\n        # Custom visualization logic\n        pass\n\n    def _visualize_verification(self, data):\n        # Custom visualization logic\n        pass\n\n# Use custom visualizer\nvisualizer = CustomVisualizer(output_dir=\"./custom_viz\")\nalgorithm.add_observer(visualizer)\n</code></pre>"},{"location":"user_guide/visualization/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user_guide/visualization/#no-visualizations-generated","title":"No Visualizations Generated","text":"<ul> <li>Check that output directory is writable</li> <li>Ensure algorithm has the observer added</li> <li>Verify the algorithm ran successfully</li> </ul>"},{"location":"user_guide/visualization/#poor-quality-visualizations","title":"Poor Quality Visualizations","text":"<pre><code># Increase resolution\nrenderer = GraphRenderer(\n    output_dir=\"./viz\",\n    dpi=300,  # Higher resolution\n    figsize=(16, 12)  # Larger figure\n)\n</code></pre>"},{"location":"user_guide/visualization/#integration-with-notebooks","title":"Integration with Notebooks","text":"<p>Visualizations work great in Jupyter notebooks:</p> <pre><code>from IPython.display import Image, display\n\n# Run algorithm with visualization\nsolution, score, metadata = algorithm.run(problem)\n\n# Display in notebook\ndisplay(Image(filename=\"./visualizations/tree_structure.png\"))\ndisplay(Image(filename=\"./visualizations/scores.png\"))\n</code></pre>"},{"location":"user_guide/visualization/#next-steps","title":"Next Steps","text":"<ul> <li>See Examples for complete examples</li> <li>Read Algorithm Reference for algorithm-specific visualization</li> <li>Check Configuration for advanced settings</li> </ul>"}]}